
# pyright: reportUnusedVariable=false, reportRedeclaration=false, reportGeneralTypeIssues=false
# pyright: reportUndefinedVariable=false, reportOptionalMemberAccess=false, reportAttributeAccessIssue=false
# type: ignore
# pylint: disable=all
# ---------------- Admin & blocking endpoints -----------------

# ...existing code...

import os, re, json, asyncio, threading, logging, pytz, time, subprocess, queue, sys, platform, traceback, uuid
from datetime import datetime, timedelta
from flask import Flask, render_template, jsonify, request, Response, send_from_directory
from telethon import TelegramClient
from apscheduler.schedulers.background import BackgroundScheduler

# Import blackout API client
try:
    from blackout_api import blackout_client
    BLACKOUT_API_AVAILABLE = True
    print("INFO: Blackout API client loaded successfully")
except Exception as e:
    BLACKOUT_API_AVAILABLE = False
    print(f"WARNING: Blackout API client not available: {e}")

# Import schedule updater
try:
    from schedule_updater import schedule_updater, start_schedule_updates
    SCHEDULE_UPDATER_AVAILABLE = True
    print("INFO: Schedule updater loaded successfully")
except Exception as e:
    SCHEDULE_UPDATER_AVAILABLE = False
    print(f"WARNING: Schedule updater not available: {e}")

# Import expanded Ukraine addresses database
try:
    from ukraine_addresses_db import UKRAINE_ADDRESSES_DB, UKRAINE_CITIES
    print(f"INFO: Ukraine addresses database loaded: {len(UKRAINE_ADDRESSES_DB)} addresses")
except Exception as e:
    UKRAINE_ADDRESSES_DB = {}
    UKRAINE_CITIES = []
    print(f"WARNING: Ukraine addresses database not available: {e}")

# SpaCy integration for enhanced Ukrainian NLP
try:
    import spacy
    nlp = spacy.load('uk_core_news_sm')
    SPACY_AVAILABLE = True
    print("INFO: SpaCy Ukrainian model uk_core_news_sm loaded successfully")
except ImportError:
    SPACY_AVAILABLE = False
    nlp = None
    print("WARNING: SpaCy library not available - NLP analysis disabled")
except OSError:
    SPACY_AVAILABLE = False
    nlp = None
    print("WARNING: SpaCy Ukrainian model uk_core_news_sm not found - NLP analysis disabled")
    print("HINT: Install with: python -m spacy download uk_core_news_sm")
except Exception as e:
    SPACY_AVAILABLE = False
    nlp = None
    print(f"ERROR: SpaCy initialization failed: {e}")

# Nominatim geocoding integration
try:
    from nominatim_geocoder import get_coordinates_nominatim
    NOMINATIM_AVAILABLE = True
except ImportError:
    NOMINATIM_AVAILABLE = False
    def get_coordinates_nominatim(city_name, region=None):
        return None

# Context-aware geocoding integration
try:
    from context_aware_geocoder import get_context_aware_geocoding
    CONTEXT_GEOCODER_AVAILABLE = True
except ImportError:
    CONTEXT_GEOCODER_AVAILABLE = False
    def get_context_aware_geocoding(text):
        return []
        return None
    nlp = None
    SPACY_AVAILABLE = False
    print("WARNING: SpaCy Ukrainian model not available. Using fallback geocoding methods.")
try:
    from telethon.errors import (
        AuthKeyDuplicatedError,
        AuthKeyUnregisteredError,
        FloodWaitError,
        SessionPasswordNeededError
    )
except ImportError:
    # Fallback dummies if some names not present in current Telethon version
    class AuthKeyDuplicatedError(Exception):
        pass
    class AuthKeyUnregisteredError(Exception):
        pass
    class FloodWaitError(Exception):
        def __init__(self, seconds=60): self.seconds = seconds
    class SessionPasswordNeededError(Exception):
        pass
from telethon.sessions import StringSession
import math

# === Kyiv Directional Enhancement Functions ===
def calculate_bearing(lat1, lon1, lat2, lon2):
    """Calculate bearing from point 1 to point 2 in degrees (0-360)"""
    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    
    dlon = lon2 - lon1
    y = math.sin(dlon) * math.cos(lat2)
    x = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)
    
    bearing = math.atan2(y, x)
    bearing = math.degrees(bearing)
    return (bearing + 360) % 360

def get_kyiv_directional_coordinates(threat_text, original_city="ÐºÐ¸Ñ—Ð²"):
    """
    For Kyiv threats, calculate directional coordinates based on threat patterns
    Returns modified coordinates showing approach direction instead of city center
    """
    kyiv_lat, kyiv_lng = 50.4501, 30.5234
    threat_lower = threat_text.lower()
    
    # Try to extract source city/direction from course patterns
    course_patterns = [
        r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ.*?Ð½Ð°.*?ÐºÐ¸Ñ—Ð².*?Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
        r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ.*?Ð½Ð°.*?ÐºÐ¸Ñ—Ð².*?Ð²Ñ–Ð´\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])', 
        r'([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?).*?ÐºÑƒÑ€Ñ.*?Ð½Ð°.*?ÐºÐ¸Ñ—Ð²',
        r'z\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?).*?ÐºÑƒÑ€Ñ.*?Ð½Ð°.*?ÐºÐ¸Ñ—Ð²'
    ]
    
    source_city = None
    for pattern in course_patterns:
        matches = re.findall(pattern, threat_lower)
        if matches:
            potential_city = matches[0].strip()
            if potential_city and len(potential_city) > 2:
                # Clean up common noise words
                noise_words = {'Ð±Ð¿Ð»Ð°', 'ÐºÑƒÑ€ÑÐ¾Ð¼', 'ÐºÑƒÑ€Ñ', 'Ð½Ð°', 'Ð½Ð°Ð´', 'Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¾Ð±Ð»', 'Ñ€Ð°Ð¹Ð¾Ð½'}
                clean_city = ' '.join([word for word in potential_city.split() if word not in noise_words])
                if clean_city:
                    source_city = clean_city
                    break
    
    if source_city:
        # Try to find coordinates for source city (we'll need to implement a simple lookup)
        # For now, use some common approach directions
        approach_directions = {
            'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²': (51.4982, 31.2893, "â†˜ ÐšÐ¸Ñ—Ð²"),
            'ÑÑƒÐ¼Ð¸': (50.9077, 34.7981, "â†™ ÐšÐ¸Ñ—Ð²"), 
            'Ñ…Ð°Ñ€ÐºÑ–Ð²': (49.9935, 36.2304, "â† ÐšÐ¸Ñ—Ð²"),
            'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°': (49.5883, 34.5514, "â†– ÐšÐ¸Ñ—Ð²"),
            'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸': (49.4444, 32.0598, "â†‘ ÐšÐ¸Ñ—Ð²"),
            'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€': (50.2547, 28.6587, "â†’ ÐšÐ¸Ñ—Ð²"),
            'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°': (49.7939, 30.1014, "â†— ÐšÐ¸Ñ—Ð²")
        }
        
        if source_city in approach_directions:
            source_lat, source_lng, direction_label = approach_directions[source_city]
            
            # Calculate bearing from source to Kyiv
            bearing = calculate_bearing(source_lat, source_lng, kyiv_lat, kyiv_lng)
            
            # Place marker on approach path (70% of the way from source to Kyiv)
            progress = 0.7  # 70% towards Kyiv
            approach_lat = source_lat + (kyiv_lat - source_lat) * progress
            approach_lng = source_lng + (kyiv_lng - source_lng) * progress
            
            return approach_lat, approach_lng, f"{direction_label} ({int(bearing)}Â°)", source_city
    
    # Fallback: use directional keywords to offset from center
    direction_offsets = {
        'Ð¿Ñ–Ð²Ð´Ð½': (-0.08, 0, "â†‘ ÐšÐ¸Ñ—Ð² (ÐŸÐ´)"),      # south
        'Ð¿Ñ–Ð²Ð´ÐµÐ½': (-0.08, 0, "â†‘ ÐšÐ¸Ñ—Ð² (ÐŸÐ´)"), 
        'Ð¿Ð½': (0.08, 0, "â†“ ÐšÐ¸Ñ—Ð² (ÐŸÐ½)"),          # north
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡': (0.08, 0, "â†“ ÐšÐ¸Ñ—Ð² (ÐŸÐ½)"),
        'ÑÑ…': (0, 0.08, "â† ÐšÐ¸Ñ—Ð² (Ð¡Ñ…)"),          # east  
        'ÑÑ…Ñ–Ð´': (0, 0.08, "â† ÐšÐ¸Ñ—Ð² (Ð¡Ñ…)"),
        'Ð·Ñ…': (0, -0.08, "â†’ ÐšÐ¸Ñ—Ð² (Ð—Ñ…)"),         # west
        'Ð·Ð°Ñ…Ñ–Ð´': (0, -0.08, "â†’ ÐšÐ¸Ñ—Ð² (Ð—Ñ…)"),
        'Ð¿Ð´-ÑÑ…': (-0.06, 0.06, "â†– ÐšÐ¸Ñ—Ð² (ÐŸÐ´Ð¡Ñ…)"), # southeast
        'Ð¿Ð½-Ð·Ñ…': (0.06, -0.06, "â†˜ ÐšÐ¸Ñ—Ð² (ÐŸÐ½Ð—Ñ…)"), # northwest
    }
    
    for direction, (lat_offset, lng_offset, label) in direction_offsets.items():
        if direction in threat_lower:
            return (kyiv_lat + lat_offset, kyiv_lng + lng_offset, 
                   label, direction)
    
    # Default: return regular Kyiv coordinates
    return kyiv_lat, kyiv_lng, "ÐšÐ¸Ñ—Ð²", None

def extract_shahed_course_info(threat_text):
    """
    Extract course information from Shahed/UAV threat messages
    Returns: (source_city, target_city, direction, bearing, course_type)
    """
    text_lower = threat_text.lower()
    
    # Common course patterns for Shahed/UAV
    course_patterns = [
        # "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð· [source] Ð½Ð° [target]"
        r'Ð±Ð¿Ð»Ð°\s+.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
        # "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [target] Ð· [source]"  
        r'Ð±Ð¿Ð»Ð°\s+.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
        # "Ð‘Ð¿Ð›Ð Ð· [source] ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [target]"
        r'Ð±Ð¿Ð»Ð°\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)\s+ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
        # "Ð‘Ð¿Ð›Ð Ð· [source] Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ [target]"
        r'Ð±Ð¿Ð»Ð°\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)\s+Ñƒ\s+Ð½Ð°Ð¿Ñ€ÑÐ¼Ðº[ÑƒÐ¸]\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
        # "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [target]" (target only)
        r'Ð±Ð¿Ð»Ð°\s+.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?=\s*(?:\n|$|[,\.\!\?;]))',
        # "[count]Ñ… Ð‘Ð¿Ð›Ð ÐºÑƒÑ€Ñ [source]-[target]"
        r'\d*Ñ…?\s*Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€Ñ\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)\s*[-â€“â€”]\s*([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\s\-\']+?)(?:\s|$|[,\.\!])',
    ]
    
    # Try to extract course information
    for pattern_idx, pattern in enumerate(course_patterns):
        matches = re.findall(pattern, text_lower)
        if matches:
            match = matches[0]
            
            if pattern_idx == 0:  # Ð· source Ð½Ð° target
                source = match[0].strip()
                target = match[1].strip()
            elif pattern_idx == 1:  # Ð½Ð° target Ð· source  
                target = match[0].strip()
                source = match[1].strip()
            elif pattern_idx == 2:  # Ð· source ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° target
                source = match[0].strip()
                target = match[1].strip()
            elif pattern_idx == 3:  # Ð· source Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ target
                source = match[0].strip()
                target = match[1].strip()
            elif pattern_idx == 4:  # ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° target (no source)
                source = None
                target = match.strip() if isinstance(match, str) else match[0].strip()
            elif pattern_idx == 5:  # ÐºÑƒÑ€Ñ source-target
                source = match[0].strip()
                target = match[1].strip()
            
            # Clean up noise words
            noise_words = {'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¾Ð±Ð»', 'Ñ€Ð°Ð¹Ð¾Ð½', 'Ñ€-Ð½', 'Ð½Ð°', 'Ð·', 'Ð²Ñ–Ð´', 'Ð´Ð¾'}
            if source:
                source = ' '.join([word for word in source.split() if word not in noise_words]).strip()
            if target:
                target = ' '.join([word for word in target.split() if word not in noise_words]).strip()
            
            # Determine course type
            if source and target:
                course_type = "full_course"  # Full trajectory
            elif target:
                course_type = "target_only"  # Only destination
            else:
                course_type = "unknown"
                
            return {
                'source_city': source,
                'target_city': target,
                'course_direction': f"Ð½Ð° {target}" if target else None,
                'raw_direction': None,
                'course_type': course_type
            }
    
    # Try to extract directional information
    direction_patterns = {
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡': 'N', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹': 'N', 'Ð¿Ð½': 'N',
        'Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ': 'S', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹': 'S', 'Ð¿Ð´': 'S', 
        'ÑÑ…Ñ–Ð´': 'E', 'ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹': 'E', 'ÑÑ…': 'E',
        'Ð·Ð°Ñ…Ñ–Ð´': 'W', 'Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹': 'W', 'Ð·Ñ…': 'W',
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹': 'NE', 'Ð¿Ð½-ÑÑ…': 'NE',
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹': 'NW', 'Ð¿Ð½-Ð·Ñ…': 'NW', 
        'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹': 'SE', 'Ð¿Ð´-ÑÑ…': 'SE',
        'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹': 'SW', 'Ð¿Ð´-Ð·Ñ…': 'SW'
    }
    
    for direction_ukr, direction_eng in direction_patterns.items():
        if direction_ukr in text_lower:
            return {
                'source_city': None,
                'target_city': None,
                'course_direction': direction_eng,
                'raw_direction': direction_ukr,
                'course_type': "directional"
            }
    
    return None

# Basic minimal subset for Render deployment. Heavy ML parts stripped for now.
# Load secrets from a local hidden .env file (key=value) if present (for local dev),
# then fall back to environment variables (for Render / production).

def _load_local_env(path: str = '.env'):
    if not os.path.exists(path):
        return
    try:
        with open(path, encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '=' not in line:
                    continue
                k, v = line.split('=', 1)
                k = k.strip(); v = v.strip().strip('"').strip("'")
                # don't override already exported env vars
                if k and k not in os.environ:
                    os.environ[k] = v
    except Exception as e:
        logging.warning(f"Failed to load .env file: {e}")

_load_local_env()

API_ID = int(os.getenv('TELEGRAM_API_ID', '0') or '0')
API_HASH = os.getenv('TELEGRAM_API_HASH', '')
_DEFAULT_CHANNELS = 'UkraineAlarmSignal,kpszsu,war_monitor,napramok,raketa_trevoga'
# TELEGRAM_CHANNELS env var (comma-separated) overrides; fallback includes numeric channel ID.
CHANNELS = [c.strip() for c in os.getenv('TELEGRAM_CHANNELS', _DEFAULT_CHANNELS).split(',') if c.strip()]

# Channels which failed resolution (entity not found / access denied) to avoid repeated spam
INVALID_CHANNELS = set()
GOOGLE_MAPS_KEY = os.getenv('GOOGLE_MAPS_KEY', '')
OPENCAGE_API_KEY = os.getenv('OPENCAGE_API_KEY', '')  # optional geocoding
ALWAYS_STORE_RAW = os.getenv('ALWAYS_STORE_RAW', '1') not in ('0','false','False')

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
log = logging.getLogger(__name__)

app = Flask(__name__)

# Initialize background scheduler for automatic schedule updates
scheduler = BackgroundScheduler(daemon=True)
scheduler_initialized = False

def init_scheduler():
    """Initialize the scheduler for automatic updates"""
    global scheduler_initialized
    
    if scheduler_initialized:
        return
    
    if SCHEDULE_UPDATER_AVAILABLE:
        try:
            # Run initial update
            start_schedule_updates()
            
            # Schedule hourly updates
            scheduler.add_job(
                func=schedule_updater.update_all_schedules,
                trigger='interval',
                hours=1,
                id='update_schedules',
                name='Update blackout schedules from DTEK and Ukrenergo',
                replace_existing=True
            )
            
            scheduler.start()
            scheduler_initialized = True
            log.info("âœ… Scheduler started: automatic updates every hour")
        except Exception as e:
            log.error(f"âŒ Failed to start scheduler: {e}")
    else:
        log.warning("âš  Schedule updater not available, skipping automatic updates")
    
    # YASNO API schedule refresh every hour
    if BLACKOUT_API_AVAILABLE:
        try:
            # Schedule YASNO schedule refresh
            scheduler.add_job(
                func=refresh_yasno_schedules,
                trigger='interval',
                hours=1,
                id='refresh_yasno',
                name='Refresh YASNO API schedules cache',
                replace_existing=True
            )
            log.info("âœ… YASNO schedule refresh scheduled every hour")
        except Exception as e:
            log.error(f"âŒ Failed to schedule YASNO refresh: {e}")

def refresh_yasno_schedules():
    """Background task to refresh YASNO schedules cache"""
    try:
        if BLACKOUT_API_AVAILABLE:
            log.info("ðŸ”„ Refreshing YASNO schedules from API...")
            result = blackout_client.fetch_yasno_schedule(force_refresh=True)
            if result:
                log.info(f"âœ… YASNO schedules refreshed successfully")
            else:
                log.warning("âš  Failed to refresh YASNO schedules")
    except Exception as e:
        log.error(f"âŒ Error refreshing YASNO schedules: {e}")

# Don't start scheduler immediately - wait for first request
# init_scheduler()

# BANDWIDTH OPTIMIZATION: Rate limiting to prevent abuse
    # Rate limiting Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½: Ð²ÑÐµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð¼ÐµÑŽÑ‚ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ñ‹Ð¹ Ð´Ð¾ÑÑ‚ÑƒÐ¿

# BANDWIDTH OPTIMIZATION: Enable gzip compression globally
from flask import Flask
import gzip
import io

# Initialize scheduler on first request
@app.before_request
def ensure_scheduler_running():
    """Ensure scheduler is initialized on first request"""
    if not scheduler_initialized:
        init_scheduler()

# Add global response compression
@app.after_request
def compress_response(response):
    """Apply gzip compression to reduce bandwidth usage."""
    if (
        response.status_code == 200 and
        'gzip' in request.headers.get('Accept-Encoding', '').lower() and
        response.content_length and response.content_length > 500 and
        response.content_type.startswith(('application/json', 'text/html', 'text/css', 'application/javascript'))
    ):
        try:
            # Compress the response data
            buffer = io.BytesIO()
            with gzip.GzipFile(fileobj=buffer, mode='wb') as f:
                f.write(response.get_data())
            
            response.set_data(buffer.getvalue())
            response.headers['Content-Encoding'] = 'gzip'
            response.headers['Content-Length'] = len(response.get_data())
            response.headers['Vary'] = 'Accept-Encoding'
        except Exception:
            pass  # If compression fails, return original response
    
    # Add cache headers for static content
    if request.endpoint == 'static':
        response.headers['Cache-Control'] = 'public, max-age=86400'  # 24 hours
    
    return response

# Custom route for serving pre-compressed static files
@app.route('/static/<path:filename>')
def static_with_gzip(filename):
    """Serve static files with gzip compression support."""
    # Rate limiting removed
    # ...existing code...
    """Serve static files with gzip compression support."""
    
    # CRITICAL BANDWIDTH PROTECTION: Rate limit static files
    client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', request.remote_addr)
    static_requests = request_counts.get(f"{client_ip}_static", [])
    now_time = time.time()
    
    # Clean old requests (last 60 seconds)
    static_requests = [req_time for req_time in static_requests if now_time - req_time < 60]
    
    # Allow only 5 static file requests per minute per IP
    if len(static_requests) >= 5:
        print(f"[CRITICAL BANDWIDTH] Blocking static file {filename} from {client_ip} - too many requests")
        return jsonify({'error': 'Static files rate limited - wait 1 minute'}), 429
    
    static_requests.append(now_time)
    request_counts[f"{client_ip}_static"] = static_requests
        # Rate limiting removed
    
    # SMART BANDWIDTH PROTECTION: Block only genuinely large files (>1MB)
    try:
        static_folder = os.path.join(os.path.dirname(__file__), 'static')
        file_path = os.path.join(static_folder, filename)
        
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            
            # Block files larger than 1MB to save bandwidth
            if file_size > 1024 * 1024:  # 1MB limit 
                print(f"[BANDWIDTH PROTECTION] Blocking large file {filename} ({file_size//1024}KB) from {client_ip}")
                return jsonify({'error': f'Large file blocked - size {file_size//1024}KB exceeds 1MB limit'}), 503
                
            # Log access to files over 100KB for monitoring
            if file_size > 100 * 1024:
                print(f"[BANDWIDTH MONITOR] Serving large file {filename} ({file_size//1024}KB) to {client_ip}")
        else:
            print(f"[STATIC FILE] File not found: {filename}")
            return jsonify({'error': 'File not found'}), 404
            
    except Exception as e:
        print(f"[BANDWIDTH ERROR] Error checking file {filename}: {e}")
        return jsonify({'error': 'File access error'}), 500
    
    # Check if client accepts gzip and we have a gzipped version
    accepts_gzip = 'gzip' in request.headers.get('Accept-Encoding', '').lower()
    
    if accepts_gzip and filename.endswith('.js'):
        gzip_path = os.path.join(app.static_folder, filename + '.gz')
        if os.path.exists(gzip_path):
            response = send_from_directory(app.static_folder, filename + '.gz')
            response.headers['Content-Encoding'] = 'gzip'
            response.headers['Content-Type'] = 'application/javascript; charset=utf-8'
            
            # Add strong caching for JS files
            response.headers['Cache-Control'] = 'public, max-age=31536000, immutable'
            response.headers['Expires'] = (datetime.now() + timedelta(days=365)).strftime('%a, %d %b %Y %H:%M:%S GMT')
            
            return response
    
    # Fall back to regular static file serving
    response = send_from_directory(app.static_folder, filename)
    
    # CRITICAL BANDWIDTH PROTECTION: Check file size
    try:
        file_path = os.path.join(app.static_folder, filename)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            if file_size > 100 * 1024:  # 100KB limit
                print(f"[CRITICAL BANDWIDTH] Large static file {filename}: {file_size/1024:.1f}KB from {client_ip}")
    except Exception:
        pass
    
    return response

# Configure caching and compression for better performance on slow connections
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 31536000  # 1 year for static files
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# Add cache headers for static files
@app.after_request
def add_cache_headers(response):
    if request.endpoint == 'static':
        # Check if this is a versioned resource (with ?v= parameter)
        if 'v=' in request.query_string.decode():
            # Cache versioned static files for 1 month (they won't change)
            response.headers['Cache-Control'] = 'public, max-age=2592000, immutable'
            response.headers['Expires'] = (datetime.now() + timedelta(days=30)).strftime('%a, %d %b %Y %H:%M:%S GMT')
        else:
            # Cache regular static files for 1 week
            response.headers['Cache-Control'] = 'public, max-age=604800, immutable'
            response.headers['Expires'] = (datetime.now() + timedelta(days=7)).strftime('%a, %d %b %Y %H:%M:%S GMT')
        
        # Add compression hints for images
        if request.path.endswith(('.png', '.jpg', '.jpeg', '.webp')):
            response.headers['Vary'] = 'Accept-Encoding'
            # Add ETag for better caching
            response.headers['ETag'] = f'"{hash(request.path + request.query_string.decode())}"'
            
    elif request.endpoint == 'index':
        # Cache main page for 5 minutes
        response.headers['Cache-Control'] = 'public, max-age=300'
        
    return response
COMMENTS = []  # retained as a small in-memory cache (recent) but now persisted to SQLite
COMMENTS_MAX = 500
ACTIVE_VISITORS = {}
ACTIVE_LOCK = threading.Lock()
ACTIVE_TTL = 70  # seconds of inactivity before a visitor is dropped
BLOCKED_FILE = 'blocked_ids.json'
STATS_FILE = 'visits_stats.json'  # persistent first-seen timestamps per visitor id
RECENT_VISITS_FILE = 'visits_recent.json'  # stores rolling today/week visitor id sets for fast counts
VISIT_STATS = None  # lazy-loaded dict: {id: first_seen_epoch}
client = None
session_str = os.getenv('TELEGRAM_SESSION')  # Telethon string session (recommended for Render)
BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')  # optional bot token fallback
AUTH_SECRET = os.getenv('AUTH_SECRET')  # simple shared secret to protect /auth endpoints
FETCH_THREAD_STARTED = False
AUTH_STATUS = {'authorized': False, 'reason': 'init'}
SUBSCRIBERS = set()  # queues for SSE clients
INIT_ONCE = False  # guard to ensure background startup once
# Persistent dynamic channels file
CHANNELS_FILE = 'channels_dynamic.json'

# Global debug storage for admin panel
DEBUG_LOGS = []
MAX_DEBUG_LOGS = 100

# Cache for fallback reparse to avoid duplicate processing
FALLBACK_REPARSE_CACHE = set()  # message IDs that have been reparsed
MAX_REPARSE_CACHE_SIZE = 1000  # Limit cache size to prevent memory growth

def add_debug_log(message, category="general"):
    """Add debug message to global debug storage for admin panel."""
    global DEBUG_LOGS
    DEBUG_LOGS.append({
        'timestamp': datetime.now().isoformat(),
        'category': category,
        'message': str(message)
    })
    # Keep only recent logs
    if len(DEBUG_LOGS) > MAX_DEBUG_LOGS:
        DEBUG_LOGS = DEBUG_LOGS[-MAX_DEBUG_LOGS:]

# -------- Air alarm tracking (oblast / raion) --------
APP_ALARM_TTL_MINUTES = 65  # auto-expire if no update ~1h
ACTIVE_OBLAST_ALARMS = {}   # canonical oblast key -> {'since': epoch, 'last': epoch}
ACTIVE_RAION_ALARMS = {}    # raion base (lowercase) -> {'since': epoch, 'last': epoch}

# P-code mapping for ADM1 (Ð¾Ð±Ð»Ð°ÑÑ‚Ñ– + special status cities)
OBLAST_PCODE = {
    'Ð°Ð²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ð° Ñ€ÐµÑÐ¿ÑƒÐ±Ð»Ñ–ÐºÐ° ÐºÑ€Ð¸Ð¼': 'UA01',
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA05',
    'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA07',
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA12',
    'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA14',
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA18',
    'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA21',
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA23',
    'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA26',
    'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA32',
    'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA35',
    'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA44',
    'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA46',
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA48',
    'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA51',
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA53',
    'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA56',
    'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA59',
    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA61',
    'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA63',
    'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA65',
    'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA68',
    'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA71',
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA73',
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': 'UA74',
    'ÐºÐ¸Ñ—Ð²': 'UA80',
    'ÑÐµÐ²Ð°ÑÑ‚Ð¾Ð¿Ð¾Ð»ÑŒ': 'UA85'
}

# ---- Alarm persistence (SQLite) ----
def init_alarms_db():
    try:
        with _visits_db_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS alarms (
                    id TEXT PRIMARY KEY,
                    level TEXT,
                    name TEXT,
                    since REAL,
                    last REAL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_alarms_level ON alarms(level)")
    except Exception as e:
        log.warning(f"alarms db init failed: {e}")

def init_alarm_events_db():
    try:
        with _visits_db_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS alarm_events (
                    id TEXT PRIMARY KEY,
                    level TEXT,
                    name TEXT,
                    event TEXT,
                    ts REAL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_alarm_events_time ON alarm_events(ts)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_alarm_events_name ON alarm_events(name)")
    except Exception as e:
        log.warning(f"alarm_events db init failed: {e}")

def log_alarm_event(level:str, name:str, event:str, ts=None):
    ts = ts or time.time()
    try:
        with _visits_db_conn() as conn:
            conn.execute("INSERT INTO alarm_events (id,level,name,event,ts) VALUES (?,?,?,?,?)",
                         (uuid.uuid4().hex[:12], level, name, event, ts))
    except Exception as e:
        log.debug(f"log_alarm_event failed: {e}")

def _alarm_key(level:str, name:str)->str:
    return f"{level}:{name}".lower()

def persist_alarm(level:str, name:str, since:float, last:float):
    try:
        with _visits_db_conn() as conn:
            conn.execute("INSERT OR REPLACE INTO alarms (id,level,name,since,last) VALUES (?,?,?,?,?)",
                         (_alarm_key(level,name), level, name, since, last))
    except Exception as e:
        log.debug(f"persist_alarm failed: {e}")

def remove_alarm(level:str, name:str):
    try:
        with _visits_db_conn() as conn:
            conn.execute("DELETE FROM alarms WHERE id=?", (_alarm_key(level,name),))
    except Exception as e:
        log.debug(f"remove_alarm failed: {e}")

def load_active_alarms(ttl_seconds:int):
    out_obl = {}
    out_raion = {}
    cutoff = time.time() - ttl_seconds
    try:
        with _visits_db_conn() as conn:
            cur = conn.execute("SELECT level,name,since,last FROM alarms WHERE last >= ?", (cutoff,))
            for level,name,since,last in cur.fetchall():
                if level == 'oblast': out_obl[name] = {'since': since, 'last': last}
                elif level == 'raion': out_raion[name] = {'since': since, 'last': last}
    except Exception as e:
        log.debug(f"load_active_alarms failed: {e}")
    return out_obl, out_raion

def load_dynamic_channels():
    try:
        if os.path.exists(CHANNELS_FILE):
            with open(CHANNELS_FILE,'r',encoding='utf-8') as f:
                dyn = json.load(f)
            if isinstance(dyn, list):
                return [str(x).strip() for x in dyn if x]
    except Exception as e:
        log.warning(f'Failed loading {CHANNELS_FILE}: {e}')
    return []

def save_dynamic_channels(extra):
    try:
        with open(CHANNELS_FILE,'w',encoding='utf-8') as f:
            json.dump(extra, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f'Failed saving {CHANNELS_FILE}: {e}')

_dyn = load_dynamic_channels()
if _dyn:
    # Merge without duplicates
    base = [c.strip() for c in CHANNELS if c.strip()]
    for d in _dyn:
        if d not in base:
            base.append(d)
    CHANNELS = base
# ---------------- Monitoring period global config (admin editable) ----------------
CONFIG_FILE = 'config.json'
MONITOR_PERIOD_MINUTES = 30  # default; editable only via admin panel

def load_config():
    """Load persisted configuration (currently only monitor period)."""
    global MONITOR_PERIOD_MINUTES
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                cfg = json.load(f)
            # Validate range 1..360 else ignore
            mp = int(cfg.get('monitor_period', MONITOR_PERIOD_MINUTES))
            if 1 <= mp <= 360:
                MONITOR_PERIOD_MINUTES = mp
    except Exception as e:
        log.warning(f'Failed loading {CONFIG_FILE}: {e}')

def save_config():
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump({'monitor_period': MONITOR_PERIOD_MINUTES}, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f'Failed saving {CONFIG_FILE}: {e}')

load_config()
if API_ID and API_HASH:
    if session_str:
        log.info('Initializing Telegram client with TELEGRAM_SESSION string.')
        client = TelegramClient(StringSession(session_str), API_ID, API_HASH)
    elif BOT_TOKEN:
        log.info('Initializing Telegram client with BOT token (limited access).')
        # Bot sessions auto-authorize on start
        client = TelegramClient(StringSession(), API_ID, API_HASH)
    else:
        log.info('Initializing Telegram client with local session file (may not persist on Render).')
        client = TelegramClient('anon', API_ID, API_HASH)

MESSAGES_FILE = 'messages.json'
HIDDEN_FILE = 'hidden_markers.json'
OPENCAGE_CACHE_FILE = 'opencage_cache.json'
OPENCAGE_TTL = 60 * 60 * 24 * 30  # 30 days
NEG_GEOCODE_FILE = 'negative_geocode_cache.json'
NEG_GEOCODE_TTL = 60 * 60 * 24 * 3  # 3 days for 'not found' entries
MESSAGES_RETENTION_MINUTES = int(os.getenv('MESSAGES_RETENTION_MINUTES', '120'))  # 2 hours retention by default
MESSAGES_MAX_COUNT = int(os.getenv('MESSAGES_MAX_COUNT', '0'))  # 0 = unlimited

def _startup_diagnostics():
    """Log one-time startup diagnostics to help investigate early exit issues on hosting platforms."""
    try:
        log.info('--- Startup diagnostics begin ---')
        log.info(f'Python: {sys.version.split()[0]} Platform: {platform.platform()} PID: {os.getpid()}')
        log.info(f'Flask version: {getattr(sys.modules.get("flask"), "__version__", "?")} Telethon version: {getattr(sys.modules.get("telethon"), "__version__", "?")}')
        log.info(f'Configured channels ({len(CHANNELS)}): {CHANNELS}')
        log.info(f'API_ID set: {bool(API_ID)} HASH set: {bool(API_HASH)} SESSION len: {len(session_str) if session_str else 0}')
        log.info(f'GOOGLE_MAPS_KEY set: {bool(GOOGLE_MAPS_KEY)} OPENCAGE_API_KEY set: {bool(OPENCAGE_API_KEY)}')
        if os.path.exists(MESSAGES_FILE):
            try:
                sz = os.path.getsize(MESSAGES_FILE)
                log.info(f'{MESSAGES_FILE} exists size={sz} bytes')
            except Exception:
                pass
        else:
            log.info(f'{MESSAGES_FILE} not present yet.')
        log.info(f'Retention minutes: {MESSAGES_RETENTION_MINUTES} Max count: {MESSAGES_MAX_COUNT}')
        log.info(f'FETCH_START_DELAY={os.getenv("FETCH_START_DELAY", "0")}')
        log.info('--- Startup diagnostics end ---')
    except Exception as e:
        log.warning(f'Diagnostics error: {e}')

def _prune_messages(data):
    """Apply retention policies (time / count). Mutates and returns list."""
    if not data:
        return data
    # Time based pruning
    if MESSAGES_RETENTION_MINUTES > 0:
        cutoff = datetime.utcnow() - timedelta(minutes=MESSAGES_RETENTION_MINUTES)
        pruned = []
        for m in data:
            try:
                dt = datetime.strptime(m.get('date',''), '%Y-%m-%d %H:%M:%S')
            except Exception:
                # keep malformed to avoid data loss
                pruned.append(m)
                continue
            if dt.replace(tzinfo=None) >= cutoff:
                pruned.append(m)
        data = pruned
    # Count based pruning (keep newest by date)
    if MESSAGES_MAX_COUNT > 0 and len(data) > MESSAGES_MAX_COUNT:
        try:
            data_sorted = sorted(data, key=lambda x: x.get('date',''))
            data = data_sorted[-MESSAGES_MAX_COUNT:]
        except Exception:
            data = data[-MESSAGES_MAX_COUNT:]
    return data

def load_messages():
    if os.path.exists(MESSAGES_FILE):
        try:
            with open(MESSAGES_FILE, encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return []
    return []

def save_messages(data):
    # Apply retention before persistence
    try:
        data = _prune_messages(data)
    except Exception as e:
        log.debug(f'Retention prune error: {e}')
    print(f"DEBUG: Saving {len(data)} messages to file")
    with open(MESSAGES_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    # After each save attempt optional git auto-commit
    try:
        maybe_git_autocommit()
    except Exception as e:
        log.debug(f'git auto-commit skipped: {e}')

# ---------------- Deduplication / merge of near-duplicate geo events -----------------
# Two messages that refer to the same object coming almost back-to-back should not
# produce two separate points: instead we update the earlier one (increment count, merge text).
# Heuristics: same threat_type, within DEDUP_DIST_KM km, within DEDUP_TIME_MIN minutes.
DEDUP_TIME_MIN = int(os.getenv('DEDUP_TIME_MIN', '5'))
DEDUP_DIST_KM = float(os.getenv('DEDUP_DIST_KM', '7'))
DEDUP_SCAN_BACK = int(os.getenv('DEDUP_SCAN_BACK', '400'))  # how many recent messages to scan

def _parse_dt(s:str):
    try:
        return datetime.strptime(s, '%Y-%m-%d %H:%M:%S')
    except Exception:
        return None

def _haversine_km(lat1, lon1, lat2, lon2):
    try:
        from math import radians, sin, cos, asin, sqrt
        R = 6371.0
        dlat = radians(lat2-lat1)
        dlon = radians(lon2-lon1)
        a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2
        c = 2*asin(sqrt(a))
        return R*c
    except Exception:
        return 999999

def maybe_merge_track(all_data:list, new_track:dict):
    """Try to merge new_track into an existing recent track.
    Returns tuple (merged: bool, track_ref: dict).
    """
    try:
        if not all_data:
            return False, new_track
        tt = (new_track.get('threat_type') or '').lower()
        if not tt:
            return False, new_track
        lat = new_track.get('lat'); lng = new_track.get('lng')
        if not isinstance(lat, (int,float)) or not isinstance(lng, (int,float)):
            return False, new_track
        new_dt = _parse_dt(new_track.get('date','')) or datetime.utcnow()
        # Scan recent slice only for performance
        scan_slice = all_data[-DEDUP_SCAN_BACK:]
        # Iterate reversed (newest first)
        for existing in reversed(scan_slice):
            if existing is new_track:  # shouldn't happen yet
                continue
            if (existing.get('threat_type') or '').lower() != tt:
                continue
            e_lat = existing.get('lat'); e_lng = existing.get('lng')
            if not isinstance(e_lat,(int,float)) or not isinstance(e_lng,(int,float)):
                continue
            dist = _haversine_km(lat,lng,e_lat,e_lng)
            if dist > DEDUP_DIST_KM:
                continue
            e_dt = _parse_dt(existing.get('date','')) or new_dt
            dt_min = abs((new_dt - e_dt).total_seconds())/60.0
            if dt_min > DEDUP_TIME_MIN:
                continue
            # Merge
            # Increment count
            existing['count'] = int(existing.get('count') or 1) + 1
            # Merge text (avoid duplication / uncontrolled growth)
            new_text = (new_track.get('text') or '').strip()
            if new_text:
                ex_text = existing.get('text') or ''
                if new_text not in ex_text:
                    combined = (ex_text + ' | ' + new_text).strip(' |') if ex_text else new_text
                    if len(combined) > 800:
                        combined = combined[:790] + 'â€¦'
                    existing['text'] = combined
            # Maintain list of merged ids
            if 'merged_ids' not in existing:
                existing['merged_ids'] = [existing.get('id')]
            nid = new_track.get('id')
            if nid and nid not in existing['merged_ids']:
                existing['merged_ids'].append(nid)
            # Update displayed date to the most recent
            if new_dt >= e_dt:
                existing['date'] = new_track.get('date') or existing.get('date')
            # Optionally capture first occurrence time
            if 'first_date' not in existing:
                existing['first_date'] = e_dt.strftime('%Y-%m-%d %H:%M:%S')
            existing['merged'] = True
            return True, existing
    except Exception as e:
        log.debug(f'dedup merge error: {e}')
    return False, new_track


def load_hidden():
    if os.path.exists(HIDDEN_FILE):
        try:
            with open(HIDDEN_FILE, encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return []
    return []

def save_hidden(data):
    with open(HIDDEN_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_blocked():
    if os.path.exists(BLOCKED_FILE):
        try:
            with open(BLOCKED_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return []
    return []

def save_blocked(blocked):
    try:
        with open(BLOCKED_FILE, 'w', encoding='utf-8') as f:
            json.dump(blocked, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f'Failed saving {BLOCKED_FILE}: {e}')

def _load_visit_stats():
    global VISIT_STATS
    if VISIT_STATS is not None:
        return VISIT_STATS
    if os.path.exists(STATS_FILE):
        try:
            with open(STATS_FILE,'r',encoding='utf-8') as f:
                VISIT_STATS = json.load(f)
        except Exception:
            VISIT_STATS = {}
    else:
        VISIT_STATS = {}
    return VISIT_STATS

def _save_visit_stats():
    if VISIT_STATS is None:
        return
    try:
        with open(STATS_FILE,'w',encoding='utf-8') as f:
            json.dump(VISIT_STATS, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f'Failed saving {STATS_FILE}: {e}')

def _prune_visit_stats(days:int=45):
    # remove entries older than N days to limit file growth
    if VISIT_STATS is None:
        return
    cutoff = time.time() - days*86400
    removed = 0
    for vid, ts in list(VISIT_STATS.items()):
        try:
            if float(ts) < cutoff:
                del VISIT_STATS[vid]
                removed += 1
        except Exception:
            continue
    if removed:
        _save_visit_stats()

# ---- Rolling daily / weekly visit tracking (for persistence of counts across deploys) ----
def _load_recent_visits():
    try:
        if os.path.exists(RECENT_VISITS_FILE):
            # Guard against oversized/corrupted file (e.g. concurrent writes producing concatenated JSON objects)
            try:
                raw = open(RECENT_VISITS_FILE, 'r', encoding='utf-8').read()
            except Exception as e_read:
                log.warning(f"Failed reading {RECENT_VISITS_FILE}: {e_read}")
                return {}
            # Quick heuristic: if multiple top-level JSON objects concatenated, keep first valid
            data = None
            if raw.strip():
                try:
                    data = json.loads(raw)
                except json.JSONDecodeError as je:
                    # Try to split by newlines and stitch until first valid JSON object
                    fragments = raw.splitlines()
                    buf = ''
                    for line in fragments:
                        buf += line.strip() + '\n'
                        try:
                            data = json.loads(buf)
                            log.warning(f"Recovered first valid JSON segment from {RECENT_VISITS_FILE} after decode error: {je}")
                            break
                        except Exception:
                            continue
                    if data is None:
                        log.warning(f"Unable to repair {RECENT_VISITS_FILE}: {je}")
                        return {}
                except Exception as e_generic:
                    log.warning(f"Generic JSON load failure {RECENT_VISITS_FILE}: {e_generic}")
                    return {}
            else:
                return {}
            if not isinstance(data, dict):
                log.warning(f"Unexpected structure in {RECENT_VISITS_FILE}, resetting")
                return {}
            data.setdefault('day', '')
            data.setdefault('week_start', '')
            data.setdefault('today_ids', [])
            data.setdefault('week_ids', [])
            return data
    except Exception as e:
        log.warning(f"Failed loading {RECENT_VISITS_FILE}: {e}")
    return {}

def _save_recent_visits(data:dict):
    try:
        tmp = RECENT_VISITS_FILE + '.tmp'
        # Ensure directory exists (in case path was changed to subfolder later); here file in CWD so skip
        with open(tmp, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        try:
            os.replace(tmp, RECENT_VISITS_FILE)
        except FileNotFoundError:
            # Rare race on some FS / AV scanners: fall back to simple write
            try:
                with open(RECENT_VISITS_FILE, 'w', encoding='utf-8') as f2:
                    json.dump(data, f2, ensure_ascii=False, indent=2)
            except Exception as e2:
                log.warning(f"Fallback direct save failed {RECENT_VISITS_FILE}: {e2}")
    except Exception as e:
        log.warning(f"Failed saving {RECENT_VISITS_FILE}: {e}")

def _update_recent_visits(vid:str):
    """Update rolling daily/week sets with visitor id. Uses Europe/Kyiv timezone.
    This offers stable daily/week unique counts even if the broader first-seen file is lost on redeploy."""
    if not vid:
        return
    data = _load_recent_visits() or {}
    tz = pytz.timezone('Europe/Kyiv')
    now_dt = datetime.now(tz)
    today = now_dt.strftime('%Y-%m-%d')
    # ISO week (Monday start) anchor date for 7-day rolling window (not strictly calendar week) -> we store date 6 days prior cutoff
    # We'll implement simple 7-day rolling: if stored week_start older than 7 days, reset week_ids
    stored_week_start = data.get('week_start') or today
    try:
        sw_dt = datetime.strptime(stored_week_start, '%Y-%m-%d')
        # make tz-aware in same timezone
        sw_dt = tz.localize(sw_dt)
    except Exception:
        sw_dt = now_dt
    if (now_dt - sw_dt).days >= 7:
        # reset week window
        stored_week_start = today
        data['week_ids'] = []
    # day rollover
    if data.get('day') != today:
        data['day'] = today
        data['today_ids'] = []
    # ensure lists
    if 'today_ids' not in data or not isinstance(data['today_ids'], list):
        data['today_ids'] = []
    if 'week_ids' not in data or not isinstance(data['week_ids'], list):
        data['week_ids'] = []
    if vid not in data['today_ids']:
        data['today_ids'].append(vid)
    if vid not in data['week_ids']:
        data['week_ids'].append(vid)
    data['week_start'] = stored_week_start
    _save_recent_visits(data)

def _recent_counts():
    data = _load_recent_visits()
    if not data:
        return None, None
    return len(set(data.get('today_ids', []))), len(set(data.get('week_ids', [])))

# Simplified message processor placeholder
import math
import sqlite3

_opencage_cache = None
_neg_geocode_cache = None

def _load_opencage_cache():
    global _opencage_cache
    if _opencage_cache is not None:
        return _opencage_cache
    if os.path.exists(OPENCAGE_CACHE_FILE):
        try:
            with open(OPENCAGE_CACHE_FILE, 'r', encoding='utf-8') as f:
                _opencage_cache = json.load(f)
        except Exception:
            _opencage_cache = {}
    else:
        _opencage_cache = {}
    return _opencage_cache

def _save_opencage_cache():
    if _opencage_cache is None:
        return
    try:
        with open(OPENCAGE_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(_opencage_cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"Failed saving OpenCage cache: {e}")

def _load_neg_geocode_cache():
    global _neg_geocode_cache
    if _neg_geocode_cache is not None:
        return _neg_geocode_cache
    if os.path.exists(NEG_GEOCODE_FILE):
        try:
            with open(NEG_GEOCODE_FILE,'r',encoding='utf-8') as f:
                _neg_geocode_cache = json.load(f)
        except Exception:
            _neg_geocode_cache = {}
    else:
        _neg_geocode_cache = {}
    return _neg_geocode_cache

def _save_neg_geocode_cache():
    if _neg_geocode_cache is None:
        return
    try:
        with open(NEG_GEOCODE_FILE,'w',encoding='utf-8') as f:
            json.dump(_neg_geocode_cache,f,ensure_ascii=False,indent=2)
    except Exception as e:
        log.warning(f"Failed saving negative geocode cache: {e}")

def _msg_timestamp(msg):
    """Extract timestamp from message for sorting and filtering"""
    if not msg:
        return 0
    
    # Try different timestamp fields
    date_str = msg.get('date') or msg.get('timestamp') or msg.get('time')
    if not date_str:
        return 0
    
    try:
        # Handle different date formats
        if isinstance(date_str, (int, float)):
            return float(date_str)
        
        # Parse datetime string
        if isinstance(date_str, str):
            # Try common formats
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%d.%m.%Y %H:%M:%S', '%d.%m.%Y %H:%M']:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.timestamp()
                except ValueError:
                    continue
            
            # Try parsing with dateutil as fallback
            try:
                from dateutil import parser
                dt = parser.parse(date_str)
                return dt.timestamp()
            except:
                pass
    except Exception:
        pass
    
    return 0

def _msg_timestamp(msg):
    """Extract timestamp from message for sorting/filtering"""
    if not msg:
        return 0
    
    # Try different timestamp fields
    date_str = msg.get('date') or msg.get('timestamp') or msg.get('ts')
    if not date_str:
        return 0
    
    try:
        # Handle different date formats
        if isinstance(date_str, (int, float)):
            return float(date_str)
        
        # Parse string dates
        if isinstance(date_str, str):
            # Try common formats
            formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d %H:%M',
                '%d.%m.%Y %H:%M:%S',
                '%d.%m.%Y %H:%M'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.timestamp()
                except ValueError:
                    continue
                    
        return 0
    except Exception:
        return 0

def neg_geocode_check(name:str):
    if not name:
        return False
    cache = _load_neg_geocode_cache()
    key = name.strip().lower()
    entry = cache.get(key)
    if not entry:
        return False
    # expire
    if int(time.time()) - entry.get('ts',0) > NEG_GEOCODE_TTL:
        try: del cache[key]; _save_neg_geocode_cache()
        except Exception: pass
        return False
    return True

def neg_geocode_add(name:str, reason:str='not_found'):
    if not name:
        return
    cache = _load_neg_geocode_cache()
    key = name.strip().lower()
    cache[key] = {'ts': int(time.time()), 'reason': reason}
    _save_neg_geocode_cache()

UA_CITIES = [
    'ÐºÐ¸Ñ—Ð²','Ñ…Ð°Ñ€ÐºÑ–Ð²','Ð¾Ð´ÐµÑÐ°','Ð¾Ð´ÐµÑÑÐ°','Ð´Ð½Ñ–Ð¿Ñ€Ð¾','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐº','Ð»ÑŒÐ²Ñ–Ð²','Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ','Ð·Ð°Ð¿Ð¾Ñ€Ð¾Ð¶ÑŒÐµ','Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ','Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²','Ð½Ð¸ÐºÐ¾Ð»Ð°ÐµÐ²',
    'Ð¼Ð°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ','Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°','Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²','Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²','Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸','Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€','ÑÑƒÐ¼Ð¸','Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹','Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–','Ñ€Ñ–Ð²Ð½Ðµ','Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº',
    'Ð»ÑƒÑ†ÑŒÐº','Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ','ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´','ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹','ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´','ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº','ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº','Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°','Ð¼ÐµÐ»Ñ–Ñ‚Ð¾Ð¿Ð¾Ð»ÑŒ','Ð±ÐµÑ€Ð´ÑÐ½ÑÑŒÐº',
    'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´','Ð½Ñ–Ð¶Ð¸Ð½','ÑˆÐ¾ÑÑ‚ÐºÐ°','ÐºÐ¾Ñ€Ð¾Ð¿','ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ'
]
UA_CITY_NORMALIZE = {
    'Ð¾Ð´ÐµÑÑÐ°':'Ð¾Ð´ÐµÑÐ°','Ð·Ð°Ð¿Ð¾Ñ€Ð¾Ð¶ÑŒÐµ':'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐº':'Ð´Ð½Ñ–Ð¿Ñ€Ð¾','ÐºÐ¸Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´':'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹','ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´':'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹',
    'Ð½Ð¸ÐºÐ¾Ð»Ð°ÐµÐ²':'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²','Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²':'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²',
    # Accusative / variant forms
    'Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ñƒ':'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°','Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ð°':'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°',
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð±Ð°Ð³Ð°Ñ‡ÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡Ñƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡ÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°',
    'ÑƒÐ»ÑÐ½Ñ–Ð²ÐºÑƒ':'ÑƒÐ»ÑÐ½Ñ–Ð²ÐºÐ°','ÑƒÐ»ÑÐ½Ð¾Ð²ÐºÑƒ':'ÑƒÐ»ÑÐ½Ñ–Ð²ÐºÐ°',
    # Ð’ÐµÐ»Ð¸ÐºÐ° Ð”Ð¸Ð¼ÐµÑ€ÐºÐ° Ð¿Ð°Ð´ÐµÐ¶Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹
    'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÐ¾Ñ— Ð´Ð¸Ð¼ÐµÑ€ÐºÐ¸':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑ–Ð¹ Ð´Ð¸Ð¼ÐµÑ€Ñ†Ñ–':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°',
    # Ð’ÐµÐ»Ð¸ÐºÐ° Ð’Ð¸ÑÐºÐ° Ð¿Ð°Ð´ÐµÐ¶Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹  
    'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð²Ð¸ÑÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð¸ÑÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÐ¾Ñ— Ð²Ð¸ÑÐºÐ¸':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð¸ÑÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑ–Ð¹ Ð²Ð¸ÑÑ†Ñ–':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð¸ÑÐºÐ°',
    # ÐœÐ°Ð»Ð° Ð´Ñ–Ð²Ð¸Ñ†Ñ
    'Ð¼Ð°Ð»Ñƒ Ð´Ñ–Ð²Ð¸Ñ†ÑŽ':'Ð¼Ð°Ð»Ð° Ð´Ñ–Ð²Ð¸Ñ†Ñ','Ð¼Ð°Ð»Ð° Ð´Ñ–Ð²Ð¸Ñ†ÑŽ':'Ð¼Ð°Ð»Ð° Ð´Ñ–Ð²Ð¸Ñ†Ñ',
    # Additional safety normalizations
    'Ð¾Ð»Ð¸ÑˆÑ–Ð²ÐºÑƒ':'Ð¾Ð»Ð¸ÑˆÑ–Ð²ÐºÐ°','Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÑƒ':'Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÐ°','ÑÑ‚Ð°Ð²Ð¸Ñ‰ÐµÑŽ':'ÑÑ‚Ð°Ð²Ð¸Ñ‰Ðµ','ÐºÐµÐ³Ð¸Ñ‡Ñ–Ð²ÐºÑƒ':'ÐºÐµÐ³Ð¸Ñ‡Ñ–Ð²ÐºÐ°','ÐºÐµÐ³Ð¸Ñ‡ÐµÐ²ÐºÑƒ':'ÐºÐµÐ³Ð¸Ñ‡Ñ–Ð²ÐºÐ°',
    # Voznesensk variants
    'Ð²Ð¾Ð·Ð½ÐµÑÐµÐ½ÑÑŒÐºÐ°':'Ð²Ð¾Ð·Ð½ÐµÑÐµÐ½ÑÑŒÐº',
    # Mykolaiv variants  
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ”Ð²Ð°':'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²',
    # Novoukrainka variants
    'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÑƒ':'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÐ°',
    'ÑÑ‚Ð°Ñ€Ð¾Ð¼Ñƒ ÑÐ°Ð»Ñ‚Ð¾Ð²Ñƒ':'ÑÑ‚Ð°Ñ€Ð¸Ð¹ ÑÐ°Ð»Ñ‚Ñ–Ð²','ÑÑ‚Ð°Ñ€Ð¾Ð¼Ñƒ ÑÐ°Ð»Ñ‚Ð¾Ð²Ñ–':'ÑÑ‚Ð°Ñ€Ð¸Ð¹ ÑÐ°Ð»Ñ‚Ñ–Ð²','ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÑƒ':'ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÐ°','Ð¼Ð°Ð³Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÑƒ':'Ð¼Ð°Ð³Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°',
    'Ð±Ð°Ð»Ð°ÐºÐ»Ñ–ÑŽ':'Ð±Ð°Ð»Ð°ÐºÐ»Ñ–Ñ','Ð±Ñ–Ð»Ñƒ Ñ†ÐµÑ€ÐºÐ²Ñƒ':'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°','Ð±Ð°Ñ€Ð¸ÑˆÑ–Ð²ÐºÑƒ':'Ð±Ð°Ñ€Ð¸ÑˆÑ–Ð²ÐºÐ°','ÑÐºÐ²Ð¸Ñ€Ñƒ':'ÑÐºÐ²Ð¸Ñ€Ð°','ÑÐ¾ÑÐ½Ð¸Ñ†ÑŽ':'ÑÐ¾ÑÐ½Ð¸Ñ†Ñ',
    'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÑ–Ð²ÐºÑƒ':'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÑ–Ð²ÐºÐ°','Ð¿Ð¾Ð½Ð¾Ñ€Ð½Ð¸Ñ†ÑŽ':'Ð¿Ð¾Ð½Ð¾Ñ€Ð½Ð¸Ñ†Ñ','ÐºÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÑƒ':'ÐºÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÐ°','Ñ‚ÐµÑ€Ð½Ð¸':'Ñ‚ÐµÑ€Ð½Ð¸',
    'ÑˆÐ¾ÑÑ‚ÐºÑƒ':'ÑˆÐ¾ÑÑ‚ÐºÐ°','Ð±ÐµÑ€ÐµÐ·Ð½Ñƒ':'Ð±ÐµÑ€ÐµÐ·Ð½Ð°','Ð·Ð°Ñ‡ÐµÐ¿Ð¸Ð»Ñ–Ð²ÐºÑƒ':'Ð·Ð°Ñ‡ÐµÐ¿Ð¸Ð»Ñ–Ð²ÐºÐ°','Ð½Ð¾Ð²Ñƒ Ð²Ð¾Ð´Ð¾Ð»Ð°Ð³Ñƒ':'Ð½Ð¾Ð²Ð° Ð²Ð¾Ð´Ð¾Ð»Ð°Ð³Ð°',
    'Ð½Ð¾Ð²Ñƒ':'Ð½Ð¾Ð²Ð° Ð²Ð¾Ð´Ð¾Ð»Ð°Ð³Ð°',  # Fallback for partial regex matches
    'ÑƒÐ±Ð½Ð¸':'Ð»ÑƒÐ±Ð½Ð¸','Ð¾Ð»Ð¼Ð¸':'Ñ…Ð¾Ð»Ð¼Ð¸','Ð»ÐµÑ‚Ð¸Ñ‡Ñ–Ð²':'Ð»ÐµÑ‚Ð¸Ñ‡Ñ–Ð²','Ð»ÐµÑ‚Ð¸Ñ‡ÐµÐ²':'Ð»ÐµÑ‚Ð¸Ñ‡Ñ–Ð²','Ð»ÐµÑ‚Ð¸Ñ‡ÐµÐ²Ðµ':'Ð»ÐµÑ‚Ð¸Ñ‡Ñ–Ð²','Ð´ÐµÑ€Ð°Ð¶Ð½ÑŽ':'Ð´ÐµÑ€Ð°Ð¶Ð½Ñ',
    'Ð´ÐµÑ€Ð°Ð¶Ð½Ðµ':'Ð´ÐµÑ€Ð°Ð¶Ð½Ñ','Ð´ÐµÑ€Ð°Ð¶Ð½Ñ–':'Ð´ÐµÑ€Ð°Ð¶Ð½Ñ','ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÑƒ':'ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÐ°','Ð±Ð¾Ñ€Ð·Ð½Ñƒ':'Ð±Ð¾Ñ€Ð·Ð½Ð°','Ð¶Ð¼ÐµÑ€Ð¸Ð½ÐºÑƒ':'Ð¶Ð¼ÐµÑ€Ð¸Ð½ÐºÐ°','Ð»Ð¾ÑÐ¸Ð½Ñ–Ð²ÐºÑƒ':'Ð»Ð¾ÑÐ¸Ð½Ñ–Ð²ÐºÐ°',
    'Ð½Ñ–Ð¶Ð¸Ð½Ñƒ':'Ð½Ñ–Ð¶Ð¸Ð½','Ð½Ñ–Ð¶Ð¸Ð½Ð°':'Ð½Ñ–Ð¶Ð¸Ð½','Ð¼ÐµÐ¶Ð¾Ð²Ñƒ':'Ð¼ÐµÐ¶Ð¾Ð²Ð°','ÑÐ²ÑÑ‚Ð¾Ð³Ñ–Ñ€ÑÑŒÐºÑƒ':'ÑÐ²ÑÑ‚Ð¾Ð³Ñ–Ñ€ÑÑŒÐº'
}

# Add accusative / genitive / variant forms for reported missing settlements
UA_CITY_NORMALIZE.update({
    'Ð³Ð¾Ñ€Ð¾Ð´Ð½ÑŽ':'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ','Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ–':'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ','Ð³Ð¾Ñ€Ð¾Ð´Ð½Ðµ':'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ','Ð³Ð¾Ñ€Ð¾Ð´Ð½Ð¸':'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ',
    'ÐºÑ€Ð¾Ð»ÐµÐ²Ñ†Ñ':'ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ','ÐºÑ€Ð¾Ð»ÐµÐ²Ñ†Ñƒ':'ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ','ÐºÑ€Ð¾Ð»ÐµÐ²Ñ†Ðµ':'ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ',
    'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÑƒ':'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°','Ð´ÑƒÐ±Ð¾Ð²ÑÐ·Ñ–Ð²ÐºÑƒ':'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°','Ð´ÑƒÐ±Ð¾Ð²ÑÐ·Ð¾Ð²ÐºÑƒ':'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°','Ð´ÑƒÐ±Ð¾Ð²ÑÐ·Ð¾Ð²ÐºÐ°':'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°',
    'Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½Ð°':'Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½','Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½Ñ–':'Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½','Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½Ð¾Ð¼':'Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½'
    ,'Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ð°':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ðµ':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ñƒ':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ñ–':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ð¾Ð¼':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ñƒ':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²Ñƒ':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²','Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²Ð°':'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²'
    ,'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»Ñ':'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ','Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŽ':'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ','Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»Ñ–':'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ','Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ':'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ'
    ,'Ð±Ð¾ÑÑ€ÐºÐ¸':'Ð±Ð¾ÑÑ€ÐºÐ°','Ð±Ð¾ÑÑ€ÐºÑƒ':'Ð±Ð¾ÑÑ€ÐºÐ°','Ð±Ð¾ÑÑ€Ñ†Ñ–':'Ð±Ð¾ÑÑ€ÐºÐ°','Ð±Ð¾ÑÑ€ÐºÐ°':'Ð±Ð¾ÑÑ€ÐºÐ°'
    # Ð§ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹
    ,'ÑÐµÐ´Ð½Ñ–Ð²':'ÑÐµÐ´Ð½Ñ–Ð²','ÑÐµÐ´Ð½Ñ–Ð²Ñƒ':'ÑÐµÐ´Ð½Ñ–Ð²','ÑÐµÐ´Ð½Ñ–Ð²Ð°':'ÑÐµÐ´Ð½Ñ–Ð²'
    ,'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´Ñƒ':'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´','Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´Ñ–':'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´','Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ð¼':'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´'
    ,'Ð¼ÐµÐ½Ñƒ':'Ð¼ÐµÐ½Ð°','Ð¼ÐµÐ½Ñ–':'Ð¼ÐµÐ½Ð°','Ð¼ÐµÐ½Ð¾ÑŽ':'Ð¼ÐµÐ½Ð°'
    ,'Ð¼Ð°ÐºÐ°Ñ€Ð¾Ð²Ð°':'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²','Ð¼Ð°ÐºÐ°Ñ€Ð¾Ð²Ñ–':'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²','Ð¼Ð°ÐºÐ°Ñ€Ð¾Ð²':'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²','Ð¼Ð°ÐºÐ°Ñ€Ð¾Ð²Ñƒ':'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²','Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²':'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²'
    ,'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ¸':'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°','Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÑƒ':'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°','Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½Ñ†Ñ–':'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°','Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°':'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°'
    ,'ÐºÑ–Ð»Ñ–Ñ—':'ÐºÑ–Ð»Ñ–Ñ','ÐºÑ–Ð»Ñ–ÑŽ':'ÐºÑ–Ð»Ñ–Ñ','ÐºÑ–Ð»Ñ–Ñ”ÑŽ':'ÐºÑ–Ð»Ñ–Ñ','ÐºÑ–Ð»Ñ–Ñ':'ÐºÑ–Ð»Ñ–Ñ'
    ,'Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¾Ð³Ð¾':'Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¸Ð¹','Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¾Ð¼Ñƒ':'Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¸Ð¹','Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¸Ð¹':'Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¸Ð¹'
    ,'Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¾Ð³Ð¾':'Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹','Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¾Ð¼Ñƒ':'Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹','Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹':'Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹'
    ,'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¾Ð³Ð¾':'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹','Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ':'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹','Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹':'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹'
    ,'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¾Ð³Ð¾':'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¸Ð¹','Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¾Ð¼Ñƒ':'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¸Ð¹','Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¸Ð¹':'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¸Ð¹'
    ,'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¾Ð³Ð¾':'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹','Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¾Ð¼Ñƒ':'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹','Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹':'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹'
    ,'ÐºÑƒÑ†ÑƒÑ€ÑƒÐ±Ð°':'ÐºÑƒÑ†ÑƒÑ€ÑƒÐ±','Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÐºÑƒ':'Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÐºÐ°','Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÐºÐ¸':'Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÐºÐ°'
    # Ð¦Ð¸Ð±ÑƒÐ»Ñ–Ð² (Ð§ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð».) Ð¿Ð°Ð´ÐµÐ¶Ð½Ñ‹Ðµ / Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹
    ,'Ñ†Ð¸Ð±ÑƒÐ»ÐµÐ²Ð°':'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²','Ñ†Ð¸Ð±ÑƒÐ»ÐµÐ²Ñ–':'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²','Ñ†Ð¸Ð±ÑƒÐ»ÐµÐ²Ñƒ':'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²','Ñ†Ð¸Ð±ÑƒÐ»ÐµÐ²Ð¾Ð¼':'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²','Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²':'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²'
    # New accusative / variants for UAV course parsing batch
    ,'Ð±Ð¾Ñ€Ð·Ð½Ñƒ':'Ð±Ð¾Ñ€Ð·Ð½Ð°','Ñ†Ð°Ñ€Ð¸Ñ‡Ð°Ð½ÐºÑƒ':'Ñ†Ð°Ñ€Ð¸Ñ‡Ð°Ð½ÐºÐ°','Ð°Ð½Ð´Ñ€Ñ–Ñ—Ð²ÐºÑƒ':'Ð°Ð½Ð´Ñ€Ñ–Ñ—Ð²ÐºÐ°','ÑÐ¼Ð¿Ð¾Ð»ÑŒ':'ÑÐ¼Ð¿Ñ–Ð»ÑŒ','ÑÐ¼Ð¿Ð¾Ð»Ñ':'ÑÐ¼Ð¿Ñ–Ð»ÑŒ','ÑÐ¼Ð¿Ñ–Ð»ÑŒ':'ÑÐ¼Ð¿Ñ–Ð»ÑŒ','Ð´Ð¸Ð¼ÐµÑ€Ñƒ':'Ð´Ð¸Ð¼ÐµÑ€','Ñ‡Ð¾Ñ€Ð½Ð¾Ð±Ð¸Ð»ÑŽ':'Ñ‡Ð¾Ñ€Ð½Ð¾Ð±Ð¸Ð»ÑŒ'
    ,'Ð´Ð¼Ð¸Ñ‚Ñ€Ñ–Ð²ÐºÑƒ':'Ð´Ð¼Ð¸Ñ‚Ñ€Ñ–Ð²ÐºÐ°','Ð´Ð¼Ð¸Ñ‚Ñ€Ñ–Ð²ÐºÑƒ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°':'Ð´Ð¼Ð¸Ñ‚Ñ€Ñ–Ð²ÐºÐ°','Ð±ÐµÑ€ÐµÑÑ‚Ð¸Ð½':'Ð±ÐµÑ€ÐµÑÑ‚Ð¸Ð½'
    ,'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÑƒ':'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ°','Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ':'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ','Ð³Ð»Ð¾Ð±Ð¸Ð½Ñƒ':'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ','Ð³Ð»Ð¾Ð±Ð¸Ð½Ð¾Ð¼Ñƒ':'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ','Ð³Ð»Ð¾Ð±Ð¸Ð½Ð°':'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ'
    ,'ÐºÑ€Ð¸Ð½Ð¸Ñ‡ÐºÐ¸':'ÐºÑ€Ð¸Ð½Ð¸Ñ‡ÐºÐ¸','ÐºÑ€Ð¸Ð½Ð¸Ñ‡Ð¾Ðº':'ÐºÑ€Ð¸Ð½Ð¸Ñ‡ÐºÐ¸','ÑÐ¾Ð»Ð¾Ð½Ðµ':'ÑÐ¾Ð»Ð¾Ð½Ðµ','ÑÐ¾Ð»Ð¾Ð½Ð¾Ð³Ð¾':'ÑÐ¾Ð»Ð¾Ð½Ðµ','ÑÐ¾Ð»Ð¾Ð½Ð¾Ð¼Ñƒ':'ÑÐ¾Ð»Ð¾Ð½Ðµ'
    ,'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð»Ñ–Ð²ÐºÑƒ':'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°','ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð»Ñ–Ð²ÐºÑƒ':'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°','ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð»Ñ–Ð²ÐºÐ°':'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°'
    ,'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÑ–Ð¹ Ð´Ð¸Ð¼ÐµÑ€Ñ†Ñ–':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°','Ð²ÐµÐ»Ð¸ÐºÐ¾Ñ— Ð´Ð¸Ð¼ÐµÑ€ÐºÐ¸':'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°'
    ,'Ð±Ñ€ÑƒÑÐ¸Ð»Ñ–Ð²':'Ð±Ñ€ÑƒÑÐ¸Ð»Ñ–Ð²','Ð±Ñ€ÑƒÑÐ¸Ð»Ð¾Ð²Ð°':'Ð±Ñ€ÑƒÑÐ¸Ð»Ñ–Ð²','Ð±Ñ€ÑƒÑÐ¸Ð»Ð¾Ð²Ñ–':'Ð±Ñ€ÑƒÑÐ¸Ð»Ñ–Ð²'
    # New cities from napramok messages September 2025
    ,'Ð´ÐµÑÐ½Ñƒ':'Ð´ÐµÑÐ½Ð°','ÐºÑ–Ð¿Ñ‚Ñ–':'ÐºÑ–Ð¿Ñ‚Ñ–','Ñ–Ñ‡Ð½ÑŽ':'Ñ–Ñ‡Ð½Ñ','Ñ†Ð²Ñ–Ñ‚ÐºÐ¾Ð²Ðµ':'Ñ†Ð²Ñ–Ñ‚ÐºÐ¾Ð²Ðµ'
    ,'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ñ–':'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ñ–','Ð·Ð²ÑÐ³ÐµÐ»ÑŒ':'Ð·Ð²ÑÐ³ÐµÐ»ÑŒ','ÑÐ°Ñ…Ð½Ð¾Ð²Ñ‰Ð¸Ð½Ñƒ':'ÑÐ°Ñ…Ð½Ð¾Ð²Ñ‰Ð¸Ð½Ð°'
    ,'ÐºÐ°Ð¼Ê¼ÑÐ½ÑÑŒÐºÐµ':'ÐºÐ°Ð¼Ê¼ÑÐ½ÑÑŒÐºÐµ','Ð¿Ñ–Ñ‰Ð°Ð½Ð¸Ð¹ Ð±Ñ€Ñ–Ð´':'Ð¿Ñ–Ñ‰Ð°Ð½Ð¸Ð¹ Ð±Ñ€Ñ–Ð´','Ð±Ð¾Ð±Ñ€Ð¸Ð½ÐµÑ†ÑŒ':'Ð±Ð¾Ð±Ñ€Ð¸Ð½ÐµÑ†ÑŒ'
    ,'Ñ‚ÐµÐ½Ð´Ñ€Ñ–Ð²ÑÑŒÐºÑƒ ÐºÐ¾ÑÑƒ':'Ñ‚ÐµÐ½Ð´Ñ€Ñ–Ð²ÑÑŒÐºÐ° ÐºÐ¾ÑÐ°'
    # ÐžÐ´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    ,'Ð²Ð¸Ð»ÐºÐ¾Ð²Ð¾':'Ð²Ð¸Ð»ÐºÐ¾Ð²Ðµ','Ð²Ð¸Ð»ÐºÐ¾Ð²Ñƒ':'Ð²Ð¸Ð»ÐºÐ¾Ð²Ðµ'
    # Common accusative forms for major cities  
    ,'Ð¾Ð´ÐµÑÑƒ':'Ð¾Ð´ÐµÑÐ°','Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñƒ':'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°','ÑÑƒÐ¼Ð°Ð¼Ð¸':'ÑÑƒÐ¼Ð¸','ÑÑƒÐ¼Ñƒ':'ÑÑƒÐ¼Ð¸'
})
# Apostrophe-less fallback for Sloviansk
UA_CITY_NORMALIZE['ÑÐ»Ð¾Ð²ÑÐ½ÑÑŒÐº'] = "ÑÐ»Ð¾Ð²'ÑÐ½ÑÑŒÐº"

# Donetsk front city normalization (latin/ukr vowel variants)
UA_CITY_NORMALIZE['Ð»Ð¸Ð¼Ð°Ð½'] = 'Ð»Ñ–Ð¼Ð°Ð½'

# ---------------- Dynamic settlement name â†’ region map (from city_ukraine.json, no coords there) ---------------
NAME_REGION_MAP = {}

def _load_name_region_map():
    global NAME_REGION_MAP
    if NAME_REGION_MAP:
        return
    path = 'city_ukraine.json'
    if not os.path.exists(path):
        return
    try:
        with open(path,'r',encoding='utf-8') as f:
            data = json.load(f)
        added = 0
        for item in data:
            if not isinstance(item, dict):
                continue
            name = str(item.get('object_name') or '').strip().lower()
            region = str(item.get('region') or '').strip().title()
            if not name or len(name) < 2:
                continue
            # Skip obviously generic words
            if name in NAME_REGION_MAP:
                continue
            NAME_REGION_MAP[name] = region
            added += 1
        log.info(f"Loaded NAME_REGION_MAP entries: {added}")
    except Exception as e:
        log.warning(f"Failed load city_ukraine.json names: {e}")

_load_name_region_map()

# Fix problematic entries in NAME_REGION_MAP that cause wrong city resolution
# Remove incomplete city names that point to wrong regions
PROBLEMATIC_ENTRIES = [
    'ÐºÑ€Ð¸Ð²Ð¸Ð¹',     # Should be 'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³' not just 'ÐºÑ€Ð¸Ð²Ð¸Ð¹' -> causes wrong region lookup
    'ÑÑ‚Ð°Ñ€Ð¸Ð¹',     # Too generic, causes conflicts
    'Ð½Ð¾Ð²Ð°',       # Too generic
    'Ð²ÐµÐ»Ð¸ÐºÐ°',     # Too generic
    'Ð¼Ð°Ð»Ð°',       # Too generic
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ°',  # Conflicts with Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ when message clearly specifies region
]

for entry in PROBLEMATIC_ENTRIES:
    NAME_REGION_MAP.pop(entry, None)

def ensure_city_coords(name: str):
    """Return (lat,lng,approx_bool) for settlement, performing lazy geocoding.
    approx_bool True means we used oblast center fallback (low precision)."""
    if not name:
        return None
    n = name.strip().lower()
    
    # Quick check in existing coordinates first
    if n in CITY_COORDS:
        lat,lng = CITY_COORDS[n]; return (lat,lng,False)
    # Check if it's a direct oblast/region name
    if n in OBLAST_CENTERS:
        lat,lng = OBLAST_CENTERS[n]; return (lat,lng,True)
    if 'SETTLEMENTS_INDEX' in globals() and n in (globals().get('SETTLEMENTS_INDEX') or {}):
        lat,lng = globals()['SETTLEMENTS_INDEX'][n]; return (lat,lng,False)
    
    # Try SpaCy normalization for single city names (with limited context)
    if SPACY_AVAILABLE:
        try:
            # Create a simple test message to leverage SpaCy normalization
            # NOTE: This should only find exact matches for the specific city
            test_message = f"Ð½Ð° {name}"
            spacy_results = spacy_enhanced_geocoding(test_message)
            
            for result in spacy_results:
                if (result['coords'] and 
                    result['normalized'] == n and 
                    result['name'].lower() == n):  # Ensure exact name match
                    lat, lng = result['coords']
                    print(f"DEBUG SpaCy single city: Found {name} -> {result['normalized']} -> ({lat}, {lng})")
                    return (lat, lng, False)
                    
        except Exception as e:
            print(f"DEBUG SpaCy single city error: {e}")
            # Continue to existing logic
    
    region_hint = NAME_REGION_MAP.get(n)
    # Attempt precise geocode if API key
    if region_hint and OPENCAGE_API_KEY:
        q = f"{n} {region_hint} Ð£ÐºÑ€Ð°Ñ—Ð½Ð°"
        coords = geocode_opencage(q)
        if coords:
            # store in both for fast reuse
            CITY_COORDS[n] = coords
            try:
                if 'SETTLEMENTS_INDEX' in globals():
                    globals()['SETTLEMENTS_INDEX'][n] = coords
            except Exception:
                pass
            return (coords[0], coords[1], False)
    # Fallback: try geocode without region if key exists
    if OPENCAGE_API_KEY:
        coords = geocode_opencage(f"{n} Ð£ÐºÑ€Ð°Ñ—Ð½Ð°")
        if coords:
            CITY_COORDS[n] = coords
            try:
                if 'SETTLEMENTS_INDEX' in globals():
                    globals()['SETTLEMENTS_INDEX'][n] = coords
            except Exception:
                pass
            return (coords[0], coords[1], False)
    # Approximate fallback: oblast center (if region hint matches an oblast name substring)
    if region_hint:
        reg_low = region_hint.lower()
        for oblast_key, (olat, olng) in OBLAST_CENTERS.items():
            if oblast_key in reg_low:
                return (olat, olng, True)
    return None

def ensure_city_coords_with_message_context(name: str, message_text: str = ""):
    """Enhanced version that tries to extract oblast from message if city not found.
    Returns (lat,lng,approx_bool) - approx_bool True means used oblast fallback."""
    
    # PRIORITY: Try SpaCy first if available
    if SPACY_AVAILABLE and message_text:
        try:
            spacy_results = spacy_enhanced_geocoding(message_text)
            
            # Look for the specific city we're searching for
            name_lower = name.lower()
            for result in spacy_results:
                if (result['normalized'] == name_lower or 
                    result['name'].lower() == name_lower):
                    if result['coords']:
                        lat, lng = result['coords']
                        print(f"DEBUG SpaCy: Found {name} via SpaCy -> ({lat}, {lng})")
                        return (lat, lng, False)  # Not approximate since SpaCy found exact match
            
        except Exception as e:
            print(f"DEBUG SpaCy fallback error: {e}")
            # Continue to regex-based processing
    
    # FALLBACK: Original regex-based processing
    
    # First, if we have message text, try to extract oblast info and build specific city keys
    if message_text:
        message_lower = message_text.lower()
        
        # ENHANCED: Find the closest oblast to the specific city name
        city_pos = message_lower.find(name.lower())
        if city_pos != -1:
            # Look for oblast in close proximity to the city (within 100 characters before/after)
            start_pos = max(0, city_pos - 100)
            end_pos = min(len(message_lower), city_pos + len(name) + 100)
            context = message_lower[start_pos:end_pos]
            
            # Enhanced regional context detection - try parenthetical oblast first
            oblast_patterns = [
                # Parenthetical oblast: "(Oblast Ð¾Ð±Ð».)" - most specific
                r'\(([^)]+)\s+Ð¾Ð±Ð»\.\)',
                r'\(([^)]+)\s+Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ\)',
                # Oblast adjective forms: "Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»."
                r'\b([Ð°-ÑÑ—Ñ–Ñ”]+ÑÑŒÐºÐ°)\s+Ð¾Ð±Ð»(?:\.|Ð°ÑÑ‚ÑŒ)?\b',
                r'\b([Ð°-ÑÑ—Ñ–Ñ”]+Ñ†ÑŒÐºÐ°)\s+Ð¾Ð±Ð»(?:\.|Ð°ÑÑ‚ÑŒ)?\b', 
                # Regional names: "Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°", "Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°", etc.
                r'\b([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ð°)\b',
                r'\b([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ñ–)\b',
                r'\b([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ñƒ)\b',
                # Additional patterns for regional context
                r'\bÐ½Ð°\s+([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ñ–)\b',  # "Ð½Ð° Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ñ–"
                r'\bÑƒ\s+([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ñ–)\b',   # "Ñƒ Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ñ–"
                r'\bÐ²\s+([Ð°-ÑÑ—Ñ–Ñ”]+Ñ‰Ð¸Ð½Ñ–)\b',   # "Ð² Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ñ–"
            ]
            
            detected_oblast_key = None
            
            for pattern in oblast_patterns:
                matches = re.findall(pattern, context)  # Search in context, not full message
                for match in matches:
                    match = match.strip().lower()
                    
                    # Normalize regional names to nominative case AND adjective form
                    if match.endswith('Ñ‰Ð¸Ð½Ñ–'):
                        match = match[:-2] + 'Ð½Ð°'  # ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñ– -> ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°
                    elif match.endswith('Ñ‰Ð¸Ð½Ñƒ'):
                        match = match[:-2] + 'Ð½Ð°'  # ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñƒ -> ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°
                    
                    # Convert regional names to adjective forms for city lookup
                    regional_to_adjective = {
                        'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼ÑÑŒÐºÐ°',
                        'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°',
                        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°',
                        'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°',
                        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°',
                        'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°',
                        'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°',
                    }
                    
                    if match in regional_to_adjective:
                        match = regional_to_adjective[match]
                    
                    # Create possible city+oblast combinations to search
                    city_variants = [
                        f"{name.lower()}({match})",  # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°(ÑÑƒÐ¼ÑÑŒÐºÐ°)
                        f"{name.lower()} ({match})",  # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° (ÑÑƒÐ¼ÑÑŒÐºÐ°)
                        f"{name.lower()} {match}",
                        f"{name.lower()} {match} Ð¾Ð±Ð».",
                        f"{name.lower()} {match} Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ",
                    ]
                    
                    print(f"DEBUG: Checking variants for {name} with closest oblast {match}: {city_variants}")
                    
                    # Try to find coordinates using these specific combinations
                    for variant in city_variants:
                        if variant in CITY_COORDS:
                            coords = CITY_COORDS[variant]
                            print(f"DEBUG: Found exact match for '{variant}' -> {coords}")
                            return (coords[0], coords[1], False)
                        
                        # Also try SETTLEMENTS_INDEX if available
                        if 'SETTLEMENTS_INDEX' in globals():
                            settlements = globals().get('SETTLEMENTS_INDEX') or {}
                            if variant in settlements:
                                coords = settlements[variant]
                                print(f"DEBUG: Found settlement match for '{variant}' -> {coords}")
                                return (coords[0], coords[1], False)
                    
                    print(f"DEBUG: No exact match found for variants: {city_variants}")
                    
                    # Store oblast key for potential fallback
                    oblast_normalizations = {
                        'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».',
                        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».',
                        'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
                        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
                        'ÑÑƒÐ¼ÑÑŒÐºÐ°': 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
                        'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°',
                        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°',
                        'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°',
                        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°',
                        'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼ÑÑŒÐºÐ°',
                    }
                    
                    if match in oblast_normalizations:
                        detected_oblast_key = oblast_normalizations[match]
                    elif match in OBLAST_CENTERS:
                        detected_oblast_key = match
                    
                    # If we found a valid oblast, stop searching
                    if detected_oblast_key:
                        break
                
                # If we found oblast in this pattern, stop searching other patterns 
                if detected_oblast_key:
                    break
    
    # Second try: standard city lookup (without oblast context)
    result = ensure_city_coords(name)
    if result:
        return result
    
    # Third try: if we have oblast key, return oblast center as fallback
    if message_text and detected_oblast_key and detected_oblast_key in OBLAST_CENTERS:
        lat, lng = OBLAST_CENTERS[detected_oblast_key]
        print(f"DEBUG: Using oblast center fallback for {name}: {detected_oblast_key} -> ({lat}, {lng})")
        return (lat, lng, True)  # True indicates this is an oblast fallback
    
    return None

def normalize_ukrainian_toponym(lemmatized_name: str, original_text: str, grammatical_case: str = None) -> str:
    """
    Universal normalization for Ukrainian place names using linguistic patterns
    
    Args:
        lemmatized_name: SpaCy lemmatized form
        original_text: Original text from message
        grammatical_case: Grammatical case detected by SpaCy (Nom, Gen, Acc, etc.)
        
    Returns:
        Properly normalized toponym
    """
    
    # Rule 1: Special exceptions that need manual handling
    special_exceptions = {
        'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': 'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ',    # "Ð§ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ" wrongly lemmatized as adjective
        'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾': 'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ',   # Genitive case of Ð§ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ
        'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ': 'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ',   # Locative case of Ð§ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ
        'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐ¾Ð¼': 'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ',    # Instrumental case of Ð§ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ
        'Ð¾Ð»ÐµÐºÑÐ°Ð´Ñ€Ñ–Ñ': 'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ñ',    # Common typo/variant
    }
    
    if lemmatized_name in special_exceptions:
        fixed = special_exceptions[lemmatized_name]
        print(f"DEBUG normalize_toponym: Special exception '{lemmatized_name}' â†’ '{fixed}'")
        return fixed
    
    # Rule 2: Adjective endings â†’ City names (most common SpaCy error)
    adjective_to_city_patterns = [
        (r'(.+)ÑÑŒÐºÐ¸Ð¹$', r'\1ÑÑŒÐº'),      # Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹ â†’ Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐº
        (r'(.+)Ñ†ÑŒÐºÐ¸Ð¹$', r'\1Ñ†ÑŒÐº'),      # ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐºÐ¸Ð¹ â†’ ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº  
        (r'(.+)Ñ€ÑÑŒÐºÐ¸Ð¹$', r'\1Ñ€ÑÑŒÐº'),    # examples like "Ð¿ÐµÑ‚Ñ€Ñ–Ð²ÑÑŒÐºÐ¸Ð¹" â†’ "Ð¿ÐµÑ‚Ñ€Ñ–Ð²ÑÑŒÐº"
        (r'(.+)Ð½ÑÑŒÐºÐ¸Ð¹$', r'\1Ð½ÑÑŒÐº'),    # examples like "ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹" â†’ "ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐº"
        (r'(.+)Ð»ÑŒÑÑŒÐºÐ¸Ð¹$', r'\1Ð»ÑŒÑÑŒÐº'),  # examples like "ÐºÑ€Ð¸Ð²ÑÑŒÐºÐ¸Ð¹" â†’ "ÐºÑ€Ð¸Ð²ÑÑŒÐº"
    ]
    
    for pattern, replacement in adjective_to_city_patterns:
        import re
        if re.match(pattern, lemmatized_name):
            normalized = re.sub(pattern, replacement, lemmatized_name)
            print(f"DEBUG normalize_toponym: Adjective pattern '{lemmatized_name}' â†’ '{normalized}'")
            return normalized
    
    # Rule 3: Handle specific case forms that need different normalization
    case_specific_fixes = {
        # Genitive forms that should be nominative
        'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ð¸Ð¹': 'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ',          # "Ð—Ð°Ñ€Ñ–Ñ‡Ð½Ð¾Ð³Ð¾" (Gen) â†’ "Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ" (Nom)
        
        # Instrumental case normalization
        'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð¼': 'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐº',  # "Ð½Ð°Ð´ ÐÐ¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾Ð¼" (Ins) â†’ base form
        'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐºÐ¾Ð¼': 'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº',      # "Ð½Ð°Ð´ ÐšÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐºÐ¾Ð¼" (Ins) â†’ base form
        'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¾Ð¼': 'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐº',          # "Ð½Ð°Ð´ ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¾Ð¼" (Ins) â†’ base form
    }
    
    if lemmatized_name in case_specific_fixes:
        fixed = case_specific_fixes[lemmatized_name]
        print(f"DEBUG normalize_toponym: Case fix '{lemmatized_name}' â†’ '{fixed}'")
        return fixed
    
    # Rule 4: Handle endings that indicate feminine places
    feminine_place_patterns = [
        (r'(.+)Ñ–Ð²ÐºÐ°$', r'\1Ñ–Ð²ÐºÐ°'),      # Keep as is: Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°, Ð³ÑƒÑÐ°Ñ€Ñ–Ð²ÐºÐ°
        (r'(.+)ÐµÐ½ÐºÐ°$', r'\1ÐµÐ½ÐºÐ°'),      # Keep as is: ÑÐ°Ð²Ð¸Ð½ÐºÐ° (but handle special cases)
    ]
    
    # Special feminine cases that need fixing
    feminine_special_cases = {
        'ÑÐ°Ð²Ð¸Ð½ÐºÐ°': 'ÑÐ°Ð²Ð¸Ð½Ñ†Ñ–',  # This is actually "Ð¡Ð°Ð²Ð¸Ð½Ñ†Ñ–" wrongly lemmatized
    }
    
    if lemmatized_name in feminine_special_cases:
        fixed = feminine_special_cases[lemmatized_name]
        print(f"DEBUG normalize_toponym: Feminine fix '{lemmatized_name}' â†’ '{fixed}'")
        return fixed
    
    # Rule 5: Use original text pattern if lemmatization looks wrong
    original_lower = original_text.lower()
    
    # If original ends with typical city suffixes but lemma doesn't, prefer original pattern
    city_ending_patterns = [r'(.+)ÑÑŒÐº$', r'(.+)Ñ†ÑŒÐº$', r'(.+)Ñ–Ð²$', r'(.+)Ð¸Ð½Ðµ$', r'(.+)Ð½Ðµ$']
    lemma_is_adjective = any(lemmatized_name.endswith(ending) for ending in ['ÑÑŒÐºÐ¸Ð¹', 'Ñ†ÑŒÐºÐ¸Ð¹', 'Ñ€ÑÑŒÐºÐ¸Ð¹', 'Ð½ÑÑŒÐºÐ¸Ð¹'])
    
    import re
    for pattern in city_ending_patterns:
        if re.match(pattern, original_lower) and lemma_is_adjective:
            print(f"DEBUG normalize_toponym: Using original pattern '{original_lower}' over lemma '{lemmatized_name}'")
            return original_lower
    
    # Rule 6: Default - return the lemmatized form if no patterns match
    return lemmatized_name


def determine_regional_context(entity, doc, detected_regions, message_text):
    """
    Determine the correct regional context for a geographical entity
    based on its position in the text relative to regional headers
    
    Args:
        entity: SpaCy entity or PseudoEntity
        doc: SpaCy Doc object
        detected_regions: List of detected region names
        message_text: Original message text
        
    Returns:
        str: Most appropriate region name or None
    """
    if not detected_regions:
        return None
    
    if len(detected_regions) == 1:
        return detected_regions[0]
    
    # For multiple regions, find the closest preceding region header
    entity_start_char = entity.start_char if hasattr(entity, 'start_char') else 0
    
    # If entity doesn't have char positions, estimate from token positions
    if not hasattr(entity, 'start_char'):
        try:
            # Find token in doc by text matching
            for token in doc:
                if token.text == entity.text and token.i >= entity.start:
                    entity_start_char = token.idx
                    break
        except:
            entity_start_char = 0
    
    # Find all region positions in text
    region_positions = []
    message_lower = message_text.lower()
    
    region_patterns = {
        'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': ['ÑÑƒÐ¼Ñ‰Ð¸Ð½'],
        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½'],
        'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½'],
        'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': ['Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½'],
        'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': ['Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½'],
        'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': ['Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½'],
        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': ['Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½'],
        'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': ['ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½'],
        'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': ['Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½'],
        'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': ['Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½'],
        'Ð¾Ð´ÐµÑÑ‰Ð¸Ð½Ð°': ['Ð¾Ð´ÐµÑÑ‰Ð¸Ð½', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½'],
        'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°': ['Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐº'],
        'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½'],
        'Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½Ð°': ['Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½', 'Ð²Ð¾Ð»Ð¸Ð½ÑŒ'],
        'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': ['Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½'],
        'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°': ['Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½'],
        'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': ['Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½', 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ð¸Ð½'],
    }
    
    for region_name in detected_regions:
        patterns = region_patterns.get(region_name, [region_name])
        for pattern in patterns:
            pos = message_lower.find(pattern)
            if pos != -1:
                region_positions.append((pos, region_name))
                break
    
    # Sort by position
    region_positions.sort(key=lambda x: x[0])
    
    # Find the closest preceding region
    closest_region = None
    closest_distance = float('inf')
    
    for pos, region_name in region_positions:
        if pos <= entity_start_char:
            distance = entity_start_char - pos
            if distance < closest_distance:
                closest_distance = distance
                closest_region = region_name
    
    # If no preceding region found, use the first one
    result = closest_region if closest_region else detected_regions[0]
    
    print(f"DEBUG Regional context: Entity '{entity.text}' at char {entity_start_char} -> region '{result}'")
    print(f"DEBUG Region positions: {region_positions}")
    
    return result


def spacy_enhanced_geocoding(message_text: str, existing_city_coords: dict = None, 
                           existing_normalizer: dict = None) -> list:
    """
    Enhanced city extraction using SpaCy NLP for Ukrainian text with proper entity recognition
    
    Args:
        message_text: Original message text
        existing_city_coords: CITY_COORDS dict (defaults to global CITY_COORDS)
        existing_normalizer: UA_CITY_NORMALIZE dict (defaults to global UA_CITY_NORMALIZE)
        
    Returns:
        List of dicts with city information:
        {
            'name': str,           # Original city name from message
            'normalized': str,     # Normalized city name for lookup
            'coords': tuple,       # (lat, lng) coordinates if found
            'region': str,         # Detected region if any
            'confidence': float,   # Confidence score 0.0-1.0
            'source': str,         # Detection method used
            'case': str           # Grammatical case if detected
        }
    """
    if not SPACY_AVAILABLE:
        return []
    
    if existing_city_coords is None:
        existing_city_coords = CITY_COORDS
    if existing_normalizer is None:
        existing_normalizer = UA_CITY_NORMALIZE
        
    results = []
    
    try:
        doc = nlp(message_text)
        
        # Extract regions first for context
        detected_regions = []
        region_patterns = {
            'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': ['ÑÑƒÐ¼Ñ‰Ð¸Ð½', 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½', 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': ['Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': ['Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½', 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': ['Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½', 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': ['Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': ['ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½', 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': ['Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½', 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': ['Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½', 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð¾Ð´ÐµÑÑ‰Ð¸Ð½Ð°': ['Ð¾Ð´ÐµÑÑ‰Ð¸Ð½', 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°'],
            'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°': ['Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐº', 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°': ['Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½', 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½Ð°': ['Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½', 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»', 'Ð²Ð¾Ð»Ð¸Ð½ÑŒ'],
            'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': ['Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½', 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°': ['Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½', 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»'],
            'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': ['Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½', 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»', 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ð¸Ð½'],
        }
        
        message_lower = message_text.lower()
        for region_name, patterns in region_patterns.items():
            if any(pattern in message_lower for pattern in patterns):
                detected_regions.append(region_name)
        
        print(f"DEBUG SpaCy NLP: Processing text: '{message_text}'")
        print(f"DEBUG SpaCy NLP: Detected regions: {detected_regions}")
        
        # Process named entities from SpaCy NER - this is the proper NLP approach
        geographical_entities = []
        for ent in doc.ents:
            if ent.label_ in ['LOC', 'GPE']:  # Location, Geopolitical entity
                print(f"DEBUG SpaCy NLP: Found entity '{ent.text}' with label '{ent.label_}' confidence: {ent._.score if hasattr(ent, '_') and hasattr(ent._, 'score') else 'N/A'}")
                geographical_entities.append(ent)
        
        # Also look for proper nouns that might be geographical names
        for token in doc:
            if (token.pos_ == 'PROPN' and 
                not any(ent.start <= token.i < ent.end for ent in geographical_entities) and
                len(token.text) > 2):  # Skip short tokens
                print(f"DEBUG SpaCy NLP: Found additional PROPN candidate: '{token.text}'")
                # Create a pseudo-entity for processing
                class PseudoEntity:
                    def __init__(self, token):
                        self.text = token.text
                        self.start = token.i
                        self.end = token.i + 1  # Add missing end attribute
                        self.label_ = 'PROPN_CANDIDATE'
                geographical_entities.append(PseudoEntity(token))
        
        for ent in geographical_entities:
            entity_text = ent.text.lower()
            
            # Skip if this is a region (already processed)
            is_region = False
            for region_name, patterns in region_patterns.items():
                if any(pattern in entity_text for pattern in patterns):
                    is_region = True
                    break
            
            if not is_region:
                # Get morphological info
                if hasattr(ent, 'start'):
                    token = doc[ent.start]
                else:
                    # For pseudo-entities, find the token
                    token = next((t for t in doc if t.text.lower() == entity_text), None)
                    if not token:
                        continue
                
                case_info = None
                if hasattr(token, 'morph') and token.morph:
                    morph_dict = token.morph.to_dict()
                    case_info = morph_dict.get('Case', None)
                
                # Normalize city name using lemma - this is proper NLP morphological analysis
                normalized_name = token.lemma_.lower() if token.lemma_ and token.lemma_ != '-PRON-' else entity_text
                
                # For multi-word geographical entities, handle them specially
                if len(ent.text.split()) > 1:
                    # For multi-word entities, use custom normalization
                    entity_lower = ent.text.lower()
                    multi_word_fixes = {
                        'ÐºÑ€Ð¸Ð²Ð¾Ð¼Ñƒ Ñ€Ð¾Ð³Ñƒ': 'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³',
                        'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³': 'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³',
                        'ÐºÑ€Ð¸Ð²Ð¾Ð³Ð¾ Ñ€Ð¾Ð³Ñƒ': 'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³',
                        'Ð½Ð¾Ð²Ð¸Ð¹ Ð±ÑƒÐ³': 'Ð½Ð¾Ð²Ð¸Ð¹ Ð±ÑƒÐ³',
                        'Ð±Ñ–Ð»Ð¸Ð¹ ÐºÐ°Ð¼Ñ–Ð½ÑŒ': 'Ð±Ñ–Ð»Ð¸Ð¹ ÐºÐ°Ð¼Ñ–Ð½ÑŒ',
                        'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐº': 'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐº',  # formerly ÐºÑ€Ð°ÑÐ½Ð¾Ð°Ñ€Ð¼ÐµÐ¹ÑÐº
                    }
                    for pattern, canonical in multi_word_fixes.items():
                        if pattern in entity_lower:
                            normalized_name = canonical
                            break
                    else:
                        # If no special case, try to reconstruct from lemmas
                        words = []
                        for i in range(ent.start, ent.end):
                            word_token = doc[i]
                            word_lemma = word_token.lemma_.lower() if word_token.lemma_ and word_token.lemma_ != '-PRON-' else word_token.text.lower()
                            words.append(word_lemma)
                        normalized_name = ' '.join(words)
                
                print(f"DEBUG SpaCy NLP: Entity '{ent.text}' -> normalized: '{normalized_name}', case: {case_info}")
                
                # Apply intelligent normalization for Ukrainian place names
                normalized_name = normalize_ukrainian_toponym(normalized_name, ent.text, case_info)
                
                # Apply existing normalization rules
                if normalized_name in existing_normalizer:
                    normalized_name = existing_normalizer[normalized_name]
                
                # Determine the most appropriate regional context for this entity
                region_context = determine_regional_context(ent, doc, detected_regions, message_text)
                
                # Look up coordinates using enhanced lookup with Nominatim fallback
                coords = get_coordinates_enhanced(normalized_name, region_context, message_text)
                
                # Determine confidence based on source
                confidence = 0.9 if ent.label_ in ['LOC', 'GPE'] else 0.7  # Higher for NER entities
                source = 'spacy_ner' if ent.label_ in ['LOC', 'GPE'] else 'spacy_propn'
                
                result = {
                    'name': ent.text,
                    'normalized': normalized_name,
                    'coords': coords,
                    'region': region_context,
                    'confidence': confidence,
                    'source': source,
                    'case': case_info
                }
                results.append(result)
                print(f"DEBUG SpaCy NLP: Added result: {result}")
        
        # Additional pattern-based extraction for missed entities (as fallback)
        preposition_patterns = ['Ð½Ð°', 'Ð¿Ð¾Ð²Ð·', 'Ñ‡ÐµÑ€ÐµÐ·', 'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', 'Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ']
        
        for i, token in enumerate(doc):
            # Simple prepositions
            if token.text.lower() in preposition_patterns[:3]:  # Ð½Ð°, Ð¿Ð¾Ð²Ð·, Ñ‡ÐµÑ€ÐµÐ·
                city_info = _extract_city_after_preposition_spacy(doc, i, detected_regions, 
                                                                existing_city_coords, existing_normalizer)
                if city_info:
                    results.append(city_info)
            
            # Direction patterns
            elif (token.text.lower() == 'Ñƒ' and i + 1 < len(doc) and 
                  doc[i + 1].text.lower() == 'Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ'):
                city_info = _extract_city_after_preposition_spacy(doc, i + 1, detected_regions,
                                                                existing_city_coords, existing_normalizer)
                if city_info:
                    results.append(city_info)
        
        # Remove duplicates while preserving order
        unique_results = []
        seen_cities = set()
        for result in results:
            city_key = result['normalized']
            if city_key not in seen_cities:
                seen_cities.add(city_key)
                unique_results.append(result)
        
        print(f"DEBUG SpaCy NLP: Final results: {unique_results}")
        return unique_results
        
    except Exception as e:
        print(f"SpaCy processing error: {e}")
        return []

def _find_coordinates_multiple_formats(city_name: str, detected_regions: list, existing_city_coords: dict) -> tuple:
    """
    Try to find coordinates using multiple key formats
    Returns coordinates tuple (lat, lng) or None
    """
    # List of key formats to try
    search_keys = [city_name]
    
    # Add regional variants if regions detected
    if detected_regions:
        for region in detected_regions:
            # Convert region names to adjective forms for coordinate lookup
            region_adj_map = {
                'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼ÑÑŒÐºÐ°',
                'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°', 
                'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°',
                'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°',
                'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°',
                'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°',
                'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°',
                'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°',
                'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ°',
                'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ°'
            }
            
            region_adj = region_adj_map.get(region, region)
            
            # Try various formats
            search_keys.extend([
                f"{city_name} {region}",           # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°
                f"{city_name}({region_adj})",      # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°(ÑÑƒÐ¼ÑÑŒÐºÐ°)
                f"{city_name} ({region_adj})",     # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° (ÑÑƒÐ¼ÑÑŒÐºÐ°)  
                f"{city_name} {region_adj}",       # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° ÑÑƒÐ¼ÑÑŒÐºÐ°
                f"{city_name} {region_adj} Ð¾Ð±Ð»",   # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»
                f"{city_name} {region_adj} Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ" # Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
            ])
    
    # Try each key format
    for key in search_keys:
        coords = existing_city_coords.get(key)
        if coords:
            print(f"DEBUG SpaCy coord lookup: Found '{city_name}' using key '{key}' -> {coords}")
            return coords
    
    print(f"DEBUG SpaCy coord lookup: No coordinates found for '{city_name}' (tried {len(search_keys)} keys)")
    return None

def get_coordinates_enhanced(city_name: str, region: str = None, context: str = "") -> tuple:
    """
    Enhanced coordinate lookup with Nominatim API fallback
    
    Args:
        city_name: Name of the settlement
        region: Optional region specification
        context: Context for military priority (e.g., "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°")
        
    Returns:
        Tuple of (latitude, longitude) or None if not found
    """
    
    # First try local database with regional context prioritization
    context_lower = context.lower()
    
    # Handle Ð—Ð°Ñ€Ñ–Ñ‡Ð½Ðµ disambiguation based on context
    if city_name == 'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ':
        if any(keyword in context_lower for keyword in ['Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾', 'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹', 'Ð±Ð¿Ð»Ð°']):
            # For Dnipropetrovska oblast contexts
            coords = DNIPRO_CITY_COORDS.get('Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°')
            if coords:
                print(f"DEBUG Enhanced coord lookup: Found '{city_name}' using Dnipropetrovska context -> {coords}")
                return coords
        elif any(keyword in context_lower for keyword in ['Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°', 'Ñ€Ñ–Ð²Ð½Ðµ']):
            # For Rivne oblast contexts  
            coords = (51.2167, 26.0833)
            print(f"DEBUG Enhanced coord lookup: Found '{city_name}' using Rivne context -> {coords}")
            return coords
    
    # Handle regional prefixes in context
    regional_indicators = {
        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°',
        'ÐºÐ¸Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 
        'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°',
        'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°',
        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°',
        'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼ÑÑŒÐºÐ°'
    }
    
    detected_region = None
    for indicator, region_name in regional_indicators.items():
        if indicator in context_lower:
            detected_region = region_name
            break
    
    # If region detected from context but not passed as parameter, use detected
    if detected_region and not region:
        region = detected_region
        print(f"DEBUG Enhanced coord lookup: Detected region '{region}' from context for '{city_name}'")
    
    # Handle specific directional contexts (e.g., "Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ Ð§ÐµÑ€Ð½Ñ–Ð³Ð¾Ð²Ð°")
    if 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ð¾Ð²' in context_lower and any(direction in context_lower for direction in ['Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½', 'Ð¿Ñ–Ð²Ð½', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡']):
        if city_name == 'Ð»ÑŽÐ±ÐµÑ‡':
            # Ð›ÑŽÐ±ÐµÑ‡ Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ Ð§ÐµÑ€Ð½Ñ–Ð³Ð¾Ð²Ð°
            coords = (51.4961, 30.2675)  # ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ– ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ð¸ Ð›ÑŽÐ±ÐµÑ‡Ð°
            print(f"DEBUG Enhanced coord lookup: Found '{city_name}' using directional context north of Chernigiv -> {coords}")
            return coords
    
    # PRIORITIZE NOMINATIM API when region is specified
    if region and NOMINATIM_AVAILABLE:
        # Normalize region name for Nominatim API
        normalized_region = region.lower()
        
        # Convert regional nicknames to standard oblast names
        region_mappings = {
            'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 
            'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ': 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ‰Ð¸Ð½Ð°': 'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð°': 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°': 'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½Ð°': 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚Ñ': 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð±ÑƒÐºÐ¾Ð²Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°': 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
            'Ð¾Ð´ÐµÑÑ‰Ð¸Ð½Ð°': 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
        }
        
        # Apply mapping if available
        nominatim_region = region_mappings.get(normalized_region, region)
        
        print(f"DEBUG Enhanced coord lookup: Trying Nominatim API first for '{city_name}' in {nominatim_region} (from {region})")
        coords = get_coordinates_nominatim(city_name, nominatim_region)
        if coords:
            print(f"DEBUG Enhanced coord lookup: Nominatim found '{city_name}' in {nominatim_region} -> {coords}")
            # Cache the result in local database for future use
            cache_key = f"{city_name} {region}"
            CITY_COORDS[cache_key] = coords
            return coords
        else:
            print(f"DEBUG Enhanced coord lookup: Nominatim could not find '{city_name}' in {nominatim_region}")
    
    # Try basic local lookup (only if no region specified or Nominatim failed)
    coords = CITY_COORDS.get(city_name)
    if coords:
        print(f"DEBUG Enhanced coord lookup: Found '{city_name}' in local database -> {coords}")
        return coords
    
    # Try settlements index if available
    if 'SETTLEMENTS_INDEX' in globals() and globals().get('SETTLEMENTS_INDEX'):
        coords = globals()['SETTLEMENTS_INDEX'].get(city_name)
        if coords:
            print(f"DEBUG Enhanced coord lookup: Found '{city_name}' in settlements index -> {coords}")
            return coords
    
    # Try with region specification in local database
    if region:
        # Normalize region name
        region_lower = region.lower().replace('Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', '').replace('Ð¾Ð±Ð».', '').replace('Ð¾Ð±Ð»', '').strip()
        search_keys = [
            f"{city_name} {region_lower}",
            f"{city_name} {region_lower} Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ",
            f"{city_name} {region_lower} Ð¾Ð±Ð»",
            f"{city_name} ({region_lower})",
            f"{city_name} ({region})"
        ]
        
        for key in search_keys:
            coords = CITY_COORDS.get(key)
            if coords:
                print(f"DEBUG Enhanced coord lookup: Found '{city_name}' using regional key '{key}' -> {coords}")
                return coords
    
    # Fallback to Nominatim API for precise geocoding
    if NOMINATIM_AVAILABLE:
        print(f"DEBUG Enhanced coord lookup: Trying Nominatim API for '{city_name}'" + (f" in {region}" if region else ""))
        coords = get_coordinates_nominatim(city_name, region)
        if coords:
            print(f"DEBUG Enhanced coord lookup: Nominatim found '{city_name}' -> {coords}")
            # Cache the result in local database for future use
            cache_key = f"{city_name}" + (f" {region}" if region else "")
            CITY_COORDS[cache_key] = coords
            return coords
        else:
            print(f"DEBUG Enhanced coord lookup: Nominatim could not find '{city_name}'")
    
    print(f"DEBUG Enhanced coord lookup: No coordinates found for '{city_name}' anywhere")
    return None

def get_coordinates_context_aware(text: str) -> tuple:
    """
    Context-aware coordinate lookup using intelligent text analysis
    
    Args:
        text: Full message text for context analysis
        
    Returns:
        Tuple of (latitude, longitude, target_city_name) or None if not found
    """
    
    if not CONTEXT_GEOCODER_AVAILABLE:
        print("DEBUG Context geocoder: Not available")
        return None
    
    print(f"DEBUG Context geocoder: Analyzing text: '{text}'")
    
    # Get prioritized geocoding candidates
    candidates = get_context_aware_geocoding(text)
    
    if not candidates:
        print("DEBUG Context geocoder: No candidates found")
        return None
    
    print(f"DEBUG Context geocoder: Found {len(candidates)} candidates: {candidates}")
    
    # Try each candidate in order of confidence
    for city_name, region, confidence in candidates:
        # Skip obviously invalid candidates
        if len(city_name) < 2 or city_name in ['-', 'Ð½Ð°Ð´', 'Ð½Ð°', 'Ñƒ', 'Ð²', 'Ð´Ð¾', 'Ð¿Ñ–Ð´', 'Ð±Ñ–Ð»Ñ', 'Ð°', 'ÐºÑƒÑ€ÑÐ¾Ð¼']:
            continue
            
        print(f"DEBUG Context geocoder: Trying candidate '{city_name}' (region: {region}, confidence: {confidence})")
        
        # Use enhanced coordinate lookup
        coords = get_coordinates_enhanced(city_name, region=region, context=text)
        
        if coords:
            print(f"DEBUG Context geocoder: SUCCESS - Found '{city_name}' -> {coords}")
            return coords[0], coords[1], city_name
    
    print("DEBUG Context geocoder: No valid coordinates found for any candidate")
    return None

def _extract_city_after_preposition_spacy(doc, prep_index: int, detected_regions: list,
                                        existing_city_coords: dict, existing_normalizer: dict) -> dict:
    """Extract city name after preposition using SpaCy tokens"""
    if prep_index + 1 >= len(doc):
        return None
    
    # Collect potential city tokens (proper nouns, nouns, adjectives)
    city_tokens = []
    start_idx = prep_index + 1
    
    for i in range(start_idx, min(start_idx + 3, len(doc))):  # Max 3 words
        token = doc[i]
        if token.pos_ in ['PROPN', 'NOUN', 'ADJ'] or token.text == '-':
            city_tokens.append(token)
        else:
            break
    
    if not city_tokens:
        return None
    
    # Build city name
    city_name = ' '.join(token.text for token in city_tokens)
    
    # Get morphological info from the main token (usually the first one)
    main_token = city_tokens[0]
    case_info = None
    if hasattr(main_token, 'morph') and main_token.morph:
        morph_dict = main_token.morph.to_dict()
        case_info = morph_dict.get('Case', None)
    
    # Normalize using lemma
    normalized_name = main_token.lemma_ if main_token.lemma_ != city_name.lower() else city_name.lower()
    
    # Apply intelligent normalization for Ukrainian place names
    normalized_name = normalize_ukrainian_toponym(normalized_name, city_name, case_info)
    
    # Apply existing normalization rules
    if normalized_name in existing_normalizer:
        normalized_name = existing_normalizer[normalized_name]
    
    # Look up coordinates using enhanced lookup with Nominatim fallback  
    region_context = detected_regions[0] if detected_regions else None
    coords = get_coordinates_enhanced(normalized_name, region_context, ' '.join(token.text for token in doc))
    
    return {
        'name': city_name,
        'normalized': normalized_name,
        'coords': coords,
        'region': detected_regions[0] if detected_regions else None,
        'confidence': 0.7,  # Medium confidence for pattern-based
        'source': 'spacy_pattern',
        'case': case_info
    }

# Consolidated static fallback coordinates
CITY_COORDS = {
        # Core cities
        'ÐºÐ¸Ñ—Ð²': (50.4501, 30.5234), 'Ñ…Ð°Ñ€ÐºÑ–Ð²': (49.9935, 36.2304), 'Ð¾Ð´ÐµÑÐ°': (46.4825, 30.7233), 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾': (48.4647, 35.0462),
        'Ð»ÑŒÐ²Ñ–Ð²': (49.8397, 24.0297), 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ': (47.8388, 35.1396), 'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ': (49.2331, 28.4682), 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²': (46.9750, 31.9946),
        'Ð¼Ð°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ': (47.0971, 37.5434), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°': (49.5883, 34.5514), 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²': (51.4982, 31.2893), 'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸': (49.4444, 32.0598),
        'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€': (50.2547, 28.6587), 'ÑÑƒÐ¼Ð¸': (50.9077, 34.7981), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (49.4229, 26.9871), 'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–': (48.2921, 25.9358),
    
    # Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - ÑƒÑÑ– Ð¾ÑÐ½Ð¾Ð²Ð½Ñ– Ð¼Ñ–ÑÑ‚Ð°
    'Ð¾Ð²Ñ€ÑƒÑ‡': (51.3244, 28.8006), 'ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½ÑŒ': (50.9550, 28.6336), 'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹': (50.5833, 27.6167), 
    'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²': (49.8978, 28.6011), 'Ð·Ð²ÑÐ³ÐµÐ»ÑŒ': (50.5833, 27.6167), 'Ð¼Ð°Ð»Ð¸Ð½': (50.7726, 29.2360), 
    'Ñ€Ð°Ð´Ð¾Ð¼Ð¸ÑˆÐ»ÑŒ': (50.4972, 29.2292), 'Ñ‡ÐµÑ€Ð½ÑÑ…Ñ–Ð²': (50.4583, 28.8500), 'Ð±Ð°Ñ€Ð°Ð½Ñ–Ð²ÐºÐ°': (50.3000, 27.6667),
    'Ð¿Ð¾Ð¿Ñ–Ð»ÑŒÐ½Ñ': (49.9333, 28.4167), 'Ñ”Ð¼Ñ–Ð»ÑŒÑ‡Ð¸Ð½Ðµ': (50.8667, 28.8500), 'Ð¾Ð»ÐµÐ²ÑÑŒÐº': (51.2167, 27.6667),
    'Ð»ÑƒÐ³Ð¸Ð½Ð¸': (50.9333, 27.2667), 'Ñ‡ÑƒÐ´nÑ–Ð²': (50.0500, 28.1167), 'Ð°Ð½Ð´Ñ€ÑƒÑˆÑ–Ð²ÐºÐ°': (50.0833, 29.8000),
    'Ñ€Ð¾Ð¼Ð°Ð½Ñ–Ð²': (50.1500, 28.2667), 'Ñ€ÑƒÐ¶Ð¸Ð½': (49.6333, 28.6000), 'Ð²Ð¾Ð»Ð¾Ð´Ð°Ñ€ÑÑŒÐº-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹': (50.5500, 28.3833),
    'ÐºÐ¾Ñ€Ð¾ÑÑ‚Ð¸ÑˆÑ–Ð²': (50.3167, 29.0333), 'Ð½Ð°Ñ€Ð¾Ð´Ð¸Ñ‡Ñ–': (51.0583, 29.1167), 'Ñ–Ð²Ð°Ð½ÐºÑ–Ð²ÐµÑ†ÑŒ': (50.1333, 29.3333),
    'Ð»ÑŽÐ±Ð°Ñ€': (49.9167, 27.7333), 'Ð²Ð¸ÑÐ¾ÐºÐµ': (51.1000, 28.1000), 'Ñ‡Ð¾Ñ€Ð½Ð¾Ð±Ð¸Ð»ÑŒ': (51.2768, 30.2219),
    'Ð¿Ð¾Ð»Ñ–ÑÑŒÐºÐµ': (51.1833, 29.5000),
    
    # Ð¤Ð¾Ñ€Ð¼Ð¸ Ð²Ñ–Ð´Ð¼Ñ–Ð½ÐºÑ–Ð² Ð´Ð»Ñ ÐžÐ²Ñ€ÑƒÑ‡Ð°
    'Ð¾Ð²Ñ€ÑƒÑ‡Ñ–': (51.3244, 28.8006), 'Ð¾Ð²Ñ€ÑƒÑ‡Ñƒ': (51.3244, 28.8006), 'Ð¾Ð²Ñ€ÑƒÑ‡ÐµÐ¼': (51.3244, 28.8006),
    'Ð¾Ð²Ñ€ÑƒÑ‡ÐµÐ¼': (51.3244, 28.8006), 'Ð¾Ð²Ñ€ÑƒÑ‡Ð°': (51.3244, 28.8006),
    
    # Ð†Ð½ÑˆÑ– Ñ„Ð¾Ñ€Ð¼Ð¸ Ð´Ð»Ñ Ð¼Ñ–ÑÑ‚ Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð¼Ð°Ð»Ð¸Ð½Ñ–': (50.7726, 29.2360), 'Ð¼Ð°Ð»Ð¸Ð½Ñƒ': (50.7726, 29.2360), 'Ð¼Ð°Ð»Ð¸Ð½Ð¾Ð¼': (50.7726, 29.2360),
    'ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½Ñ–': (50.9550, 28.6336), 'ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½Ñƒ': (50.9550, 28.6336), 'ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½ÐµÐ¼': (50.9550, 28.6336),
    'Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ñ–': (49.8978, 28.6011), 'Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ñƒ': (49.8978, 28.6011), 'Ð±ÐµÑ€Ð´Ð¸Ñ‡ÐµÐ²Ð¾Ð¼': (49.8978, 28.6011),
    'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ–-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¾Ð¼Ñƒ': (50.5833, 27.6167), 'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¾Ð¼Ñƒ': (50.5833, 27.6167),
    
    # Ð‘ÐµÑ€ÑˆÐ°Ð´ÑŒ - Ñ€Ð°Ð¹Ñ†ÐµÐ½Ñ‚Ñ€ Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŒ': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´Ñ–': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŽ': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŒÑŽ': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÐµÐ¹': (48.3667, 29.5167),
    
    # Added per user report (Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð» alert should map): ÐšÐ¾ÑÑ‚ÑÐ½Ñ‚Ð¸Ð½Ñ–Ð²ÐºÐ° (Donetsk Obl.)
    'ÐºÐ¾ÑÑ‚ÑÐ½Ñ‚Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.5277, 37.7050),
    # Mezhova (Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».) to avoid fallback to Dnipro
    'Ð¼ÐµÐ¶Ð¾Ð²Ð°': (48.2583, 36.7363),
    # Sviatohirsk (Ð¡Ð²ÑÑ‚Ð¾Ð³Ñ–Ñ€ÑÑŒÐº) Donetsk Oblast
    'ÑÐ²ÑÑ‚Ð¾Ð³Ñ–Ñ€ÑÑŒÐº': (49.0339, 37.5663),
    # Antonivka (Kherson urban-type settlement, user report for UAV threat)
    'Ð°Ð½Ñ‚Ð¾Ð½Ñ–Ð²ÐºÐ°': (46.6925, 32.7186),
    # Alexandria (Kirovohrad Oblast) - avoid confusion with other cities named Alexandria
    'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ñ': (48.8033, 33.1147),
    # Vilshany (Kirovohrad Oblast) - separate from Vilshanka in other regions  
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð¸': (48.4667, 32.2667),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ð¼': (48.4667, 32.2667),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ñ…': (48.4667, 32.2667),
    # Baturyn (Chernihiv Obl.) for directional course reports
    'Ð±Ð°Ñ‚ÑƒÑ€Ð¸Ð½': (51.3450, 32.8761),
        'Ñ€Ñ–Ð²Ð½Ðµ': (50.6199, 26.2516), 'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº': (48.9226, 24.7111), 'Ð»ÑƒÑ†ÑŒÐº': (50.7472, 25.3254), 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ': (49.5535, 25.5948),
        'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´': (48.6208, 22.2879), 'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (48.5079, 32.2623), 'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº': (49.0670, 33.4204), 'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº': (48.7389, 37.5848),
        'Ð¼ÐµÐ»Ñ–Ñ‚Ð¾Ð¿Ð¾Ð»ÑŒ': (46.8489, 35.3650), 'Ð±ÐµÑ€Ð´ÑÐ½ÑÑŒÐº': (46.7553, 36.7885), 'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´': (48.5350, 35.8700), 'Ð½Ñ–ÐºÐ¾Ð¿Ð¾Ð»ÑŒ': (47.5667, 34.4061),
        'Ð¼Ð°Ñ€Ð³Ð°Ð½ÐµÑ†ÑŒ': (47.6433, 34.6289), 'Ñ…ÐµÑ€ÑÐ¾Ð½': (46.6350, 32.6169),
        'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ°': (46.64, 32.88),  # Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
        'Ñ‡Ð¾Ñ€Ð½Ð¾Ð±Ð°Ñ—Ð²ÐºÐ°': (46.6964, 32.5469),  # Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    
    # ÐÐµÐ´Ð¾ÑÑ‚Ð°ÑŽÑ‰Ð¸Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð¸Ð· UAV ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ (ÑÐµÐ½Ñ‚ÑÐ±Ñ€ÑŒ 2025)
    'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ': (51.2167, 26.0833),      # Ð Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'ÑÐµÐ½ÐºÐµÐ²Ð¸Ñ‡Ñ–Ð²ÐºÐ°': (51.5667, 25.8333), # Ð’Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'Ð³Ð¾Ð»Ð¾Ð±Ð¸': (50.7833, 25.2167),       # Ð’Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    
    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð¸Ð· UAV ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ (ÑÐµÐ½Ñ‚ÑÐ±Ñ€ÑŒ 2025)
    'ÐºÐ¾Ñ€Ð½Ð¸Ð½': (50.9167, 29.1167),       # Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ, ÐœÐ°Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½
    'ÐºÐ¾Ñ€Ð½Ð¸Ð½Ñƒ': (50.9167, 29.1167),
    'ÐºÐ¾Ñ€Ð½Ð¸Ð½Ð¾Ð¼': (50.9167, 29.1167),
    'ÐºÐ¾Ñ€Ð½Ð¸Ð½Ð°': (50.9167, 29.1167),
    'ÑƒÑÑ‚Ð¸Ð½Ñ–Ð²ÐºÐ°': (50.7481, 29.0028),    # Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ, ÐœÐ°Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½
    'ÑƒÑÑ‚Ð¸Ð½Ñ–Ð²Ñ†Ñ–': (50.7481, 29.0028),
    'ÑƒÑÑ‚Ð¸Ð½Ñ–Ð²ÐºÑƒ': (50.7481, 29.0028),
    'ÑƒÑÑ‚Ð¸Ð½Ñ–Ð²ÐºÐ¾ÑŽ': (50.7481, 29.0028),
    'Ð´Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ñ–Ñ€': (50.2053, 24.4239),    # Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ, ÐµÐ½ÐµÑ€Ð³ÐµÑ‚Ð¸Ñ‡Ð½Ð¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€
    'Ð´Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ð¾Ñ€Ñ–': (50.2053, 24.4239),
    'Ð´Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ð¾Ñ€Ñƒ': (50.2053, 24.4239),
    'Ð´Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ð¾Ñ€Ð¾Ð¼': (50.2053, 24.4239),
    'Ð´Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ð¾Ñ€Ð°': (50.2053, 24.4239),
    'ÑÐ»Ð¾Ð²\'ÑÐ½ÑÑŒÐº': (48.8417, 37.5983), 'Ð´Ñ€ÑƒÐ¶ÐºÑ–Ð²ÐºÐ°': (48.6203, 37.5263),
    # Fallback key without apostrophe (some sources strip it)
    'ÑÐ»Ð¾Ð²ÑÐ½ÑÑŒÐº': (48.8417, 37.5983),
    'Ð±Ñ–Ð»Ð¾Ð¿Ñ–Ð»Ð»Ñ': (51.1500, 34.3014),
        # Extended regional towns & settlements
        'Ð³Ð°Ð´ÑÑ‡': (50.3713, 34.0109), 'Ñ‡Ð¾Ñ€Ð½ÑƒÑ…Ð¸': (50.2833, 33.0000), 'Ð²ÐµÐ»Ð¸ÐºÑ– ÑÐ¾Ñ€Ð¾Ñ‡Ð¸Ð½Ñ†Ñ–': (50.0167, 33.9833), 'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ°': (50.6633, 32.3933),
        'Ð»ÑƒÐ±Ð½Ð¸': (50.0186, 32.9931), 'ÑˆÐ¸ÑˆÐ°ÐºÐ¸': (49.8992, 34.0072), 'ÑˆÐ¸Ñ€Ð¾ÐºÐµ': (47.6833, 34.5667), 'Ð·ÐµÐ»ÐµÐ½Ð¾Ð´Ð¾Ð»ÑŒÑÑŒÐº': (47.5667, 33.5333),
        'Ð±Ð°Ð±Ð°Ð½ÐºÐ°': (48.9833, 30.4167), 'Ð½Ð¾Ð²Ð¸Ð¹ Ð±ÑƒÐ³': (47.6833, 32.5167), 'Ð±ÐµÑ€ÐµÐ·Ð½ÐµÐ³ÑƒÐ²Ð°Ñ‚Ðµ': (47.3167, 32.8500), 'Ð½Ð¾Ð²Ð¾Ð°Ñ€Ñ…Ð°Ð½Ð³ÐµÐ»ÑŒÑÑŒÐº': (48.6667, 30.8000),
        'Ð»Ð¸Ð¿Ð½ÑÐ¶ÐºÐ°': (48.6167, 30.8667), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐº': (48.3772, 30.5322), 'Ð±Ð¸ÑˆÑ–Ð²': (50.3167, 29.9833), 'Ð¾Ð±ÑƒÑ…Ñ–Ð²': (50.1072, 30.6211),
        'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ¸': (50.2500, 30.2500), 'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°': (49.7950, 30.1310), 'ÑÐºÐ²Ð¸Ñ€Ð°': (49.7333, 29.6667), 'Ñ‡Ð¾Ñ€Ð½Ð¾Ð±Ð¸Ð»ÑŒ': (51.2768, 30.2219),
        'Ð¿ÑƒÐ»Ð¸Ð½Ð¸': (50.4333, 28.4333), 'Ð³Ð¾Ð»Ð¾Ð²Ð¸Ð½Ðµ': (50.3833, 28.6667), 'Ñ€Ð°Ð´Ð¾Ð¼Ð¸ÑˆÐ»ÑŒ': (50.4972, 29.2292), 'ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½ÑŒ': (50.9500, 28.6333),
        'Ð¿Ð¾Ð³Ñ€ÐµÐ±Ð¸Ñ‰Ðµ': (49.4833, 29.2667), 'Ñ‚ÐµÐ¿Ð»Ð¸Ðº': (48.6667, 29.6667), 'Ð¾Ñ€Ð°Ñ‚Ñ–Ð²': (48.9333, 29.5167), 'Ð´Ð°ÑˆÑ–Ð²': (48.9000, 29.4333),
        'ÑˆÐ°Ñ€Ð³Ð¾Ñ€Ð¾Ð´': (48.7333, 28.0833), 'Ð±Ñ–Ñ€ÐºÐ¸': (49.7517, 36.1025), 'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ': (49.9800, 35.5300), 'Ð±Ð°Ð»Ð°ÐºÐ»Ñ–Ñ': (49.4627, 36.8586),
        'Ð±ÐµÑ€ÐµÑÑ‚Ð¸Ð½': (50.2000, 35.0000), 'ÑÑ‚Ð°Ñ€Ð¸Ð¹ ÑÐ°Ð»Ñ‚Ñ–Ð²': (50.0847, 36.7424), 'Ð±Ð¾Ñ€ÐºÐ¸': (49.9380, 36.1260), 'ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ': (51.5481, 33.3847),
    'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ': (49.3833, 33.2667), 'ÐºÑ€Ð¸Ð½Ð¸Ñ‡ÐºÐ¸': (47.2333, 34.3500), 'ÑÐ¾Ð»Ð¾Ð½Ðµ': (48.1436, 35.9933), 'Ð±Ñ€ÑƒÑÐ¸Ð»Ñ–Ð²': (50.2800, 29.5300),
    'Ñ‚ÐµÑ€Ð½Ð¸': (50.9070, 34.0130), 'Ð¿Ð¾Ð½Ð¾Ñ€Ð½Ð¸Ñ†Ñ': (51.8033, 32.5333), 'ÐºÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÐ°': (51.3520, 31.6480),
    # Additional settlements from multi-region UAV course messages (September 2025)
    'Ð¿ÑƒÑ‚Ð¸Ð²Ð»ÑŒ': (51.3361, 33.8692), 'Ð±Ð°Ñ…Ð¼Ð°Ñ‡': (51.1808, 32.8203), 'Ð½Ð¾ÑÑ–Ð²ÐºÐ°': (50.9444, 32.0167), 'ÐºÐ¾Ð·ÐµÐ»ÐµÑ†ÑŒ': (50.9167, 31.1833),
    'ÑÑ‚Ñ€Ð°Ñ…Ð¾Ð»Ñ–ÑÑÑ': (50.5167, 30.8833), 'Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ñ–': (51.1667, 27.75),
    'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°': (50.0167, 35.95), 'Ð±Ð¾Ð¶ÐµÐ´Ñ€Ñ–Ð²ÐºÐ°': (48.4167, 35.0167), 'Ð¿Ê¼ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¸': (48.5667, 33.6833),
    'Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ðµ': (48.7333, 32.8), 'Ð±Ñ€Ð¸Ð»Ñ–Ð²ÐºÐ°': (46.8167, 32.7833),
    # Additional missing cities from napramok messages (September 2025)
    'Ð´ÐµÑÐ½Ð°': (51.0333, 31.1667), 'ÐºÑ–Ð¿Ñ‚Ñ–': (51.2833, 31.2167), 'Ñ–Ñ‡Ð½Ñ': (50.3833, 34.8833), 'Ñ†Ð²Ñ–Ñ‚ÐºÐ¾Ð²Ðµ': (49.7167, 32.2167),
    'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ñ–': (50.8333, 28.7333), 'Ð·Ð²ÑÐ³ÐµÐ»ÑŒ': (50.5833, 27.6667), 'ÐºÐ°Ð¼Ê¼ÑÐ½ÑÑŒÐºÐµ': (48.5167, 34.6167),
    'Ð¿Ñ–Ñ‰Ð°Ð½Ð¸Ð¹ Ð±Ñ€Ñ–Ð´': (48.1167, 32.0833), 'Ð±Ð¾Ð±Ñ€Ð¸Ð½ÐµÑ†ÑŒ': (48.0333, 32.1833), 'Ñ‚ÐµÐ½Ð´Ñ€Ñ–Ð²ÑÑŒÐºÐ° ÐºÐ¾ÑÐ°': (46.1833, 31.5333),
        'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹': (51.9874, 33.2620), 'ÑÐ¾ÑÐ½Ð¸Ñ†Ñ': (51.5236, 32.4953), 'Ð¾Ð»Ð¸ÑˆÑ–Ð²ÐºÐ°': (51.0725, 31.3525), 'Ð±ÐµÑ€ÐµÐ·Ð½Ð°': (51.5756, 31.7431),
        'Ð·Ð°Ñ‡ÐµÐ¿Ð¸Ð»Ñ–Ð²ÐºÐ°': (49.1717, 35.2742), 'Ð±Ð»Ð¸Ð·Ð½ÑŽÐºÐ¸': (48.8520, 36.5440), 'Ð½Ð¾Ð²Ð° Ð²Ð¾Ð´Ð¾Ð»Ð°Ð³Ð°': (49.7270, 35.8570), 'ÑÐ°Ñ…Ð½Ð¾Ð²Ñ‰Ð¸Ð½Ð°': (49.1544, 35.1460),
        'Ð³ÑƒÐ±Ð¸Ð½Ð¸Ñ…Ð°': (48.7437, 35.2960), 'Ð¿ÐµÑ€ÐµÑ‰ÐµÐ¿Ð¸Ð½Ðµ': (48.6260, 35.3580), 'ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÐ°': (49.4586, 35.1272), 'Ð¼Ð°Ð³Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.8836, 34.8669),
        'ÑÐ°Ð²Ð¸Ð½Ñ†Ñ–': (49.4365, 37.2981), 'ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÐ¾Ð²Ðµ': (49.6996, 37.1770), 'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÐºÐ°': (48.6035, 34.8530), 'ÐºÑƒÑ€Ð¸Ð»Ñ–Ð²ÐºÐ°': (48.6715, 34.8740),
        'Ð¿ÐµÑ‚Ñ€Ð¸ÐºÑ–Ð²ÐºÐ°': (48.7330, 34.6300), 'Ð¿Ñ–Ð´Ð³Ð¾Ñ€Ð¾Ð´Ð½Ðµ': (48.5747, 35.1482), 'ÑÐ°Ð¼Ð°Ñ€': (48.6500, 35.4200), 'Ð²ÐµÑ€Ñ…Ð½ÑŒÐ¾Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐº': (48.6535, 34.3372),
        'Ð³Ð¾Ñ€Ñ–ÑˆÐ½Ñ– Ð¿Ð»Ð°Ð²Ð½Ñ–': (49.0123, 33.6450), "ÐºÐ°Ð¼'ÑÐ½ÑÑŒÐºÐµ": (48.5110, 34.6021), 'ÐºÐ°Ð¼ÑÐ½ÑÑŒÐºÐµ': (48.5110, 34.6021), 'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°': (50.5700, 33.7900),
        'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ': (50.4833, 34.9667), 'Ð»ÐµÐ±ÐµÐ´Ð¸Ð½': (50.5872, 34.4912), 'ÑƒÐ»ÑÐ½Ñ–Ð²ÐºÐ°': (50.8530, 34.3170), 'ÑƒÐ»ÑÐ½Ð¾Ð²ÐºÐ°': (50.8530, 34.3170),
        'Ð±Ð¾Ð³Ð¾Ð´ÑƒÑ…Ñ–Ð²': (50.1646, 35.5279), 'Ñ…Ð¾Ð»Ð¼Ð¸': (51.6272, 32.5531), 'Ð±Ð»Ð¸ÑÑ‚Ð¾Ð²Ð°': (51.6833, 32.6333), 'Ð½Ñ–Ð¶Ð¸Ð½': (51.0480, 31.8860),
        'Ð¼ÐµÐ½Ð°': (51.5211, 32.2147), 'Ð´ÐµÑÐ½Ð°': (51.0833, 30.9333), 'Ñ”Ð²Ð¼Ð¸Ð½ÐºÐ°': (51.3167, 31.7167), 'Ð¼.-ÐºÐ¾Ñ†ÑŽÐ±Ð¸Ð½ÑÑŒÐºÐµ': (51.5833, 31.1167),
        'Ð»Ð¾ÑÐ¸Ð½Ñ–Ð²ÐºÐ°': (51.1333, 31.7167), 'Ñ–Ñ‡Ð½Ñ': (51.0722, 32.3931), 'Ð±Ð¾Ñ€Ð·Ð½Ð°': (51.2542, 32.4192), 'Ð¿Ñ€Ð¸Ð»ÑƒÐºÐ¸': (50.5931, 32.3878),
        'Ð»Ð¸Ð½Ð¾Ð²Ð¸Ñ†Ñ': (50.7833, 32.4167), 'Ð²Ð°Ð»ÐºÐ¸': (49.8427, 35.6150), 'ÐºÐµÐ³Ð¸Ñ‡Ñ–Ð²ÐºÐ°': (49.5440, 35.7760), 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°': (50.1946, 33.7894),
        'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡ÐºÑƒ': (50.1946, 33.7894), 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡Ñƒ': (50.1946, 33.7894), 'Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ñƒ': (50.5700, 33.7900), 'Ð·Ð°Ñ‚Ð¾ÐºÐ°': (46.0660, 30.4680),
        'ÑÐ½ÑÑŒÐºÐµ': (48.4567, 36.3342), 'Ñ‡ÑƒÐ³ÑƒÑ—Ð²': (49.8376, 36.6881), 'Ð²Ð¾Ñ€Ð¾Ð¶Ð±Ð°': (51.8031, 34.4972), 'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ñ–Ð»Ð»Ñ': (50.4422, 35.3081),
        'Ð±Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ': (50.3527, 30.9550), 'Ð¶Ð°ÑˆÐºÑ–Ð²': (49.2431, 30.1122), 'ÐµÑÐ¼Ð°Ð½ÑŒ': (51.8833, 34.2833), 'Ð¼ÐµÑ€ÐµÑ„Ð°': (49.8181, 36.0572),
        'Ð³Ð»ÑƒÑ…Ñ–Ð²': (51.6781, 33.9169), 'Ð½ÐµÐ´Ñ€Ð¸Ð³Ð°Ð¹Ð»Ñ–Ð²': (50.8281, 33.8781), 'Ð²Ð¾Ñ€Ð¾Ð½Ñ–Ð¶': (51.8081, 33.3722), 'Ñ€Ð¾Ð¼Ð½Ð¸': (50.7497, 33.4746),
    'ÑÐ¼Ð¿Ñ–Ð»ÑŒ': (51.2247, 34.3224), 'ÑÐ¼Ð¿Ñ–Ð»ÑŒ ÑÑƒÐ¼ÑÑŒÐºÐ°': (51.2247, 34.3224), 'ÑÐ¼Ð¿Ð¾Ð»ÑŒ ÑÑƒÐ¼ÑÑŒÐºÐ°': (51.2247, 34.3224),
    'Ñ…ÑƒÑ‚Ñ–Ñ€-Ð¼Ð¸Ñ…Ð°Ð¹Ð»Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (51.8000, 33.5000),
        'ÑƒÐ·Ð¸Ð½': (49.8216, 30.4567), 'Ð³Ð¾Ð½Ñ‡Ð°Ñ€Ñ–Ð²ÑÑŒÐºÐµ': (51.6272, 31.3192), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐº': (48.3772, 30.5322), 'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÐ°': (48.3122, 31.5272),
        'Ñ‚ÑƒÐ»ÑŒÑ‡Ð¸Ð½': (48.6783, 28.8486), 'Ð±Ñ€Ð¾Ð²Ð°Ñ€Ð¸': (50.5110, 30.7909), 'ÐºÐ°Ð½Ñ–Ð²': (49.7517, 31.4717), 'Ð¼Ð¸Ñ€Ð¾Ð½Ñ–Ð²ÐºÐ°': (49.6631, 31.0100),
        'Ð±Ð¾Ñ€Ð¾Ð²Ð°': (49.3742, 36.4892), 'Ð±ÑƒÑ€Ð¸Ð½ÑŒ': (51.2000, 33.8500), 'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿': (51.2417, 33.2022), 'ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†': (51.5486, 33.3856), 'Ð¾ÑÑ‚ÐµÑ€': (50.9481, 30.8831),
        'Ð¿Ð»Ð°Ð²Ð½Ñ–': (49.0123, 33.6450), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (48.3772, 30.5322), 'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (48.3122, 31.5272),
        'Ð±ÐµÐ·Ð»ÑŽÐ´Ñ–Ð²ÐºÐ°': (49.8872, 36.2731), 'Ñ€Ð¾Ð³Ð°Ð½ÑŒ': (49.9342, 36.4942), 'ÑÐ°Ð²Ð¸Ð½Ñ†Ñ–(Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°)': (49.6272, 36.9781),
        'ÑÐ»Ð°Ñ‚Ð¸Ð½Ðµ': (49.7500, 36.1500),  # Ð¡Ð»Ð°Ñ‚Ð¸Ð½Ðµ, Ð”ÐµÑ€Ð³Ð°Ñ‡Ñ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€-Ð½, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».
        'Ð³ÑƒÑ‚Ð¸': (50.0167, 36.3833),  # Ð“ÑƒÑ‚Ð¸, Ð¥Ð°Ñ€ÑŒÐºÐ¾Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
        # Temporary missing cities (should be in external data sources)
        'Ð³ÑƒÑÐ°Ñ€Ñ–Ð²ÐºÐ°': (49.1000, 37.1500),  # Ð“ÑƒÑÐ°Ñ€Ñ–Ð²ÐºÐ°, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - TEMP: SpaCy Ð½Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
        'Ð¿Ñ€Ð¾Ñ‚Ð¾Ð¿Ð¾Ð¿Ñ–Ð²ÐºÐ°': (49.7000, 37.0000),  # ÐŸÑ€Ð¾Ñ‚Ð¾Ð¿Ð¾Ð¿Ñ–Ð²ÐºÐ°, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - TEMP: SpaCy Ð½Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
        'ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÐ°': (50.1447, 30.7381), 'Ñ†Ð°Ñ€Ð¸Ñ‡Ð°Ð½ÐºÐ°': (48.9767, 34.3772), 'Ñ€Ñ–Ð¿ÐºÐ¸': (51.8122, 31.0817), 'Ð¼Ð¸Ñ…Ð°Ð¹Ð»Ð¾-ÐºÐ¾Ñ†ÑŽÐ±Ð¸Ð½ÑÑŒÐºÐµ': (51.5833, 31.1167),
    'Ð°Ð½Ð´Ñ€Ñ–Ñ—Ð²ÐºÐ°': (49.9380, 36.9510),
        'Ð¼Ð°ÐºÐ¾ÑˆÐ¸Ð½Ðµ': (51.6275, 32.2731), 'Ð¿Ð°Ñ€Ð°Ñ„Ñ–Ñ—Ð²ÐºÐ°': (50.9833, 32.2833), 'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°': (51.1833, 33.7833), 'Ð±Ð¾Ñ€Ð¾Ð¼Ð»Ñ': (50.7500, 34.9833),
    # Newly added (missing in earlier dictionary lookups reported by user)
    'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ñ': (51.8892, 31.6011),
        'Ð¶ÑƒÐºÐ¸Ð½': (50.7800, 30.6820), 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°': (50.8140, 30.8080), 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ': (50.8140, 30.8080), 'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´': (50.5840, 30.4890),
    'Ð´Ð¸Ð¼ÐµÑ€': (50.8390, 30.3050),
        'Ñ€Ð¶Ð¸Ñ‰Ñ–Ð²': (49.9719, 31.0500), 'Ð²Ð¸ÑˆÐµÐ½ÑŒÐºÐ¸': (50.2987, 30.6445), 'Ð¶ÑƒÐ»ÑÐ½Ð¸': (50.4017, 30.4519), 'Ñ‚Ñ€Ð¾Ñ”Ñ‰Ð¸Ð½Ð°': (50.5130, 30.6030),
        'Ñ‚Ñ€Ð¾Ñ”Ñ‰Ð¸Ð½Ñƒ': (50.5130, 30.6030), 'ÐºÐ¾Ð½Ñ‡Ð°-Ð·Ð°ÑÐ¿Ð°': (50.2650, 30.5760), 'Ð»ÑŽÐ±Ð°Ñ€': (50.0500, 27.7500), 'ÑÑ‚Ð°Ñ€Ð¸Ð¹ Ð¾ÑÑ‚Ñ€Ð¾Ð¿Ñ–Ð»ÑŒ': (49.6503, 27.2291),
        'Ð¿ÐµÑ‚Ñ€Ñ–Ð²Ñ†Ñ–': (50.4167, 30.5833),
        'Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÐ°': (50.4950, 31.7780), 'Ð¼Ð°Ð»Ð° Ð´Ñ–Ð²Ð¸Ñ†Ñ': (50.8240, 32.4700), 'ÑÐ³Ð¾Ñ‚Ð¸Ð½': (50.2360, 31.7700), 'ÑÑ‚Ð°Ð²Ð¸Ñ‰Ðµ': (49.3958, 30.1875),
        'Ð±ÐµÑ€ÐµÐ·Ð°Ð½ÑŒ': (50.3085, 31.4576), 'Ð±Ð¾Ñ€Ñ‚Ð½Ð¸Ñ‡Ñ–': (50.3915, 30.6695), 'ÑÑ‚Ð°Ñ€Ð¾ÐºÐ¾ÑÑ‚ÑÐ½Ñ‚Ð¸Ð½Ñ–Ð²': (49.7574, 27.2039), 'Ð°Ð´Ð°Ð¼Ð¿Ñ–Ð»ÑŒ': (49.6500, 27.3000),
        # Additional single-city early parser support
        'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ': (48.1180, 36.2470), 'Ð¿ÐµÑ‚Ñ€Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°': (48.5000, 36.4500), 'ÑˆÐ°Ñ…Ñ‚Ð°Ñ€ÑÑŒÐºÐµ': (47.9500, 36.0500),
        # 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°': Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ - Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
        'Ð½Ð¸Ð·Ð¸': (50.7435, 34.9860), 'Ð±Ð°Ñ€Ð²Ñ–Ð½ÐºÐ¾Ð²Ðµ': (48.9000, 37.0167), 'Ð¿Ñ–ÑÐ¾Ñ‡Ð¸Ð½': (49.9500, 36.1330), 'Ð±ÐµÑ€ÐµÑÑ‚Ð¾Ð²Ðµ': (49.3500, 37.0000),
    'ÐºÐ¾Ð±ÐµÐ»ÑÐºÐ¸': (49.1500, 34.2000), 'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²': (49.8942, 28.5986),
    'ÐºÑƒÑ†ÑƒÑ€ÑƒÐ±': (46.7906, 31.9222), 'Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÐºÐ°': (50.4850, 30.6090),
    # Newly added Kyiv & Odesa region settlements / raion centers for alerts
    'Ð³Ð¾ÑÑ‚Ð¾Ð¼ÐµÐ»ÑŒ': (50.5853, 30.2617), 'Ð±Ð¾ÑÑ€ÐºÐ°': (50.3301, 30.5201), 'Ð¼Ð°ÐºÐ°Ñ€Ñ–Ð²': (50.4645, 29.8114),
    'Ð±Ð¾Ñ€Ð¾Ð´ÑÐ½ÐºÐ°': (50.6447, 29.9202), 'ÐºÑ–Ð»Ñ–Ñ': (45.4553, 29.2640),
    # Cherkasy region settlement (directional course report: "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð¦Ð¸Ð±ÑƒÐ»Ñ–Ð²")
    'Ñ†Ð¸Ð±ÑƒÐ»Ñ–Ð²': (49.0733, 29.8472),
    # Raion centers (approx: use main settlement or administrative center)
    'Ñ–Ð·Ð¼Ð°Ñ—Ð»ÑŒÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (45.3516, 28.8365), # near Izmail
    'Ð±Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (50.5110, 30.7909),
    'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (50.1072, 30.6211),
    'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (46.6350, 32.6169),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (49.2331, 28.4682)
    ,# --- Newly added (Sept 2 multi-line UAV message) ---
    'Ð½ÐµÑ‡Ð°ÑÐ½Ðµ': (46.8840, 32.0310), 'Ð½ÐµÑ‡Ð°ÑÐ½ÐµÑƒ': (46.8840, 32.0310), 'Ð½ÐµÑ‡Ð°ÑÐ½Ð¾Ð¼Ñƒ': (46.8840, 32.0310),
    'Ñ‡ÐµÑ‡ÐµÐ»ÑŒÐ½Ð¸Ðº': (48.2156, 29.3708), 'Ñ‡ÐµÑ‡ÐµÐ»ÑŒÐ½Ð¸ÐºÑƒ': (48.2156, 29.3708),
    'Ð·Ð°Ð»Ñ–ÑÑÑ': (51.0836, 30.1914), 'Ð·Ð°Ð»Ñ–ÑÑÑÐ¼': (51.0836, 30.1914),
    'ÐºÑ€Ð°ÑÑÑ‚Ð¸Ñ‡Ñ–': (51.0410, 29.6000), 'ÐºÑ€Ð°ÑÑÑ‚Ð¸Ñ‡Ñ–Ð²': (51.0410, 29.6000),
    'Ñ–Ñ€ÑˆÐ°Ð½ÑÑŒÐº': (50.7833, 28.9000), 'Ñ–Ñ€ÑˆÐ°Ð½ÑÑŒÐºÑƒ': (50.7833, 28.9000),
    'Ñ…Ð¾Ñ€Ð¾ÑˆÑ–Ð²': (50.7167, 28.6167), 'Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐ²Ñ–': (50.7167, 28.6167), 'Ñ…Ð¾Ñ€Ð¾ÑˆÐµÐ²Ñƒ': (50.7167, 28.6167),
    'Ð½Ð°Ñ€Ð¾Ð´Ð¸Ñ‡Ñ–': (51.2000, 29.0833), 'Ð½Ð°Ñ€Ð¾Ð´Ð¸Ñ‡Ñ–Ð²': (51.2000, 29.0833),
    'Ð½Ð¾Ð²Ñ– Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ñ–': (51.1333, 27.7667), 'Ð½Ð¾Ð²Ñ– Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ñ–Ð²': (51.1333, 27.7667),
    'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ð¸Ñ†Ñ': (51.3270, 27.3460), 'Ð³Ð¾Ñ€Ð¾Ð´Ð½Ð¸Ñ†ÑŽ': (51.3270, 27.3460),
    'Ð±ÐµÑ€ÐµÐ·Ð½Ðµ': (50.9833, 26.7500), 'Ð±ÐµÑ€ÐµÐ·Ð½Ð¾Ð¼Ñƒ': (50.9833, 26.7500),
    'Ð±Ð¾Ð¶ÐµÐ´Ð°Ñ€Ñ–Ð²ÐºÐ°': (48.3014, 34.5522), 'Ð±Ð¾Ð¶ÐµÐ´Ð°Ñ€Ñ–Ð²ÐºÑƒ': (48.3014, 34.5522), 'Ð±Ð¾Ð¶ÐµÐ´Ð°Ñ€Ñ–Ð²Ñ†Ñ–': (48.3014, 34.5522),
    'Ð¿Ê¼ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¸': (48.5667, 33.6833), "Ð¿'ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¸": (48.5667, 33.6833), 'Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¸': (48.5667, 33.6833),
    'Ð¶Ð¾Ð²Ñ‚Ñ– Ð²Ð¾Ð´Ð¸': (48.3500, 33.5000), 'Ð¶Ð¾Ð²Ñ‚Ð¸Ñ… Ð²Ð¾Ð´': (48.3500, 33.5000),
    'Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÑƒ': (50.4950, 31.7780), 'Ð·Ð³ÑƒÑ€Ñ–Ð²Ñ†Ñ–': (50.4950, 31.7780),
    'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½': (49.7167, 28.8333), 'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½Ð°': (49.7167, 28.8333),
    'Ñ‚ÐµÐ¿Ð»Ð¸Ðº': (48.6650, 29.7480), 'Ñ‚ÐµÐ¿Ð»Ð¸ÐºÐ°': (48.6650, 29.7480),
    'Ð½Ð¾Ð²Ð¸Ð¹ Ð±ÑƒÐ³': (47.6833, 32.5167), 'Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð±ÑƒÐ³Ð°': (47.6833, 32.5167),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÑƒ': (50.6633, 32.3933), 'Ð»ÑƒÐ±Ð½Ð¸': (50.0186, 32.9931),
    'Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÐ° (ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°)': (50.4950, 31.7780),
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÑƒ': (50.2500, 30.2500), 'Ð·Ð³ÑƒÑ€Ñ–Ð²ÐºÐ¾': (50.4950, 31.7780),
    # Ð“Ñ€ÐµÐ±Ñ–Ð½ÐºÐ° (ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ) - Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ– ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ‚Ð¸
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ°': (50.1058, 32.4464), 'Ð³Ñ€ÐµÐ±Ñ–Ð½Ñ†Ñ–': (50.1058, 32.4464), 'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÑƒ Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°': (50.1058, 32.4464),
    'Ñ…Ð¾Ñ‚Ñ–Ð½ÑŒ': (51.0550, 34.0000), 'Ñ…Ð¾Ñ‚Ñ–Ð½Ñ–': (51.0550, 34.0000),
    # Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ñ–ÑÑ‚Ð° Ð· Ð½Ð¾Ð²Ð¸Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ
    'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸Ðº': (49.5500, 27.9667), 'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸ÐºÑƒ': (49.5500, 27.9667), 'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸ÐºÐ°': (49.5500, 27.9667),
    'Ð±Ð°Ð»Ñ‚Ð°': (47.9667, 29.6167), 'Ð±Ð°Ð»Ñ‚Ñƒ': (47.9667, 29.6167), 'Ð±Ð°Ð»Ñ‚Ñ–': (47.9667, 29.6167),
    'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐº': (48.7667, 31.4833), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐºÑƒ': (48.7667, 31.4833), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐºÐ°': (48.7667, 31.4833),
    'Ð¿Ð¾Ð¼Ñ–Ñ‡Ð½Ð°': (48.4333, 32.5333), 'Ð¿Ð¾Ð¼Ñ–Ñ‡Ð½Ñƒ': (48.4333, 32.5333), 'Ð¿Ð¾Ð¼Ñ–Ñ‡Ð½Ñ–Ð¹': (48.4333, 32.5333),
    'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´ÐºÐ°': (49.0803, 30.9617), 'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´ÐºÑƒ': (49.0803, 30.9617), 'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´Ñ†Ñ–': (49.0803, 30.9617),
    'Ð±ÑƒÐºÐ¸': (49.1167, 30.8833), 'Ð±ÑƒÐºÐ°Ð¼': (49.1167, 30.8833), 'Ð±ÑƒÐºÐ°Ñ…': (49.1167, 30.8833),
    'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÐ°': (50.7833, 34.3667), 'Ð´ÑƒÐ±Ð¾Ð²Ê¼ÑÐ·Ñ–Ð²ÐºÑƒ': (50.7833, 34.3667), "Ð´ÑƒÐ±'ÑÐ·Ñ–Ð²ÐºÐ°": (50.7833, 34.3667), "Ð´ÑƒÐ±Ð¾Ð²'ÑÐ·Ñ–Ð²ÐºÐ°": (50.7833, 34.3667),
    'Ð¿Ð°Ñ€Ð°Ñ„Ñ–Ñ—Ð²ÐºÐ°': (51.4833, 31.1167), 'Ð¿Ð°Ñ€Ð°Ñ„Ñ–Ñ—Ð²ÐºÑƒ': (51.4833, 31.1167), 'Ð¿Ð°Ñ€Ð°Ñ„Ñ–Ñ—Ð²Ñ†Ñ–': (51.4833, 31.1167),
    'Ð½Ð¾Ð²Ð° Ð±Ð¾Ñ€Ð¾Ð²Ð°': (50.7167, 27.9167), 'Ð½Ð¾Ð²Ñƒ Ð±Ð¾Ñ€Ð¾Ð²Ñƒ': (50.7167, 27.9167), 'Ð½Ð¾Ð²Ñ–Ð¹ Ð±Ð¾Ñ€Ð¾Ð²Ñ–Ð¹': (50.7167, 27.9167),
    'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ñ–': (50.4333, 28.1167), 'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ñ–Ð²': (50.4333, 28.1167), 'Ñ‡Ð¾Ð¿Ð¾Ð²Ð¸Ñ‡Ð°Ñ…': (50.4333, 28.1167),
    'Ñ”Ð¼Ñ–Ð»ÑŒÑ‡Ð¸Ð½Ðµ': (50.8667, 27.8667), 'Ñ”Ð¼Ñ–Ð»ÑŒÑ‡Ð¸Ð½Ñƒ': (50.8667, 27.8667), 'Ñ”Ð¼Ñ–Ð»ÑŒÑ‡Ð¸Ð½Ð¾Ð¼Ñƒ': (50.8667, 27.8667),
    'ÑÑ‚Ð°Ñ€Ð° ÑÐ¸Ð½ÑÐ²Ð°': (49.6333, 27.6167), 'ÑÑ‚Ð°Ñ€Ñƒ ÑÐ¸Ð½ÑÐ²Ñƒ': (49.6333, 27.6167), 'ÑÑ‚Ð°Ñ€Ñ–Ð¹ ÑÐ¸Ð½ÑÐ²Ñ–': (49.6333, 27.6167),
    'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²ÐºÐ°': (50.1833, 27.0667), 'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²ÐºÑƒ': (50.1833, 27.0667), 'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²Ñ†Ñ–': (50.1833, 27.0667),
    'ÑÐ°Ñ€Ð½Ð¸': (51.3333, 26.6000), 'ÑÐ°Ñ€Ð½Ð°Ñ…': (51.3333, 26.6000), 'ÑÐ°Ñ€Ð½Ñ–Ð²': (51.3333, 26.6000),
    'ÑÑ‚ÐµÐ¿Ð°Ð½ÑŒ': (50.5167, 25.9167), 'ÑÑ‚ÐµÐ¿Ð°Ð½Ñ–': (50.5167, 25.9167), 'ÑÑ‚ÐµÐ¿Ð°Ð½Ñ': (50.5167, 25.9167),
    'Ð»ÑŽÐ±ÐµÑˆÑ–Ð²': (51.9000, 25.3333), 'Ð»ÑŽÐ±ÐµÑˆÐµÐ²Ñ–': (51.9000, 25.3333), 'Ð»ÑŽÐ±ÐµÑˆÐ¾Ð²Ñƒ': (51.9000, 25.3333),
    'Ð¼Ð°Ð½ÐµÐ²Ð¸Ñ‡Ñ–': (51.3000, 25.5167), 'Ð¼Ð°Ð½ÐµÐ²Ð¸Ñ‡Ñ–Ð²': (51.3000, 25.5167), 'Ð¼Ð°Ð½ÐµÐ²Ð¸Ñ‡Ð°Ñ…': (51.3000, 25.5167),
    'ÐºÐ¾Ð»ÐºÐ¸': (51.9333, 24.7167), 'ÐºÐ¾Ð»ÐºÐ°Ñ…': (51.9333, 24.7167), 'ÐºÐ¾Ð»ÐºÐ°Ð¼': (51.9333, 24.7167),
    'Ñ€Ð¾Ð¶Ð¸Ñ‰Ðµ': (51.6333, 24.2333), 'Ñ€Ð¾Ð¶Ð¸Ñ‰Ñƒ': (51.6333, 24.2333), 'Ñ€Ð¾Ð¶Ð¸Ñ‰Ñ–': (51.6333, 24.2333),
    'Ñ€Ð¾ÐºÐ¸Ð½Ñ–': (50.9833, 24.5167), 'Ñ€Ð¾ÐºÐ¸Ð½ÑÑ…': (50.9833, 24.5167), 'Ñ€Ð¾ÐºÐ¸Ð½Ñ–Ð²': (50.9833, 24.5167),
    'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€': (50.8500, 24.3167), 'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñƒ': (50.8500, 24.3167), 'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñ–': (50.8500, 24.3167),
    # Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ñ–ÑÑ‚Ð° Ð· Ð½Ð¾Ð²Ð¸Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ 2
    'Ð¶Ð¾Ð²ÐºÐ²Ð°': (49.9333, 23.9667), 'Ð¶Ð¾Ð²ÐºÐ²Ñƒ': (49.9333, 23.9667), 'Ð¶Ð¾Ð²ÐºÐ²Ñ–': (49.9333, 23.9667),
    'ÐºÑ€Ð¸Ð²Ðµ Ð¾Ð·ÐµÑ€Ð¾': (47.9500, 30.3500), 'ÐºÑ€Ð¸Ð²Ð¾Ð¼Ñƒ Ð¾Ð·ÐµÑ€Ñƒ': (47.9500, 30.3500), 'ÐºÑ€Ð¸Ð²Ð¾Ð³Ð¾ Ð¾Ð·ÐµÑ€Ð°': (47.9500, 30.3500),
    'Ð¼Ð¾Ð³Ð¸Ð»Ñ–Ð²-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¸Ð¹': (48.4833, 27.8000), 'Ð¼Ð¾Ð³Ð¸Ð»ÐµÐ²Ð°-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¾Ð³Ð¾': (48.4833, 27.8000),
    'ÑÐ¼Ð¿Ñ–Ð»ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833), 'ÑÐ¼Ð¿Ð¾Ð»ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833),
    'ÑÐ¼Ð¿Ð¾Ð»ÑŽ': (48.1333, 28.2833), 'ÑÐ¼Ð¿Ð¾Ð»Ñ–': (48.1333, 28.2833),
    'Ð´Ð·Ð¸Ð³Ñ–Ð²ÐºÐ°': (49.2167, 28.1500), 'Ð´Ð·Ð¸Ð³Ñ–Ð²ÐºÑƒ': (49.2167, 28.1500), 'Ð´Ð·Ð¸Ð³Ñ–Ð²Ñ†Ñ–': (49.2167, 28.1500),
    'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²ÐºÐ°': (46.8167, 30.9167), 'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²ÐºÑƒ': (46.8167, 30.9167), 'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²Ñ†Ñ–': (46.8167, 30.9167),
    # 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°': Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ - Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ð¾Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
    'Ð²Ñ–Ð»ÑŒÐ½Ð¾Ð³Ñ–Ñ€ÑÑŒÐº': (47.9333, 34.0167), 'Ð²Ñ–Ð»ÑŒÐ½Ð¾Ð³Ñ–Ñ€ÑÑŒÐºÑƒ': (47.9333, 34.0167), 'Ð²Ñ–Ð»ÑŒÐ½Ð¾Ð³Ñ–Ñ€ÑÑŒÐºÐ°': (47.9333, 34.0167),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð¸ÑÐºÐ°': (49.2333, 32.1833), 'Ð²ÐµÐ»Ð¸ÐºÐ¾Ñ— Ð²Ð¸ÑÐºÐ¸': (49.2333, 32.1833), 'Ð²ÐµÐ»Ð¸ÐºÑ–Ð¹ Ð²Ð¸ÑÑ†Ñ–': (49.2333, 32.1833),
    'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð²Ð¸ÑÐºÑƒ': (49.2333, 32.1833),  # accusative form
    'Ð´Ð¾Ð±Ñ€Ð¾ÑÐ»Ð°Ð²': (46.6000, 30.0500), 'Ð´Ð¾Ð±Ñ€Ð¾ÑÐ»Ð°Ð²Ñƒ': (46.6000, 30.0500), 'Ð´Ð¾Ð±Ñ€Ð¾ÑÐ»Ð°Ð²Ñ–': (46.6000, 30.0500),
    'Ñ‚Ð¸ÑˆÐºÑ–Ð²ÐºÐ°': (48.7667, 32.6833), 'Ñ‚Ð¸ÑˆÐºÑ–Ð²ÐºÑƒ': (48.7667, 32.6833), 'Ñ‚Ð¸ÑˆÐºÑ–Ð²Ñ†Ñ–': (48.7667, 32.6833),
    'ÑÐ°Ð»ÑŒÐºÐ¾Ð²Ðµ': (48.6167, 32.4500), 'ÑÐ°Ð»ÑŒÐºÐ¾Ð²Ñƒ': (48.6167, 32.4500), 'ÑÐ°Ð»ÑŒÐºÐ¾Ð²Ð¾Ð¼Ñƒ': (48.6167, 32.4500),
    'Ð±Ð»Ð°Ð³Ð¾Ð²Ñ–Ñ‰ÐµÐ½ÑÑŒÐºÐµ': (48.4167, 32.8833), 'Ð±Ð»Ð°Ð³Ð¾Ð²Ñ–Ñ‰ÐµÐ½ÑÑŒÐºÑƒ': (48.4167, 32.8833), 'Ð±Ð»Ð°Ð³Ð¾Ð²Ñ–Ñ‰ÐµÐ½ÑÑŒÐºÐ¾Ð³Ð¾': (48.4167, 32.8833),
    'Ð¾Ñ€Ð¶Ð¸Ñ†Ñ': (50.0667, 32.3833), 'Ð¾Ñ€Ð¶Ð¸Ñ†ÑŽ': (50.0667, 32.3833), 'Ð¾Ñ€Ð¶Ð¸Ñ†Ñ–': (50.0667, 32.3833),
    'Ñ‚Ð°Ð»ÑŒÐ½Ðµ': (49.1833, 30.6833), 'Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ': (49.1833, 30.6833), 'Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾': (49.1833, 30.6833),
    'Ð±Ð¾Ð±Ñ€Ð¾Ð²Ð¸Ñ†Ñ': (51.0833, 32.1167), 'Ð±Ð¾Ð±Ñ€Ð¾Ð²Ð¸Ñ†ÑŽ': (51.0833, 32.1167), 'Ð±Ð¾Ð±Ñ€Ð¾Ð²Ð¸Ñ†Ñ–': (51.0833, 32.1167),
    'Ñ…Ð¾Ð»Ð¼Ð¸': (51.6833, 32.4000), 'Ñ…Ð¾Ð»Ð¼Ð°Ð¼': (51.6833, 32.4000), 'Ñ…Ð¾Ð»Ð¼Ð°Ñ…': (51.6833, 32.4000),
    'ÑÐ¾ÑÐ½Ð¸Ñ†Ñ': (51.9500, 32.4667), 'ÑÐ¾ÑÐ½Ð¸Ñ†ÑŽ': (51.9500, 32.4667), 'ÑÐ¾ÑÐ½Ð¸Ñ†Ñ–': (51.9500, 32.4667),
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´': (50.5833, 30.4833), 'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´Ñƒ': (50.5833, 30.4833), 'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´Ñ–': (50.5833, 30.4833),
    'Ð¼Ð°Ð»Ð¸Ð½': (50.7667, 29.2333), 'Ð¼Ð°Ð»Ñ–Ð½Ñƒ': (50.7667, 29.2333), 'Ð¼Ð°Ð»Ñ–Ð½Ñ–': (50.7667, 29.2333),
    'Ð±ÑƒÑ‡Ð¼Ð°Ð½Ð¸': (50.2833, 28.3333), 'Ð±ÑƒÑ‡Ð¼Ð°Ð½Ð°Ð¼': (50.2833, 28.3333), 'Ð±ÑƒÑ‡Ð¼Ð°Ð½Ð°Ñ…': (50.2833, 28.3333),
    'Ñ‡ÐµÑ€Ð²Ð¾Ð½Ðµ': (50.4167, 28.9167), 'Ñ‡ÐµÑ€Ð²Ð¾Ð½Ð¾Ð¼Ñƒ': (50.4167, 28.9167), 'Ñ‡ÐµÑ€Ð²Ð¾Ð½Ð¾Ð³Ð¾': (50.4167, 28.9167),
    'Ð¿Ð¾Ð½Ñ–Ð½ÐºÐ°': (49.6167, 27.4500), 'Ð¿Ð¾Ð½Ñ–Ð½ÐºÑƒ': (49.6167, 27.4500), 'Ð¿Ð¾Ð½Ñ–Ð½Ñ†Ñ–': (49.6167, 27.4500),
    'Ñ‚ÐµÐ¾Ñ„Ñ–Ð¿Ð¾Ð»ÑŒ': (49.8833, 27.6500), 'Ñ‚ÐµÐ¾Ñ„Ñ–Ð¿Ð¾Ð»ÑŽ': (49.8833, 27.6500), 'Ñ‚ÐµÐ¾Ñ„Ñ–Ð¿Ð¾Ð»Ñ–': (49.8833, 27.6500),
    'Ð³Ð¾Ñ‰Ð°': (50.6167, 26.4167), 'Ð³Ð¾Ñ‰Ñƒ': (50.6167, 26.4167), 'Ð³Ð¾Ñ‰Ñ–': (50.6167, 26.4167),
    'ÐºÐ»ÐµÐ²Ð°Ð½ÑŒ': (50.7667, 25.9833), 'ÐºÐ»ÐµÐ²Ð°Ð½Ñ–': (50.7667, 25.9833), 'ÐºÐ»ÐµÐ²Ð°Ð½ÑŽ': (50.7667, 25.9833),
    'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€ÐµÑ†ÑŒ': (51.4333, 25.9167), 'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñ†ÑŽ': (51.4333, 25.9167), 'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñ†Ñ–': (51.4333, 25.9167),
    'Ð»Ð¾ÐºÐ°Ñ‡Ñ–': (51.1167, 24.2667), 'Ð»Ð¾ÐºÐ°Ñ‡Ð°Ð¼': (51.1167, 24.2667), 'Ð»Ð¾ÐºÐ°Ñ‡Ð°Ñ…': (51.1167, 24.2667),
    'Ñ–Ð²Ð°Ð½Ð¸Ñ‡Ñ–': (51.2333, 24.3167), 'Ñ–Ð²Ð°Ð½Ð¸Ñ‡Ð°Ð¼': (51.2333, 24.3167), 'Ñ–Ð²Ð°Ð½Ð¸Ñ‡Ð°Ñ…': (51.2333, 24.3167),
    'Ñ‚ÑƒÑ€Ñ–Ð¹ÑÑŒÐº': (51.0833, 24.7000), 'Ñ‚ÑƒÑ€Ñ–Ð¹ÑÑŒÐºÑƒ': (51.0833, 24.7000), 'Ñ‚ÑƒÑ€Ñ–Ð¹ÑÑŒÐºÐ°': (51.0833, 24.7000),
    # Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ñ–ÑÑ‚Ð° Ð· Ð²ÐµÐ»Ð¸ÐºÐ¸Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ
    'Ð²ÐµÐ»Ð¸ÐºÑ– Ð¼Ð¾ÑÑ‚Ð¸': (48.9167, 25.3333), 'Ð²ÐµÐ»Ð¸ÐºÐ¸Ñ… Ð¼Ð¾ÑÑ‚Ñ–Ð²': (48.9167, 25.3333), 'Ð²ÐµÐ»Ð¸ÐºÐ¸Ñ… Ð¼Ð¾ÑÑ‚Ð°Ñ…': (48.9167, 25.3333),
    'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²ÐºÐ°': (46.7500, 32.8167), 'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²ÐºÑƒ': (46.7500, 32.8167), 'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²Ñ†Ñ–': (46.7500, 32.8167),
    'Ð±Ð°Ñ€Ð°Ð½Ñ–Ð²ÐºÐ°': (50.3000, 27.6667), 'Ð±Ð°Ñ€Ð°Ð½Ñ–Ð²ÐºÑƒ': (50.3000, 27.6667), 'Ð±Ð°Ñ€Ð°Ð½Ñ–Ð²Ñ†Ñ–': (50.3000, 27.6667),
    'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹': (50.5833, 27.6167), 'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ð°-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¾Ð³Ð¾': (50.5833, 27.6167),
    'ÐºÑ€Ð°ÑÐ¸Ð»Ñ–Ð²': (49.6500, 27.1667), 'ÐºÑ€Ð°ÑÐ¸Ð»Ð¾Ð²Ñƒ': (49.6500, 27.1667), 'ÐºÑ€Ð°ÑÐ¸Ð»Ð¾Ð²Ñ–': (49.6500, 27.1667),
    'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²ÐºÐ°': (50.1833, 27.0667), 'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²ÐºÑƒ': (50.1833, 27.0667), 'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²Ñ†Ñ–': (50.1833, 27.0667),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð°': (50.3000, 26.8667), 'ÑÐ»Ð°Ð²ÑƒÑ‚Ñƒ': (50.3000, 26.8667), 'ÑÐ»Ð°Ð²ÑƒÑ‚Ñ–': (50.3000, 26.8667),
    'Ð½ÐµÑ‚Ñ–ÑˆÐ¸Ð½': (50.3333, 26.6333), 'Ð½ÐµÑ‚Ñ–ÑˆÐ¸Ð½Ñƒ': (50.3333, 26.6333), 'Ð½ÐµÑ‚Ñ–ÑˆÐ¸Ð½Ñ–': (50.3333, 26.6333),
    'Ð¾ÑÑ‚Ñ€Ð¾Ð³': (50.3333, 26.5167), 'Ð¾ÑÑ‚Ñ€Ð¾Ð³Ñƒ': (50.3333, 26.5167), 'Ð¾ÑÑ‚Ñ€Ð¾Ð·Ñ–': (50.3333, 26.5167),
    'Ð´ÑƒÐ±Ð½Ð¾': (50.4167, 25.7667), 'Ð´ÑƒÐ±Ð½Ñƒ': (50.4167, 25.7667), 'Ð´ÑƒÐ±Ð½Ñ–': (50.4167, 25.7667),
    'Ð²Ð°Ñ€Ð°Ñˆ': (51.3500, 25.8500), 'Ð²Ð°Ñ€Ð°ÑˆÑ–': (51.3500, 25.8500), 'Ð²Ð°Ñ€Ð°ÑˆÑƒ': (51.3500, 25.8500),
    'ÐºÐ¾ÑÑ‚Ð¾Ð¿Ñ–Ð»ÑŒ': (50.8833, 26.4500), 'ÐºÐ¾ÑÑ‚Ð¾Ð¿Ð¾Ð»ÑŽ': (50.8833, 26.4500), 'ÐºÐ¾ÑÑ‚Ð¾Ð¿Ð¾Ð»Ñ–': (50.8833, 26.4500),
    'ÑÐ°Ñ€Ð½Ð¸': (51.3333, 26.6000), 'ÑÐ°Ñ€Ð½Ð°Ð¼': (51.3333, 26.6000), 'ÑÐ°Ñ€Ð½Ð°Ñ…': (51.3333, 26.6000),
    'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ðµ': (50.9333, 26.1667), 'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ð¾Ð¼Ñƒ': (50.9333, 26.1667), 'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ð¾Ð³Ð¾': (50.9333, 26.1667),
    'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†Ñ': (51.5667, 26.5667), 'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†ÑŽ': (51.5667, 26.5667), 'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†Ñ–': (51.5667, 26.5667),
    'Ð±ÐµÑ€ÐµÐ·Ð½Ðµ': (51.4500, 26.7167), 'Ð±ÐµÑ€ÐµÐ·Ð½Ð¾Ð¼Ñƒ': (51.4500, 26.7167), 'Ð±ÐµÑ€ÐµÐ·Ð½Ð¾Ð³Ð¾': (51.4500, 26.7167),
    'ÑˆÐ¾ÑÑ‚ÐºÑƒ': (51.8667, 33.4833), 'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿Ñƒ': (51.2417, 33.2022), 'Ð½ÐµÐ´Ñ€Ð¸Ð³Ð°Ð¹Ð»Ñ–Ð²': (50.8281, 33.8781),
    'Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ñƒ': (50.5700, 33.7900), 'Ð½Ð¾ÑÑ–Ð²ÐºÑƒ': (50.9444, 32.0167), 'Ð±Ð°Ñ…Ð¼Ð°Ñ‡': (51.1808, 32.8203), 'Ð±Ð°Ñ…Ð¼Ð°Ñ‡Ð°': (51.1808, 32.8203)
    ,'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÐ°': (50.6767, 29.5283), 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÑƒ': (50.6767, 29.5283), 'Ð¿Ñ–ÑÐºÑ–Ð²Ñ†Ñ–': (50.6767, 29.5283)
    ,'Ð·Ñ–Ð½ÑŒÐºÑ–Ð²': (49.2019, 34.3744), 'Ð·Ñ–Ð½ÑŒÐºÐ¾Ð²Ñ–': (49.2019, 34.3744), 'Ð·Ñ–Ð½ÑŒÐºÑ–Ð²Ñƒ': (49.2019, 34.3744), 'Ð·Ñ–Ð½ÑŒÐºÐ¾Ð²Ð°': (49.2019, 34.3744)
}

# Donetsk Oblast cities (Ð¿Ð¾Ð²Ð½Ð¸Ð¹ Ð¿ÐµÑ€ÐµÐ»Ñ–Ðº Ð¼Ñ–ÑÑ‚ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–). Added per user request to ensure precise mapping.
# Sources: OpenStreetMap / GeoNames (approx to 4 decimal places). Using setdefault to avoid overriding existing entries.
DONETSK_CITY_COORDS = {
    'Ð´Ð¾Ð½ÐµÑ†ÑŒÐº': (48.0028, 37.8053),
    'Ð¼Ð°ÐºÑ–Ñ—Ð²ÐºÐ°': (48.0478, 37.9258),
    'Ð³Ð¾Ñ€Ð»Ñ–Ð²ÐºÐ°': (48.3336, 38.0925),
    'Ð¼Ð°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ': (47.0971, 37.5434),  # already present
    'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº': (48.7389, 37.5848),  # already present
    'ÑÐ»Ð¾Ð²\'ÑÐ½ÑÑŒÐº': (48.8417, 37.5983),  # already present
    'Ð´Ñ€ÑƒÐ¶ÐºÑ–Ð²ÐºÐ°': (48.6203, 37.5263),  # already present
    'ÐºÐ¾ÑÑ‚ÑÐ½Ñ‚Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.5277, 37.7050),  # already present
    'Ð±Ð°Ñ…Ð¼ÑƒÑ‚': (48.5937, 38.0000),
    'Ð°Ð²Ð´Ñ–Ñ—Ð²ÐºÐ°': (48.1417, 37.7425),
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐº': (48.2833, 37.1833),
    'Ð¼Ð¸Ñ€Ð½Ð¾Ð³Ñ€Ð°Ð´': (48.3000, 37.2667),
    'Ñ‚Ð¾Ñ€ÐµÑ†ÑŒÐº': (48.3976, 37.8687),
    'Ð´Ð¾Ð±Ñ€Ð¾Ð¿Ñ–Ð»Ð»Ñ': (48.4697, 37.0851),
    'ÑÐµÐ»Ð¸Ð´Ð¾Ð²Ðµ': (48.1500, 37.3000),
    'Ð½Ð¾Ð²Ð¾Ð³Ñ€Ð¾Ð´Ñ–Ð²ÐºÐ°': (48.2065, 37.3467),
    'Ð²Ð¾Ð»Ð½Ð¾Ð²Ð°Ñ…Ð°': (47.6000, 37.5000),
    'Ð²ÑƒÐ³Ð»ÐµÐ´Ð°Ñ€': (47.7811, 37.2358),
    'Ð»Ñ–Ð¼Ð°Ð½': (48.9890, 37.8020),
    'ÑÐ²ÑÑ‚Ð¾Ð³Ñ–Ñ€ÑÑŒÐº': (49.0339, 37.5663),  # already present
    'ÑÑ–Ð²ÐµÑ€ÑÑŒÐº': (48.8667, 38.1000),
    'ÑÐ¾Ð»ÐµÐ´Ð°Ñ€': (48.5356, 38.0875),
    'Ñ‡Ð°ÑÑ–Ð² ÑÑ€': (48.5969, 37.8350),
    'ÑˆÐ°Ñ…Ñ‚Ð°Ñ€ÑÑŒÐº': (48.0500, 38.4500),
    'Ñ”Ð½Ð°ÐºÑ–Ñ”Ð²Ðµ': (48.2333, 38.2000),
    'Ð°Ð¼Ð²Ñ€Ð¾ÑÑ–Ñ—Ð²ÐºÐ°': (47.7956, 38.4772),
    'Ð´ÐµÐ±Ð°Ð»ÑŒÑ†ÐµÐ²Ðµ': (48.3400, 38.4000),
    'Ð´Ð¾ÐºÑƒÑ‡Ð°Ñ”Ð²ÑÑŒÐº': (47.7489, 37.6789),
    'Ñ–Ð»Ð¾Ð²Ð°Ð¹ÑÑŒÐº': (47.9233, 38.1950),
    'Ð¶Ð´Ð°Ð½Ñ–Ð²ÐºÐ°': (48.1500, 38.2667),
    'Ð·ÑƒÐ³Ñ€ÐµÑ': (48.0167, 38.2667),
    'Ñ…Ð°Ñ€Ñ†Ð¸Ð·ÑŒÐº': (48.0400, 38.1500),
    'Ð²ÑƒÐ³Ð»ÐµÐ³Ñ–Ñ€ÑÑŒÐº': (48.3167, 38.2167),
    'ÑÑÐ¸Ð½ÑƒÐ²Ð°Ñ‚Ð°': (48.1167, 37.8333),
    'ÑÐ½Ñ–Ð¶Ð½Ðµ': (48.0333, 38.7667),
    'ÐºÐ°Ð»ÑŒÐ¼Ñ–ÑƒÑÑŒÐºÐµ': (47.6528, 38.0664),
    'Ð¼Ð¾ÑÐ¿Ð¸Ð½Ðµ': (47.8583, 38.0000),
    'ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐº': (48.0333, 37.9000),
    'Ñ€Ð¾Ð´Ð¸Ð½ÑÑŒÐºÐµ': (48.3500, 37.2000),
    'Ð·Ð°Ð»Ñ–Ð·Ð½Ðµ': (48.3539, 37.8483),
    # Historical / alt names not added to avoid noise; add normalization separately if needed.
}

for _dn_name, _dn_coords in DONETSK_CITY_COORDS.items():
    CITY_COORDS.setdefault(_dn_name, _dn_coords)

# Kharkiv Oblast cities and key settlements (Ð¼Ñ–ÑÑ‚Ð° + Ð²Ð°Ð¶Ð»Ð¸Ð²Ñ– ÑÐµÐ»Ð¸Ñ‰Ð°) per user request.
# Many already present; using setdefault to avoid override. Includes normalized variants.
KHARKIV_CITY_COORDS = {
    'Ñ–Ð·ÑŽÐ¼': (49.2103, 37.2483),
    'ÐºÑƒÐ¿\'ÑÐ½ÑÑŒÐº': (49.7106, 37.6156),
    'ÐºÑƒÐ¿ÑÐ½ÑÑŒÐº': (49.7106, 37.6156),  # variant without apostrophe
    'Ð»Ð¾Ð·Ð¾Ð²Ð°': (48.8897, 36.3175),
    'Ð»Ð¸Ð¿Ñ†Ñ–': (50.3061, 36.7597),  # ÑÐµÐ»Ð¾ Ð±Ñ–Ð»Ñ ÐºÐ¾Ñ€Ð´Ð¾Ð½Ñƒ Ð· Ð Ð¾ÑÑ–Ñ”ÑŽ
    'Ð¿ÐµÑ€Ð²Ð¾Ð¼Ð°Ð¹ÑÑŒÐºÐ¸Ð¹': (49.3914, 36.2147),
    'Ð²Ð¾Ð²Ñ‡Ð°Ð½ÑÑŒÐº': (50.3000, 36.9500),
    'Ð»ÑŽÐ±Ð¾Ñ‚Ð¸Ð½': (49.9486, 35.9292),
    'Ð´ÐµÑ€Ð³Ð°Ñ‡Ñ–': (50.1061, 36.1217),
    'Ð·Ð¼Ñ–Ñ—Ð²': (49.6897, 36.3472),
    'ÐºÑ€Ð°ÑÐ½Ð¾Ð³Ñ€Ð°Ð´': (49.3740, 35.4405),
    'Ð¿ÐµÑ‡ÐµÐ½Ñ–Ð³Ð¸': (49.8667, 36.9667),
    'Ð·Ð¾Ð»Ð¾Ñ‡Ñ–Ð²(Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°)': (50.2744, 36.3592),
    'Ð·Ð¾Ð»Ð¾Ñ‡Ñ–Ð²': (50.2744, 36.3592),  # may conflict with Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».; disambiguation via region context
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð±ÑƒÑ€Ð»ÑƒÐº': (50.0514, 37.3903),
    'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ðµ': (49.8667, 36.0500),
    'Ð¿Ð¾ÐºÐ¾Ñ‚Ð¸Ð»Ñ–Ð²ÐºÐ°': (49.9345, 36.0603),
    'Ð¼Ð°Ð½Ñ‡ÐµÐ½ÐºÐ¸': (49.9840, 35.9680),
    'Ð¼Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (49.6550, 36.7060),
    'ÐºÐ¾Ð»Ð¾Ð¼Ð°Ðº': (49.8422, 35.2761),
    'ÐºÐ¾Ð·Ð°Ñ‡Ð° Ð»Ð¾Ð¿Ð°Ð½ÑŒ': (49.8872, 36.4167),  # Ð¡ÐœÐ¢ Ð² Ð”ÐµÑ€Ð³Ð°Ñ‡Ñ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ Ñ€Ð°Ð¹Ð¾Ð½Ñ–
    'Ñ‡ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ': (49.7155296, 36.9322501),  # ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ðµ Ð§ÐºÐ°Ð»Ð¾Ð²ÑÑŒÐºÐµ Ð² Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð¿ÐµÑ€ÑˆÐ¾Ñ‚Ñ€Ð°Ð²Ð½ÐµÐ²Ð¸Ð¹': (49.3914, 36.2147),  # Ñ. ÐŸÐµÑ€ÑˆÐ¾Ñ‚Ñ€Ð°Ð²Ð½ÐµÐ²Ðµ, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (same as ÐŸÐµÑ€Ð²Ð¾Ð¼Ð°Ð¹ÑÑŒÐºÐ¸Ð¹)
    'ÑÑ‚Ð²Ð¾Ñ€ Ð½Ð°ÑÐµÐ»ÐµÐ½Ð¾Ð³Ð¾ Ð¿ÑƒÐ½ÐºÑ‚Ñƒ Ð±Ð°Ð»ÐºÐ¸?': (49.4627, 36.8586),  # placeholder example â€“ remove/replace if noise
}

for _kh_name, _kh_coords in KHARKIV_CITY_COORDS.items():
    CITY_COORDS.setdefault(_kh_name, _kh_coords)

# Chernihiv Oblast cities / key settlements (Ð¼Ñ–ÑÑ‚Ð° Ñ‚Ð° Ð²Ð°Ð¶Ð»Ð¸Ð²Ñ– ÑÐµÐ»Ð¸Ñ‰Ð°)
# Many base ones already in CITY_COORDS (Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð², Ð½Ñ–Ð¶Ð¸Ð½, Ð¿Ñ€Ð¸Ð»ÑƒÐºÐ¸, Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹, ÐºÐ¾Ñ€Ð¾ÑÑ‚ÐµÐ½ÑŒ (Ñ–Ð½ÑˆÐ° Ð¾Ð±Ð».), ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÐ° maybe missing).
CHERNIHIV_CITY_COORDS = {
    'Ð½Ñ–Ð¶Ð¸Ð½': (51.0480, 31.8860),  # already present
    'Ð¿Ñ€Ð¸Ð»ÑƒÐºÐ¸': (50.5931, 32.3878),  # already present
    'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹': (51.9874, 33.2620),  # already present
    'ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÐ°': (51.7725, 32.2494),
    'ÐºÐ¾Ð·ÐµÐ»ÐµÑ†ÑŒ': (51.5625, 31.2058),  # Added
    'Ð½Ð¾ÑÑ–Ð²ÐºÐ°': (51.0325, 31.5522),   # Added
    'ÐºÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÐ°': (51.3667, 32.2000),  # Added
    'Ð¼ÐµÐ½Ð°': (51.5211, 32.2147),      # Fixed typo
    'Ñ–Ñ‡Ð½Ñ': (51.0722, 32.3931),      # Added
    'Ð±Ð¾Ñ€Ð·Ð½Ð°': (51.2542, 32.4192),  # already present
    'Ð±Ð°Ñ‚Ð¸Ñ—Ð²ÐºÐ°?': (51.4982, 31.2893),  # placeholder if appears; else remove
    'Ð¼ÐµÐ½a': (51.5211, 32.2147),  # variant with latin a? (typo guard)
    'Ð¼ ÐµÐ½Ð°': (51.5211, 32.2147),  # spacing anomaly fallback
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ°(Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°)': (52.1833, 32.5833),  # north settlement (if referenced)
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ° Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°': (52.1833, 32.5833),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ°': (52.1833, 32.5833),  # might conflict with Poltava one; context disambiguation may be needed
    'ÑÐ½Ð¾Ð²ÑÑŒÐº': (51.8200, 31.9500),
    'ÐºÐ¾Ñ€Ð¾Ð¿': (51.5667, 32.9667),
    'Ñ–Ñ…Ð½Ñ': (51.0722, 32.3931),  # misspelling variant of Ñ–Ñ‡Ð½Ñ
    'Ð¸Ñ‡Ð½Ñ': (51.0722, 32.3931),  # alt transliteration
    'Ð³Ð»ÑƒÑ…Ñ–Ð²?': (51.6781, 33.9169),  # actually Sumy oblast; placeholder if mis-tag appears
    'ÑÐ¾ÑÐ½Ð¸Ñ†Ñ': (51.5236, 32.4953),  # already present
    'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿?': (51.2417, 33.2022),  # Sumy oblast - guard only
    'Ð¾ÑÑ‚ÐµÑ€': (50.9481, 30.8831),  # already present
    'Ð½Ñ–Ð¶Ð¸Ð½Ñƒ': (51.0480, 31.8860),  # accusative
    'Ð±Ð¾Ñ€Ð·Ð½Ñ–': (51.2542, 32.4192),
    'ÐºÐ¾Ñ€Ð¾Ð¿Ñ–': (51.5667, 32.9667),
    'ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²Ñ†Ñ–': (51.7725, 32.2494),
    'ÐºÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÑƒ': (51.7725, 32.2494),
    'ÑÐ½Ð¾Ð²ÑÑŒÐºÑƒ': (51.8200, 31.9500),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²Ñ†Ñ–': (52.1833, 32.5833),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÑƒ': (52.1833, 32.5833),
    # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð¸ Ñ„Ð¾Ñ€Ð¼Ñ‹
    'ÑÐµÐ´Ð½Ñ–Ð²': (51.5211, 32.1897),
    'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´': (51.9874, 33.2620),  # Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹
    'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹': (51.9874, 33.2620),
}

for _ch_name, _ch_coords in CHERNIHIV_CITY_COORDS.items():
    CITY_COORDS.setdefault(_ch_name, _ch_coords)

# Dnipropetrovsk (Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°) Oblast cities & key settlements.
DNIPRO_CITY_COORDS = {
    'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³': (47.9105, 33.3918),  # already implied in stems
    'Ð¶Ñ–Ð¾Ð²Ñ‚Ñ– Ð²Ð¾Ð´Ð¸': (48.3456, 33.5022),  # typo guard for Ð¶Ð¾Ð²Ñ‚Ñ– Ð²Ð¾Ð´Ð¸
    'Ð¶Ð¾Ð²Ñ‚Ñ– Ð²Ð¾Ð´Ð¸': (48.3456, 33.5022),
    'ÐºÐ°Ð¼\'ÑÐ½ÑÑŒÐºÐµ': (48.5110, 34.6021),  # already present as variant
    'ÐºÐ°Ð¼ÑÐ½ÑÑŒÐºÐµ': (48.5110, 34.6021),  # present
    'Ð½Ñ–ÐºÐ¾Ð¿Ð¾Ð»ÑŒ': (47.5667, 34.4061),  # present
    'Ð¼Ð°Ñ€Ð³Ð°Ð½ÐµÑ†ÑŒ': (47.6433, 34.6289),  # present
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²': (47.6542, 34.1167),
    'Ñ‚ÐµÑ€Ð½Ñ–Ð²ÐºÐ°': (48.5319, 36.0681),
    'Ð¿ÐµÑ€ÑˆÐ¾Ñ‚Ñ€Ð°Ð²ÐµÐ½ÑÑŒÐº': (48.3460, 36.4030),
    'Ð²Ñ–Ð»ÑŒÐ½Ð¾Ð³Ñ–Ñ€ÑÑŒÐº': (48.4850, 34.0300),
    'Ð¶Ð¾Ð²Ñ‚Ñ–': (48.3456, 33.5022),  # truncated mention mapping
    'Ð½Ð¾Ð²Ð¾Ð¼Ð¾ÑÐºÐ¾Ð²ÑÑŒÐº': (48.6333, 35.2167),
    'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ': (48.15, 35.2),  # Ð—Ð°Ñ€Ñ–Ñ‡Ð½Ðµ, ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½, Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°': (48.15, 35.2),  # Ð¡Ð¿ÐµÑ†Ñ–Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ð²Ñ–Ð¹ÑÑŒÐºÐ¾Ð²Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ñƒ  
    'Ð·Ð°Ñ€Ñ–Ñ‡Ð½Ðµ Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': (48.15, 35.2),  # ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ð¸Ð¹ ÐºÐ»ÑŽÑ‡
    'ÑÐ¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²Ðµ': (48.3167, 35.5167),
    'Ð¿ÐµÑ‚Ñ€Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÐ°': (48.5000, 36.4500),  # present
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ(Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°)': (48.1180, 36.2470),
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°': (48.1180, 36.2470),
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸': (48.1180, 36.2470),
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ': (48.1180, 36.2470),  # present
    'Ð±Ð¾Ð³Ð´Ð°Ð½Ñ–Ð²ÐºÐ°(Ð´Ð½Ñ–Ð¿Ñ€Ð¾)': (48.4647, 35.0462),  # fallback to oblast center if ambiguous
    'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÑ–Ð²ÐºÐ°': (48.3550, 36.1240),
    'Ð²Ð°Ñ€Ð²Ð°Ñ€Ñ–Ð²ÐºÐ°': (48.7440, 34.7000),
    'Ð²ÐµÑ€Ñ…Ñ–Ð²Ñ†ÐµÐ²Ðµ': (48.4769, 34.3458),
    'Ð²ÐµÑ€Ñ…Ð½ÑŒÐ¾Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐº': (48.6535, 34.3372),  # present
    'Ð³ÑƒÐ±Ð¸Ð½Ð¸Ñ…Ð°': (48.7437, 35.2960),  # present
    'Ð´Ð¾Ð¼Ð¾Ñ‚ÐºÐ°Ð½ÑŒ': (48.6680, 34.2160),
    'Ð¶ÐµÑ€ÐµÐ±ÐµÑ‚Ñ–Ð²ÐºÐ°': (48.2500, 36.7000),
    'Ð·Ð°Ð¹Ñ†ÐµÐ²Ðµ(Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°)': (48.4647, 35.0462),  # generic fallback center
    'Ð·ÐµÐ»ÐµÐ½Ð¸Ð¹ Ð³Ð°Ð¹': (48.4200, 35.1200),
    'Ð·ÐµÐ»ÐµÐ½Ð¾Ð´Ð¾Ð»ÑŒÑÑŒÐº': (47.5667, 33.5333),  # present
    'ÐºÐ°Ñ€Ð½Ð°ÑƒÑ…Ñ–Ð²ÐºÐ°': (48.4870, 34.5480),
    'ÐºÐ°Ñ€Ð¿Ñ–Ð²ÐºÐ°': (47.5930, 33.5960),
    'Ð¼Ð°Ð»Ð¾Ð¼Ð¸Ñ…Ð°Ð¹Ð»Ñ–Ð²ÐºÐ°': (48.2300, 36.4500),
    'Ð¼ÐµÐ»Ñ–Ð¾Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ðµ': (48.6340, 35.1750),
    'Ð¼Ð°Ð³Ñ–Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.8836, 34.8669),  # typo guard for Ð¼Ð°Ð³Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°
    'Ð¼Ð°Ð³Ð´Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.8836, 34.8669),  # present
    'Ð¼ÐµÐ¶Ð¾Ð²Ð°': (48.2583, 36.7363),  # present
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°(Ð´Ð½Ñ–Ð¿Ñ€Ð¾)': (48.4647, 35.0462),
    'Ð½Ð¾Ð²Ð¾Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°(Ð´Ð½Ñ–Ð¿Ñ€Ð¾)': (48.4647, 35.0462),
    'Ð¾Ð±ÑƒÑ…Ñ–Ð²ÐºÐ°': (48.6035, 34.8530),  # present
    'Ð¾Ñ€Ð»Ñ–Ð²Ñ‰Ð¸Ð½Ð°': (48.6110, 34.9550),
    'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´': (48.5350, 35.8700),  # present
    'Ð¿ÐµÑ€ÐµÑ‰ÐµÐ¿Ð¸Ð½Ðµ': (48.6260, 35.3580),  # present
    'Ð¿ÐµÑ‚Ñ€Ð¸ÐºÑ–Ð²ÐºÐ°': (48.7330, 34.6300),  # present
    'Ð¿Ñ–Ð´Ð³Ð¾Ñ€Ð¾Ð´Ð½Ðµ': (48.5747, 35.1482),  # present
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ (ÑÐ¼Ñ‚)': (48.1180, 36.2470),
    'Ñ€Ð°Ð´ÑƒÑˆÐ½Ðµ': (47.9840, 33.4930),
    'ÑÐ°Ð¼Ð°Ñ€': (48.6500, 35.4200),  # present
    'ÑÑƒÑ€ÑÑŒÐºÐ¾-Ð»Ð¸Ñ‚Ð¾Ð²ÑÑŒÐºÐµ': (48.3720, 34.8130),
    'Ñ‚ÐµÑ€Ð½Ñ–Ð²ÑÑŒÐºÑ– Ñ…ÑƒÑ‚Ð¾Ñ€Ð¸': (48.6600, 34.9400),
    'Ñ‚Ð¾Ð¼Ð°ÐºÑ–Ð²ÐºÐ°': (47.8130, 34.7450),
    'Ñ†Ð°Ñ€Ð¸Ñ‡Ð°Ð½ÐºÐ°': (48.9767, 34.3772),  # present
    'Ñ‡ÑƒÐ¼Ð°ÐºÐ¾Ð²Ðµ': (48.3400, 35.2800),
    'ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐµ(Ð´Ð½Ñ–Ð¿Ñ€Ð¾)': (48.4647, 35.0462),
    'ÑŽÑ€â€™Ñ—Ð²ÐºÐ°': (48.7250, 36.0130),
    'ÑŽÑ€'"'"'Ñ—Ð²ÐºÐ°': (48.7250, 36.0130),  # attempt to guard variant â€“ may adjust quoting
    'ÑŽÑ€Ñ—Ð²ÐºÐ°': (48.7250, 36.0130),
    'ÑŽÑ€â€™Ñ—Ð²ÐºÑƒ': (48.7250, 36.0130),
    'ÑŽÑ€'"'"'Ñ—Ð²ÐºÑƒ': (48.7250, 36.0130),
}

for _dp_name, _dp_coords in DNIPRO_CITY_COORDS.items():
    CITY_COORDS.setdefault(_dp_name, _dp_coords)

# Sumy (Ð¡ÑƒÐ¼ÑÑŒÐºÐ°) Oblast cities & key settlements.
SUMY_CITY_COORDS = {
    'ÑÑƒÐ¼Ð¸': (50.9077, 34.7981),  # already present
    'ÑˆÐ¾ÑÑ‚ÐºÐ°': (51.8733, 33.4800),
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº?': (49.0670, 33.4204),  # ignore (other oblast) placeholder if mis-ref
    'Ð¾Ñ…Ñ‚Ð¸Ñ€ÐºÐ°': (50.3116, 34.8988),
    'Ñ€ Ð¾Ð¼Ð½Ð¸': (50.7497, 33.4746),  # space anomaly guard
    'Ñ€Ð¾Ð¼Ð½Ð¸': (50.7497, 33.4746),  # present
    'Ðº Ð¾ Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ': (50.4833, 34.9667),  # anomaly
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ': (50.4833, 34.9667),  # present
    'Ð³Ð»ÑƒÑ…Ñ–Ð²': (51.6781, 33.9169),  # present
    'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿': (51.2417, 33.2022),  # present
    'Ð»Ñ–Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°': (50.5700, 33.7900),  # typo
    'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°': (50.5700, 33.7900),  # present
    'Ð±ÑƒÑ€Ð¸Ð½ÑŒ': (51.2000, 33.8500),  # present
    'Ð¿ÑƒÑ‚Ñ–Ð²Ð»ÑŒ': (51.3375, 33.8700),
    'ÑÐµÑ€ÐµÐ´Ð¸Ð½Ð°-Ð±ÑƒÐ´Ð°': (52.1900, 33.9300),
    'Ð»ÐµÐ±ÐµÐ´Ð¸Ð½': (50.5872, 34.4912),  # present
    'Ð½ÐµÐ´Ñ€Ð¸Ð³Ð°Ð¹Ð»Ñ–Ð²': (50.8281, 33.8781),  # present
    'Ð² ÑÑƒÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÐ°?': (50.9077, 34.7981),  # fallback noise
    'Ð±Ñ–Ð»Ð¾Ð¿Ñ–Ð»Ð»Ñ': (51.1500, 34.3014),  # present
    'ÐºÑ€Ð°ÑÐ½Ð¾Ð¿Ñ–Ð»Ð»Ñ': (50.4422, 35.3081),  # present
    'ÑÑƒÐ¼ÑÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ– Ñ†ÐµÐ½Ñ‚Ñ€': (50.9077, 34.7981),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð¿Ð¸ÑÐ°Ñ€Ñ–Ð²ÐºÐ°': (50.4250, 35.4650),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð¿Ð¸ÑÐ°Ñ€Ñ–Ð²ÐºÐ° ÑÐ¼Ñ‚': (50.4250, 35.4650),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð¿Ð¸ÑÐ°Ñ€Ñ–Ð²ÐºÑƒ': (50.4250, 35.4650),
    'Ð±Ñ€Ð°Ð½Ñ†Ñ–Ð²ÐºÐ°?': (50.9077, 34.7981),
    'Ð´Ñ€ÑƒÐ¶Ð±Ð°(ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°)': (51.5230, 34.5770),
    'Ð´Ñ€ÑƒÐ¶Ð±Ð°': (51.5230, 34.5770),
    'Ð·Ð¾Ñ€Ð¸Ð½Ðµ?': (50.9077, 34.7981),
    'ÐºÐ¸Ñ€ Ð¸Ðº Ñ–Ð²ÐºÐ°?': (51.2000, 33.8500),
    'Ðº Ð¸ Ñ€Ð¸Ðº Ñ–Ð²ÐºÐ°': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€Ð·Ð°ÐºÑ–Ð²ÐºÐ°?': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸Ðº Ñ–Ð²ÐºÑƒ': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸ÐºÑ–Ð²ÐºÐ°': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸ÐºÑ–Ð²ÐºÑƒ': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸Ðº Ñ–Ð²Ñ†Ñ–': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸Ðº Ñ–Ð²Ñ†Ñ–?': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸ÐºÑ–Ð²Ñ†Ñ–': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸ÐºÑ–Ð²Ñ†Ñ–?': (51.2000, 33.8500),
    'ÐºÐ¸Ñ€ Ð¸Ñ‡Ñ–Ð²ÐºÐ°?': (51.2000, 33.8500),
    'Ðº Ð¸Ñ€Ð¸Ðº Ñ–Ð²ÐºÐ°': (51.2000, 33.8500),
    'ÑÐ¼Ð¾Ñ€Ð¾Ð´Ð¸Ð½Ðµ': (50.9660, 34.5500),
    'ÑÐ¼Ð¾Ñ€Ð¾Ð´Ð¸Ð½Ð¾': (50.9660, 34.5500),
    'ÑÐ¼Ð¾Ñ€Ð¾Ð´Ð¸Ð½Ð¾Ð¼Ñƒ': (50.9660, 34.5500),
    'ÑÐ¼Ð¾Ñ€Ð¾Ð´Ð¸Ð½Ð¾Ð³Ð¾': (50.9660, 34.5500),
    'ÑÑÐµÐ½Ð¾Ðº?': (51.5230, 34.5770),
    'ÑÑÐµÐ½Ð¾Ðº': (51.5230, 34.5770),
    'ÑÑÐµÐ½ÐºÑƒ': (51.5230, 34.5770),
    'ÑÑÐµÐ½ÐºÑƒ?': (51.5230, 34.5770),
    'ÑÑÐµÐ½ÐºÐ°': (51.5230, 34.5770),
    'ÑÑÐµÐ½ÐºÐ°?': (51.5230, 34.5770),
    'ÑÑÐµÐ½Ñ†Ñ–': (51.5230, 34.5770),
    'ÑÑÐµÐ½Ñ†Ñ–?': (51.5230, 34.5770),
    'ÑÑÐµÐ½Ñ†Ñ–Ð²': (51.5230, 34.5770),
    'ÑÑÐµÐ½Ñ†Ñ–Ð²?': (51.5230, 34.5770),
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°(ÑÑƒÐ¼ÑÑŒÐºÐ°)': (51.5667, 34.1333),  # ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°, Ñ€Ð°Ð¹Ð¾Ð½Ð½Ð¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€ Ð¡ÑƒÐ¼ÑÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° (ÑÑƒÐ¼ÑÑŒÐºÐ°)': (51.5667, 34.1333),  # Ð· Ð¿Ñ€Ð¾Ð±Ñ–Ð»Ð¾Ð¼
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÑƒ(ÑÑƒÐ¼ÑÑŒÐºÐ°)': (51.5667, 34.1333),  # Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ð´ÐµÐ¶
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÑƒ (ÑÑƒÐ¼ÑÑŒÐºÐ°)': (51.5667, 34.1333),  # Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ð´ÐµÐ¶ Ð· Ð¿Ñ€Ð¾Ð±Ñ–Ð»Ð¾Ð¼
}

for _sm_name, _sm_coords in SUMY_CITY_COORDS.items():
    CITY_COORDS.setdefault(_sm_name, _sm_coords)

# Zaporizhzhia (Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°) Oblast cities & key settlements.
ZAPORIZHZHIA_CITY_COORDS = {
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ': (47.8388, 35.1396),  # already present
    'Ð±ÐµÑ€Ð´ÑÐ½ÑÑŒÐº': (46.7553, 36.7885),  # present
    'Ð¼ÐµÐ»Ñ–Ñ‚Ð¾Ð¿Ð¾Ð»ÑŒ': (46.8489, 35.3650),  # present
    'ÐµÐ½ÐµÑ€Ð³Ð¾Ð´Ð°Ñ€': (47.4980, 34.6580),
    'Ñ‚Ð¾ÐºÐ¼Ð°Ðº': (47.2550, 35.7120),
    'Ð¿Ð¾Ð»Ð¾Ð³Ð¸': (47.4768, 36.2543),
    'Ð²Ñ–Ð»ÑŒÐ½ÑÐ½ÑÑŒÐº': (47.9450, 35.4350),
    'Ð¾Ñ€Ñ–Ñ…Ñ–Ð²': (47.5672, 35.7814),
    'Ð³ÑƒÐ»ÑÐ¹Ð¿Ð¾Ð»Ðµ': (47.6611, 36.2567),
    'Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÑŒÐº': (46.7310, 36.3440),
    'Ð´Ð°Ð·Ð¾Ð²ÑÑŒÐº': (47.8388, 35.1396),  # typo guard (Ð°Ð·Ð¾Ð²ÑÑŒÐº?) fallback
    'Ð°Ð·Ð¾Ð²ÑÑŒÐº?': (47.8388, 35.1396),
    'Ð²Ð°ÑÐ¸Ð»ÑŒÑ–Ð²ÐºÐ°': (47.4393, 35.2745),
    'Ð´Ð¾Ñ€Ð¾Ð¶Ð½ÑÐ½ÐºÐ°': (47.5540, 36.0950),
    'Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸Ð½Ðµ': (47.3780, 35.9300),
    'Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸Ð½Ð¾Ð¼Ñƒ': (47.3780, 35.9300),
    'Ñ€Ð¾Ð±Ð¾Ñ‚Ð¸Ð½Ð¾Ð³Ð¾': (47.3780, 35.9300),
    'Ñ€Ð°Ð±Ð¾Ñ‚Ð¸Ð½Ðµ': (47.3780, 35.9300),  # transliteration variant
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÐºÐ°(Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ)': (47.0870, 36.2320),
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÐºÐ°': (47.0870, 36.2320),
    'Ð¼Ð¸Ñ…Ð°Ð¹Ð»Ñ–Ð²ÐºÐ°(Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ)': (47.2730, 35.2200),
    'Ð¼Ð¸Ñ…Ð°Ð¹Ð»Ñ–Ð²ÐºÐ°': (47.2730, 35.2200),  # may conflict (other oblast) â€“ context disambiguation
    'ÐºÐ¾ÑÑ‚ÑÐ½Ñ‚Ð¸Ð½Ñ–Ð²ÐºÐ°(Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ)': (47.2460, 35.3220),  # small settlement, not Donetsk one
    'ÑÑ‚ÐµÐ¿Ð½Ð¾Ð³Ñ–Ñ€ÑÑŒÐº': (47.5660, 35.2850),
    'ÐºÐ°Ð¼ÑÐ½ÐºÐ°-Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ°': (47.4980, 34.4000),
    'ÐºÐ°Ð¼\'ÑÐ½ÐºÐ°-Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ°': (47.4980, 34.4000),
    'ÐºÐ¸Ñ€Ð¿Ð¾Ñ‚Ð¸Ð½Ðµ?': (47.3780, 35.9300),  # noise variant to robotyne
    'Ð¼Ð°Ð»Ð¾ÐºÐ°Ñ‚ÐµÑ€Ð¸Ð½Ñ–Ð²ÐºÐ°': (47.7040, 35.2710),
    'ÐºÐ¾Ð¼Ð¸ÑˆÑƒÐ²Ð°Ñ…Ð°(Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°)': (47.5760, 35.5090),
    'ÐºÐ¾Ð¼Ð¸ÑˆÑƒÐ²Ð°Ñ…Ð°': (47.5760, 35.5090),  # duplicate name in other oblasts
    'ÐºÐ¾Ð¼Ð¸ÑˆÑƒÐ²Ð°ÑÑ–': (47.5760, 35.5090),
    'ÐºÐ¾Ð¼Ð¸ÑˆÑƒÐ²Ð°Ñ…Ñƒ': (47.5760, 35.5090),
    'Ñ‡ÐµÑ€Ð³Ð¾Ð²Ðµ?': (47.8388, 35.1396),
    'Ð½ Ð¾Ð²Ðµ?': (47.8388, 35.1396),
    'Ð½Ð¾Ð²Ð¾Ð²Ð°ÑÐ¸Ð»Ñ–Ð²ÐºÐ°': (47.3290, 35.5130),
    'Ð¼Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°(Ð·Ð°Ð¿)': (47.6260, 35.8700),
    'Ð¼Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ° Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°': (47.6260, 35.8700),
    'Ð¼Ð°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (47.6260, 35.8700),  # may conflict with Kharkiv one
    'Ð²ÐµÑÐµÐ»Ðµ(Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ)': (47.1700, 35.1750),
    'Ð²ÐµÑÐµÐ»Ðµ': (47.1700, 35.1750),  # generic name multiple oblasts
    'Ð±Ð°Ð»Ð°Ð±Ð¸Ð½Ðµ': (47.7520, 35.1660),
    'ÐºÑƒÑˆÑƒÐ³ÑƒÐ¼': (47.7630, 35.2200),
    'Ñ‚ÐµÑ€Ð½ÑƒÐ²Ð°Ñ‚Ðµ': (47.8520, 35.5560),
    'Ñ‡Ð°Ð¹ÐºÐ¸Ð½Ðµ?': (47.8388, 35.1396),
}

for _zp_name, _zp_coords in ZAPORIZHZHIA_CITY_COORDS.items():
    CITY_COORDS.setdefault(_zp_name, _zp_coords)

# Poltava (ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°) Oblast cities & key settlements.
POLTAVA_CITY_COORDS = {
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°': (49.5883, 34.5514),  # already present
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº': (49.0670, 33.4204),  # present
    'Ð³Ð¾Ñ€Ñ–ÑˆÐ½Ñ– Ð¿Ð»Ð°Ð²Ð½Ñ–': (49.0123, 33.6450),  # present
    'Ð»ÑƒÐ±Ð½Ð¸': (50.0186, 32.9931),  # present
    'Ð¼Ð¸Ñ€Ð³Ð¾Ñ€Ð¾Ð´': (49.9688, 33.6083),
    'Ð³Ð°Ð´ÑÑ‡': (50.3713, 34.0109),  # present
    'ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÐ°': (49.4586, 35.1272),  # present
    'Ñ€ÐµÑˆÐµÑ‚Ð¸Ð»Ñ–Ð²ÐºÐ°': (49.5630, 34.0720),
    'Ð¿Ð¸Ñ€ÑÑ‚Ð¸Ð½': (50.2444, 32.5144),
    'Ñ…Ð¾Ñ€Ð¾Ð»': (49.7850, 33.2200),
    'Ñ‡ÑƒÑ‚Ð¾Ð²Ðµ': (49.7070, 35.0960),
    'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ': (49.3958, 33.2664),
    'Ð³Ð¾Ð»Ñ‚Ð²a?': (49.5883, 34.5514),  # noise
    'Ð½Ð¾Ð²Ñ– ÑÐ°Ð½Ð¶Ð°Ñ€Ð¸': (49.3280, 34.3170),
    'Ð²ÐµÐ»Ð¸ÐºÑ– ÑÐ¾Ñ€Ð¾Ñ‡Ð¸Ð½Ñ†Ñ–': (50.0167, 33.9833),  # present
    'Ð´Ð¸ÐºÐ¸ÑÑ‚Ð°Ð½ÑŒ?': (49.9688, 33.6083),  # noise dykanka variant
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÑƒ': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÑƒ': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÐ¸': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ¸': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ¾ÑŽ?': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ¾ÑŽ.': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½ÐºÐ¾ÑŽ,': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÐ¾ÑŽ,': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½ÐºÐ¾ÑŽ.': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ñ†Ñ–?': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ñ†Ñ–?': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ð¾Ðº?': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ð¾Ðº': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ð¾Ðº?': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ð¾Ðº': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ñ†Ñ–,': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ñ†Ñ–,': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ð¾Ðº.': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ð¾Ðº.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ð¾Ðº,': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ð¾Ðº,': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½ÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½ÐºÑƒ': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ Ðº Ð°Ð½Ñ†Ñ–.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ°Ð½Ñ†Ñ–.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ñ†Ñ–.': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ñ†Ñ–.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½ÐºÐ¾ÑŽ.': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ¾ÑŽ.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½ÐºÐ¾ÑŽ,': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ¾ÑŽ,': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÐºÑƒ': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÑ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ°?': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ°.': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ°,': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ñ†Ñ–?': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ð¾Ðº': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ð¾Ðº?': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ð¾Ðº.': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ð¾Ðº,': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ° Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ð¾Ðº': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ð¾Ðº?': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ð¾Ðº.': (49.8214, 34.5769),
    'Ð´Ð¸ ÐºÐ° Ð½Ð¾Ðº,': (49.8214, 34.5769),
    'Ð·Ð°Ð²Ð¾Ð´ÑÑŒÐºÐµ': (50.0750, 33.4000),
    'Ð¼Ð°ÑˆÑ–Ð²ÐºÐ°': (49.4410, 34.8680),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ° (Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°)': (50.6633, 32.3933),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ° Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°': (50.6633, 32.3933),
    'ÑÐµÐ¼ÐµÐ½Ñ–Ð²ÐºÐ°': (50.6633, 32.3933),  # conflict with others
    'ÐºÐ¾Ð±ÐµÐ»ÑÐºÐ¸': (49.1500, 34.2000),  # present
    'Ñ‡Ð¾Ñ€Ð½ÑƒÑ…Ð¸': (50.2833, 33.0000),  # present
    'ÑˆÐ¸ÑˆÐ°ÐºÐ¸': (49.8992, 34.0072),  # present
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÐºÐ°?': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÑŒÑ†Ñ–?': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÐºÑƒ': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÐºÐ¾ÑŽ,': (49.8214, 34.5769),
    'Ð´Ð¸ÐºÐ°Ð½ÐºÐ¾ÑŽ.': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½ÐºÐ°': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½Ñ†Ñ–': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½ÐºÐ¾ÑŽ': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½Ð¾Ðº': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½Ð¾Ðº?': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½Ð¾Ðº.': (49.8214, 34.5769),
    'Ð´Ð¸Ðº Ð°Ð½Ð¾Ðº,': (49.8214, 34.5769),
    
    # ========== CITY+OBLAST SPECIFIC COORDINATES ==========
    # These entries resolve ambiguous city names by including oblast context
    
    # Ð¡Ñ€Ñ–Ð±Ð½Ðµ (different cities in different oblasts)
    'ÑÑ€Ñ–Ð±Ð½Ðµ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°': (51.1300, 31.9400),  # Ð¡Ñ€Ñ–Ð±Ð½Ðµ, Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ  
    'ÑÑ€Ñ–Ð±Ð½Ðµ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (51.1300, 31.9400),
    'ÑÑ€Ñ–Ð±Ð½Ðµ (Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».)': (51.1300, 31.9400),
    'ÑÑ€Ñ–Ð±Ð½Ðµ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': (51.1300, 31.9400),
    'ÑÑ€Ñ–Ð±Ð½Ðµ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ñ–': (51.1300, 31.9400),
    'ÑÑ€Ñ–Ð±Ð½Ðµ': (51.1300, 31.9400),  # Default to Chernihiv oblast variant
    
    # Ð—Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ (fixing incorrect coordinates - was pointing to Donetsk)
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°': (49.9800, 35.5300),  # Ð—Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (correct)
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.9800, 35.5300),
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ (Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».)': (49.9800, 35.5300),
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': (49.9800, 35.5300),
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ñ–': (49.9800, 35.5300),
    # Keep old incorrect entry as fallback for other messages, but correct default
    'Ð·Ð»Ð°Ñ‚Ð¾Ð¿Ñ–Ð»ÑŒ': (49.9800, 35.5300),  # Override with correct Kharkiv oblast coordinates
    
    # Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - Ð´Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð¼Ñ–ÑÑ‚Ð°
    'Ð»ÑŽÐ±ÐµÑ‡': (51.4961, 30.2675),  # Ð›ÑŽÐ±ÐµÑ‡, Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'Ð»ÑŽÐ±ÐµÑ‡ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°': (51.4961, 30.2675),
    'Ð»ÑŽÐ±ÐµÑ‡ Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (51.4961, 30.2675),
    'Ð»ÑŽÐ±ÐµÑ‡ (Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».)': (51.4961, 30.2675),
}

for _pl_name, _pl_coords in POLTAVA_CITY_COORDS.items():
    CITY_COORDS.setdefault(_pl_name, _pl_coords)

# Mykolaiv (ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°) Oblast cities & key settlements.
MYKOLAIV_CITY_COORDS = {
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²': (46.9750, 31.9946),  # already present
    'Ð¿ÐµÑ€Ð²Ð¾Ð¼Ð°Ð¹ÑÑŒÐº': (48.0449, 30.8500),  # not to confuse with ÐŸÐµÑ€Ð²Ð¾Ð¼Ð°Ð¹ÑÑŒÐºÐ¸Ð¹ (Kharkiv)
    'Ð²Ð¾Ð·Ð½ÐµÑÐµÐ½ÑÑŒÐº': (47.5679, 31.3336),
    'ÑŽÐ¶Ð½Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐº': (47.8178, 31.1800),
    'Ð½Ð¾Ð²Ð¸Ð¹ Ð±ÑƒÐ³': (47.6833, 32.5167),  # present
    'Ð½Ð¾Ð²Ð° Ð¾Ð´ÐµÑÐ°': (47.3100, 31.7830),
    'Ð½Ð¾Ð²Ñƒ Ð¾Ð´ÐµÑÑƒ': (47.3100, 31.7830),
    'Ð½Ð¾Ð²Ñ–Ð¹ Ð¾Ð´ÐµÑÑ–': (47.3100, 31.7830),
    'Ð¾Ñ‡Ð°ÐºÑ–Ð²': (46.6167, 31.5500),
    'Ð¾Ñ‡Ð°ÐºÐ¾Ð²': (46.6167, 31.5500),  # russian variant
    'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²ÐºÐ°': (47.0750, 32.8050),
    'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²ÐºÑƒ': (47.0750, 32.8050),
    'ÑÐ½Ñ–Ð³ÑƒÑ€Ñ–Ð²Ñ†Ñ–': (47.0750, 32.8050),
    'ÐºÐ°Ð·Ð°Ð½ÐºÐ°': (47.8460, 32.8460),
    'Ð°Ñ€Ð±Ð°Ñ‚ÑÑŒÐºÐ° ÑÑ‚Ñ€Ñ–Ð»ÐºÐ°?': (46.9750, 31.9946),  # noise guard
    'Ð´Ð¾Ð¼Ð°Ð½Ñ–Ð²ÐºÐ°': (47.6290, 30.9920),
    'Ð²Ñ€Ð°Ð´Ñ–Ñ—Ð²ÐºÐ°': (47.8820, 30.5910),
    'ÐºÑ€Ð¸Ð²Ðµ Ð¾Ð·ÐµÑ€Ð¾': (47.9500, 30.3500),
    'Ñ”Ð»Ð°Ð½ÐµÑ†ÑŒ': (47.8667, 31.8667), 'Ñ”Ð»Ð°Ð½Ñ†Ñ': (47.8667, 31.8667), 'Ñ”Ð»Ð°Ð½Ñ†ÑŽ': (47.8667, 31.8667), 'Ñ”Ð»Ð°Ð½Ñ†Ñ–': (47.8667, 31.8667),
    'Ð±Ð°ÑˆÑ‚Ð°Ð½ÐºÐ°': (47.4086, 32.4389), 'Ð±Ð°ÑˆÑ‚Ð°Ð½ÐºÐ¸': (47.4086, 32.4389), 'Ð±Ð°ÑˆÑ‚Ð°Ð½ÐºÑƒ': (47.4086, 32.4389), 'Ð±Ð°ÑˆÑ‚Ð°Ð½Ñ†Ñ–': (47.4086, 32.4389),
    'Ð±ÐµÑ€ÐµÐ·Ð½ÐµÐ³ÑƒÐ²Ð°Ñ‚Ðµ': (47.3167, 32.8500),  # present
    'Ð±ÐµÑ€ÐµÐ·Ð½ÐµÐ³ÑƒÐ²Ð°Ñ‚Ð¾Ð¼Ñƒ': (47.3167, 32.8500),
    'Ð°Ñ€ÐºÐ°ÑÐ¾Ð²Ðµ?': (46.9750, 31.9946),
    'ÐºÐ°ÑˆÐ¿ÐµÑ€Ð¾-Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°': (47.3620, 31.8790),
    'Ð¿Ð°Ñ€ÑƒÑ‚Ð¸Ð½Ðµ': (46.7530, 31.0160),
    'Ð¿Ð°Ñ€ÑƒÑ‚Ð¸Ð½Ð¾Ð¼Ñƒ': (46.7530, 31.0160),
    'Ð¿Ð°Ñ€ÑƒÑ‚Ð¸Ð½Ð¾Ð³Ð¾': (46.7530, 31.0160),
    'Ð¿Ð°Ñ€ÑƒÑ‚Ð¸Ð½Ñ–': (46.7530, 31.0160),
    'ÐºÐ¾Ð±Ð»ÐµÐ²Ðµ': (46.6670, 31.2170),
    'ÐºÐ¾Ð±Ð»ÐµÐ²Ð¾': (46.6670, 31.2170),
    'ÐºÐ¾Ð±Ð»ÐµÐ²Ð¾Ð¼Ñƒ': (46.6670, 31.2170),
    'ÐºÐ¾Ð±Ð»ÐµÐ²Ð¾Ð³Ð¾': (46.6670, 31.2170),
    'ÐºÐ¾Ð±Ð»ÐµÐ²Ñ–': (46.6670, 31.2170),
    'Ð³Ð°Ð»Ñ–Ñ†Ð¸Ð½Ð¾Ð²Ðµ': (46.9710, 31.9400),
    'Ð³Ð°Ð»Ð¸Ñ†Ð¸Ð½Ð¾Ð²Ðµ': (46.9710, 31.9400),
    'Ð³Ð°Ð»Ñ–Ñ†Ð¸Ð½Ð¾Ð²Ð¾Ð¼Ñƒ': (46.9710, 31.9400),
    'Ð³Ð°Ð»Ñ–Ñ†Ð¸Ð½Ð¾Ð²Ð¾': (46.9710, 31.9400),
    'Ð»Ð¸Ð¼Ð°Ð½ (Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²)': (46.5410, 31.3270),
    'ÐºÑƒÑ‚ÑƒÐ·Ñ–Ð²ÐºÐ°?': (46.9750, 31.9946),
    'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ð²ÐºÐ°(Ð¼Ð¸Ðº)': (46.7160, 31.8660),
    'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ð²ÐºÐ° Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°': (46.7160, 31.8660),
    'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ð²ÐºÐ°': (46.7160, 31.8660),  # multiple oblasts
    'ÑÑ‚Ð°Ñ€Ð¸Ð¹ Ð±ÑƒÐ³?': (47.6833, 32.5167),
    # Ð”Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ñ– Ð½Ð°ÑÐµÐ»ÐµÐ½Ñ– Ð¿ÑƒÐ½ÐºÑ‚Ð¸ ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð±Ñ€Ð°Ñ‚ÑÑŒÐºÐµ': (47.5333, 32.1667), 'Ð±Ñ€Ð°Ñ‚ÑÑŒÐºÑƒ': (47.5333, 32.1667), 'Ð±Ñ€Ð°Ñ‚ÑÑŒÐºÐ¾Ð³Ð¾': (47.5333, 32.1667),
    'Ð²Ð¾Ð·ÑÑ–ÑÑ‚ÑÑŒÐºÐµ': (46.8167, 32.0833), 'Ð²Ð¾Ð·ÑÑ–ÑÑ‚ÑÑŒÐºÑƒ': (46.8167, 32.0833), 'Ð²Ð¾Ð·ÑÑ–ÑÑ‚ÑÑŒÐºÐ¾Ð³Ð¾': (46.8167, 32.0833),
}

for _my_name, _my_coords in MYKOLAIV_CITY_COORDS.items():
    CITY_COORDS.setdefault(_my_name, _my_coords)

# Odesa (ÐžÐ´ÐµÑÑŒÐºÐ°) Oblast cities & key settlements.
ODESA_CITY_COORDS = {
    'Ð¾Ð´ÐµÑÐ°': (46.4825, 30.7233),  # already present
    'Ð¾Ð´ÐµÑÑÐ°': (46.4825, 30.7233),  # russian variant
    'Ñ‡Ð¾Ñ€Ð½Ð¾Ð¼Ð¾Ñ€ÑÑŒÐº': (46.3019, 30.6548),
    'ÑŽÐ¶Ð½Ðµ': (46.6226, 31.1013),
    'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': (46.1900, 30.3400),
    'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐº': (46.1900, 30.3400),
    'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÑƒ': (46.1900, 30.3400),
    'Ð±ÐµÐ»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½ÐµÑÑ‚Ñ€Ð¾Ð²ÑÐºÐ¸Ð¹': (46.1900, 30.3400),
    'Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐº': (47.7425, 29.5322),
    'Ð¿Ð¾Ð´Ð¾Ð»ÑŒÑÑŒÐº': (47.7425, 29.5322),
    'Ð±Ð°Ð»Ñ‚Ð°': (47.9381, 29.6125),
    'Ð±Ð°Ð»Ñ‚Ñ–': (47.9381, 29.6125),
    'Ð±Ð°Ð»Ñ‚Ñƒ': (47.9381, 29.6125),
    'Ð±Ð°Ð»Ñ‚Ð¾ÑŽ': (47.9381, 29.6125),
    'Ð°Ñ€Ñ‚ÐµÐ¼Ñ–Ð²ÑÑŒÐº(Ð¾Ð´ÐµÑÐ°)?': (46.4825, 30.7233),  # noise
    'Ñ€Ð¾Ð·Ð´Ñ–Ð»ÑŒÐ½Ð°': (46.8450, 30.0780),
    'Ñ€Ð¾Ð·Ð´Ñ–Ð»ÑŒÐ½Ñ–Ð¹': (46.8450, 30.0780),
    'Ñ€Ð¾Ð·Ð´Ñ–Ð»ÑŒÐ½Ñƒ': (46.8450, 30.0780),
    'ÐºÐ¸Ð»Ñ–Ñ': (45.4550, 29.2680),
    'Ñ–Ð·Ð¼Ð°Ñ—Ð»': (45.3511, 28.8367),
    'Ñ–Ð·Ð¼Ð°Ñ—Ð»Ñ–': (45.3511, 28.8367),
    'Ñ–Ð·Ð¼Ð°Ñ—Ð» Ñƒ': (45.3511, 28.8367),
    'Ñ–Ð·Ð¼Ð°Ñ—Ð»Ñƒ': (45.3511, 28.8367),
    'Ð²Ð¸Ð»ÐºÐ¾Ð²Ðµ': (45.4031, 29.5986),
    'Ð²Ð¸Ð»ÐºÐ¾Ð²Ð¾Ð¼Ñƒ': (45.4031, 29.5986),
    # Ð§Ð°Ð¿Ð»Ð¸Ð½Ðµ (Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ)
    'Ñ‡Ð°Ð¿Ð»Ð¸Ð½Ðµ': (47.3811, 34.5619),
    'Ñ‡Ð°Ð¿Ð»Ð¸Ð½Ð¾Ð¼': (47.3811, 34.5619),
    'Ñ‡Ð°Ð¿Ð»Ð¸Ð½Ð¾Ð¼Ñƒ': (47.3811, 34.5619),
    'Ñ€ÐµÐ½Ð¸Ð¹': (45.4560, 28.2830),
    'Ñ€ÐµÐ½Ñ–': (45.4560, 28.2830),
    'Ñ€ÐµÐ½Ð¾?': (45.4560, 28.2830),
    'Ñ‚Ð°Ñ‚Ð°Ñ€Ð±ÑƒÐ½Ð°Ñ€Ð¸': (45.8417, 29.6128),
    'ÑÐ°Ñ€Ð½Ð¸(Ð¾Ð´ÐµÑÐ°)?': (46.4825, 30.7233),  # noise
    'Ð±Ð¾Ð»Ð³Ñ€Ð°Ð´': (45.6772, 28.6147),
    'Ð±Ð¾Ð»Ð³Ñ€Ð°Ð´Ñ–': (45.6772, 28.6147),
    'Ð±Ð¾Ð»Ð³Ñ€Ð°Ð´Ð¾Ð¼': (45.6772, 28.6147),
    'Ð°Ñ€Ñ†Ð¸Ð·': (45.9919, 29.4181),
    'Ð°Ñ€Ñ†Ð¸Ð·Ñ–': (45.9919, 29.4181),
    'Ð°Ñ€Ñ†Ð¸Ð·Ð¾Ð¼': (45.9919, 29.4181),
    'Ñ‚Ð°Ð²Ñ€Ñ–Ñ(Ð¾Ð´ÐµÑÐ°)?': (46.4825, 30.7233),  # noise
    'Ð»ÑŽÐ±Ð°ÑˆÑ–Ð²ÐºÐ°': (47.8540, 30.2550),
    'Ð»ÑŽÐ±Ð°ÑˆÑ–Ð²Ñ†Ñ–': (47.8540, 30.2550),
    'Ð°Ð½Ð°Ð½ÑŒÑ—Ð²': (47.7244, 29.9686),
    'Ð°Ð½Ð°Ð½ÑŒÑ”Ð²': (47.7244, 29.9686),
    'Ð°Ð½Ð°Ð½ÑŒÑ”Ð²Ñ–': (47.7244, 29.9686),
    'Ð°Ð½Ð°Ð½ÑŒÑ”Ð²Ñƒ': (47.7244, 29.9686),
    'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²ÐºÐ°': (47.2050, 30.9080),
    'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²Ñ†Ñ–': (47.2050, 30.9080),
    'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²ÐºÑƒ': (47.2050, 30.9080),
    'Ð·Ð°Ñ‚Ð¾ ÐºÐ°?': (46.0660, 30.4680),  # noise for Ð·Ð°Ñ‚Ð¾ÐºÐ°
    'Ð·Ð°Ñ‚Ð¾ÐºÐ°': (46.0660, 30.4680),  # present
    'ÐºÐ°Ñ€Ð¾Ð»Ñ–Ð½Ð¾-Ð±ÑƒÐ³Ð°Ð·': (46.1530, 30.5200),
    'ÐºÐ°Ñ€Ð¾Ð»Ñ–Ð½Ð¾-Ð±ÑƒÐ³Ð°Ð·Ñ–': (46.1530, 30.5200),
    'ÐºÐ°Ñ€Ð¾Ð»Ñ–Ð½Ð¾-Ð±ÑƒÐ³Ð°Ð·Ñƒ': (46.1530, 30.5200),
    'ÐºÐ°Ñ€Ð¾Ð»Ñ–Ð½Ð¾-Ð±ÑƒÐ³Ð°Ð·Ð¾Ð¼': (46.1530, 30.5200),
    'Ð³Ñ€Ð°Ð´Ñ–Ð·ÑÑŒÐº(Ð¾Ð´ÐµÑÐ°)?': (46.4825, 30.7233),  # noise
    'Ñ‚Ð°Ñ—Ñ€Ð¾Ð²Ðµ': (46.3990, 30.6940),
    'Ñ‚Ð°Ñ—Ñ€Ð¾Ð²Ð¾Ð¼Ñƒ': (46.3990, 30.6940),
    'ÑÐµÑ€Ð³Ñ–Ñ—Ð²ÐºÐ°': (46.0006, 29.9578),
    'ÑÐµÑ€Ð³Ñ–Ñ—Ð²Ñ†Ñ–': (46.0006, 29.9578),
    'ÑÐµÑ€Ð³Ñ–Ñ—Ð²ÐºÑƒ': (46.0006, 29.9578),
    'ÑÐµÑ€Ð³Ñ–Ñ—Ð²ÐºÐ¾ÑŽ': (46.0006, 29.9578),
    'Ñ‚ÑƒÐ·Ð»Ð¸': (45.8650, 30.0975),
    'Ñ‚ÑƒÐ·Ð»': (45.8650, 30.0975),
    'Ñ‚ÑƒÐ·Ð»Ð°Ñ…': (45.8650, 30.0975),
    'Ñ‚ÑƒÐ·Ð»Ð°Ð¼Ð¸': (45.8650, 30.0975),
    'Ñ‚Ð°Ñ—Ñ€Ð¾Ð²Ð¾': (46.3990, 30.6940),
    'Ð»Ð¸Ð¼Ð°Ð½(Ð¾Ð´ÐµÑÐ°)': (46.3530, 30.6500),
    'Ð»Ð¸Ð¼Ð°Ð½ Ð¾Ð´ÐµÑÐ°': (46.3530, 30.6500),
    'Ð»Ð¸Ð¼Ð°Ð½ (Ð¾Ð´ÐµÑÐ°)': (46.3530, 30.6500),
}

for _od_name, _od_coords in ODESA_CITY_COORDS.items():
    CITY_COORDS.setdefault(_od_name, _od_coords)

# Kyiv (ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°) Oblast cities & key settlements (excluding Kyiv already present).
KYIV_OBLAST_CITY_COORDS = {
    'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°': (49.7950, 30.1310),  # present
    'Ð±Ñ€Ð¾Ð²Ð°Ñ€Ð¸': (50.5110, 30.7909),  # present
    'Ð±Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ': (50.3527, 30.9550),  # present
    'Ð³Ð½Ñ–Ð´Ð¸Ð½': (50.3722, 30.8639),  # ÑÐµÐ»Ð¾ Ð±Ñ–Ð»Ñ Ð‘Ð¾Ñ€Ð¸ÑÐ¿Ð¾Ð»Ñ
    'Ñ–Ñ€Ð¿Ñ–Ð½ÑŒ': (50.5218, 30.2506),
    'Ñ–Ñ€Ð¿ÐµÐ½Ñ': (50.5218, 30.2506),
    'Ð±ÑƒÑ‡Ð°': (50.5436, 30.2120),
    'Ð±ÑƒÑ‡Ñƒ': (50.5436, 30.2120),
    'Ð±ÑƒÑ‡Ñ–': (50.5436, 30.2120),
    'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÑ–Ð²': (50.1846, 30.3133),
    'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÐ¾Ð²Ñ–': (50.1846, 30.3133),
    'Ð²Ð°ÑÐ¸Ð»ÑŒÐºÐ¾Ð²Ñ–?': (50.1846, 30.3133),
    'Ñ„Ð°ÑÑ‚Ñ–Ð²': (50.0780, 29.9170),
    'Ñ„Ð°ÑÑ‚Ð¾Ð²Ñ–': (50.0780, 29.9170),
    'Ñ„Ð°ÑÑ‚Ð¾Ð²Ñƒ': (50.0780, 29.9170),
    'Ð¾Ð±ÑƒÑ…Ñ–Ð²': (50.1072, 30.6211),  # present
    'Ð¾Ð±ÑƒÑ…Ð¾Ð²Ñ–': (50.1072, 30.6211),
    'Ð¾Ð±ÑƒÑ…Ð¾Ð²Ñƒ': (50.1072, 30.6211),
    'Ð¾Ð±ÑƒÑ…Ð¾Ð²Ð¾Ð¼': (50.1072, 30.6211),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð¸Ñ‡': (51.5226, 30.7203),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð¸Ñ‡Ñ–': (51.5226, 30.7203),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð¸Ñ‡Ñƒ': (51.5226, 30.7203),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð¸Ñ‡ÐµÐ¼': (51.5226, 30.7203),
    'Ð±ÐµÑ€ÐµÐ·Ð°Ð½ÑŒ': (50.3085, 31.4576),  # present
    'Ð±ÐµÑ€ÐµÐ·Ð°Ð½Ñ–': (50.3085, 31.4576),
    'Ð±ÐµÑ€ÐµÐ·Ð°Ð½ÑŒÑÑŒÐºÐ°?': (50.3085, 31.4576),
    'ÑÐºÐ²Ð¸Ñ€Ð°': (49.7333, 29.6667),  # present
    'ÑÐºÐ²Ð¸Ñ€Ñ–': (49.7333, 29.6667),
    'ÑÐºÐ²Ð¸Ñ€Ñƒ': (49.7333, 29.6667),
    'ÐºÐ°Ð³Ð°Ñ€Ð»Ð¸Ðº': (49.6607, 30.8172),
    'ÐºÐ°Ð³Ð°Ñ€Ð»Ð¸ÐºÑƒ': (49.6607, 30.8172),
    'ÐºÐ°Ð³Ð°Ñ€Ð»Ð¸ÐºÑƒ?': (49.6607, 30.8172),
    'Ð¼Ð¸Ñ€Ð¾Ð½Ñ–Ð²ÐºÐ°': (49.6631, 31.0100),  # present
    'Ð¼Ð¸Ñ€Ð¾Ð½Ñ–Ð²Ñ†Ñ–': (49.6631, 31.0100),
    'Ð¼Ð¸Ñ€Ð¾Ð½Ñ–Ð²ÐºÑƒ': (49.6631, 31.0100),
    'Ð±Ð¾Ð³ÑƒÑÐ»Ð°Ð²': (49.5494, 30.8741),
    'Ð±Ð¾Ð³ÑƒÑÐ»Ð°Ð²Ñ–': (49.5494, 30.8741),
    'Ð±Ð¾Ð³ÑƒÑÐ»Ð°Ð²Ñƒ': (49.5494, 30.8741),
    'ÑƒÐ·Ð¸Ð½': (49.8216, 30.4567),  # present
    'ÑƒÐ·Ð¸Ð½Ñ–': (49.8216, 30.4567),
    'ÑƒÐ·Ð¸Ð½Ð¾Ð¼': (49.8216, 30.4567),
    'Ñ‚ÐµÑ‚Ñ–Ñ”Ð²': (49.3717, 29.6969),
    'Ñ‚ÐµÑ‚Ñ–Ñ”Ð²Ñ–': (49.3717, 29.6969),
    'Ñ‚ÐµÑ‚Ñ–Ñ”Ð²Ñƒ': (49.3717, 29.6969),
    'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñ–Ð²ÐºÐ°(ÐºÐ¸Ñ—Ð²ÑÐºÐ°?)': (50.4501, 30.5234),  # noise center fallback
    'Ð²Ð¾Ð»Ð¾Ð´Ð°Ñ€ÐºÐ°': (49.5240, 29.9120),
    'Ð²Ð¾Ð»Ð¾Ð´Ð°Ñ€Ñ†Ñ–': (49.5240, 29.9120),
    'Ð²Ð¾Ð»Ð¾Ð´Ð°Ñ€ÐºÑƒ': (49.5240, 29.9120),
    'Ñ‚Ð°Ð²Ñ€Ð¸Ð¶Ð¶Ñ?': (50.4501, 30.5234),
    'ÐºÐ¾Ð»Ð¾Ð½Ñ‰Ð¸Ð½Ð°': (50.4150, 29.9990),
    'ÐºÐ¾Ð»Ð¾Ð½Ñ‰Ð¸Ð½Ñ–': (50.4150, 29.9990),
    'ÐºÐ¾Ð»Ð¾Ð½Ñ‰Ð¸Ð½Ñƒ': (50.4150, 29.9990),
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ¸': (50.2500, 30.2500),  # present
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ°Ñ…': (50.2500, 30.2500),
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ°Ð¼': (50.2500, 30.2500),
    'Ð³Ñ€ÐµÐ±Ñ–Ð½ÐºÐ°Ð¼Ð¸': (50.2500, 30.2500),
    'Ð³Ñ€ÐµÐ±Ñ–Ð½Ð¾Ðº': (50.2500, 30.2500),
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´': (50.5840, 30.4890),  # present
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´Ñ–': (50.5840, 30.4890),
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´Ñƒ': (50.5840, 30.4890),
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´Ð¾Ð¼': (50.5840, 30.4890),
    'Ð²Ð¸ÑˆÐ½ÐµÐ²Ðµ': (50.3899, 30.3932),
    'Ð²Ð¸ÑˆÐ½ÐµÐ²Ð¾Ð¼Ñƒ': (50.3899, 30.3932),
    'Ð²Ð¸ÑˆÐ½ÐµÐ²Ð¾': (50.3899, 30.3932),
    'Ð²Ð¸ÑˆÐ½ÐµÐ²Ð¾Ð³Ð¾': (50.3899, 30.3932),
    'Ñ–Ñ€Ð¿Ñ–Ð½ÑÑŒÐºÐ¾-Ð±ÑƒÑ‡Ð°Ð½ÑÑŒÐºÐ° Ð°Ð³Ð»Ð¾Ð¼ÐµÑ€Ð°Ñ†Ñ–Ñ?': (50.5218, 30.2506),
    'ÐºÐ¾Ð·Ð¸Ð½ (ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°)': (50.1520, 30.6450),
    'ÐºÐ¾Ð·Ð¸Ð½ ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°': (50.1520, 30.6450),
    'ÐºÐ¾Ð·Ð¸Ð½': (50.1520, 30.6450),  # multiple oblasts
    'Ð¿ÐµÑ‚Ñ€Ñ–Ð²ÑÑŒÐºÐµ?': (50.4501, 30.5234),
    'Ð¿ÐµÑ‚Ñ€Ñ–Ð²ÑÑŒÐºÐµ(ÐºÐ¸Ñ—Ð²)': (50.4501, 30.5234),
    'Ð¿ÐµÑ‚Ñ€Ñ–Ð²ÑÑŒÐºÐµ ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°': (50.4501, 30.5234),
    'Ð¿ÐµÑ€ÐµÑÑÐ»Ð°Ð²': (50.0769, 31.4610),  # ÐŸÐµÑ€ÐµÑÑÐ»Ð°Ð²-Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹
    'Ð¿ÐµÑ€ÐµÑÑÐ»Ð¾Ð²Ñ–': (50.0769, 31.4610),
    'Ð¿ÐµÑ€ÐµÑÑÐ»Ð°Ð²Ñƒ': (50.0769, 31.4610),
    'Ð¿ÐµÑ€ÐµÑÑÐ»Ð°Ð²Ð¾Ð¼': (50.0769, 31.4610),
    'Ð²Ð»Ð°ÑÑ–Ð²ÐºÐ°': (50.3706, 31.2381),  # Ð’Ð»Ð°ÑÑ–Ð²ÐºÐ°, Ð‘Ñ€Ð¾Ð²Ð°Ñ€ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½
    'Ð²Ð»Ð°ÑÑ–Ð²Ñ†Ñ–': (50.3706, 31.2381),
    'Ð²Ð»Ð°ÑÑ–Ð²ÐºÑƒ': (50.3706, 31.2381),
    'Ð²Ð»Ð°ÑÑ–Ð²ÐºÐ¾ÑŽ': (50.3706, 31.2381),
}

for _kv_name, _kv_coords in KYIV_OBLAST_CITY_COORDS.items():
    CITY_COORDS.setdefault(_kv_name, _kv_coords)

# Cherkasy (Ð§ÐµÑ€ÐºÐ°ÑÑŒÐºÐ°) Oblast cities & key settlements.
CHERKASY_CITY_COORDS = {
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸': (49.4444, 32.0598),  # already present
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ°Ñ…': (49.4444, 32.0598),
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ°Ð¼': (49.4444, 32.0598),
    'ÑƒÐ¼Ð°Ð½ÑŒ': (48.7484, 30.2219),
    'ÑƒÐ¼Ð°Ð½Ñ–': (48.7484, 30.2219),
    'ÑƒÐ¼Ð°Ð½ÑŒÑƒ?': (48.7484, 30.2219),
    'ÑÐ¼iÐ»Ð°': (49.2222, 31.8878),  # alt i
    'ÑÐ¼Ñ–Ð»Ð°': (49.2222, 31.8878),
    'ÑÐ¼Ñ–Ð»Ñ–': (49.2222, 31.8878),
    'ÑÐ¼iÐ»Ñ–': (49.2222, 31.8878),
    'ÑÐ¼iÐ»Ñƒ': (49.2222, 31.8878),
    'ÑÐ¼Ñ–Ð»Ñƒ': (49.2222, 31.8878),
    'Ð·Ð¾Ð»Ð¾Ñ‚Ð¾Ð½Ð¾ÑˆÐ°': (49.6676, 32.0401),
    'Ð·Ð¾Ð»Ð¾Ñ‚Ð¾Ð½Ð¾ÑˆÑ–': (49.6676, 32.0401),
    'Ð·Ð¾Ð»Ð¾Ñ‚Ð¾Ð½Ð¾ÑˆÑƒ': (49.6676, 32.0401),
    'Ð·Ð¾Ð»Ð¾Ñ‚Ð¾Ð½Ð¾ÑˆÐµÑŽ': (49.6676, 32.0401),
    'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´ÐºÐ°': (49.0777, 30.9697),
    'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´Ñ†Ñ–': (49.0777, 30.9697),
    'Ð·Ð²ÐµÐ½Ð¸Ð³Ð¾Ñ€Ð¾Ð´ÐºÑƒ': (49.0777, 30.9697),
    'Ð·Ð²ÐµÐ½ÐµÐ³Ð¾Ñ€Ð¾Ð´ÐºÐ°': (49.0777, 30.9697),  # rus typo
    'Ð·Ð²ÐµÐ½ÐµÐ³Ð¾Ñ€Ð¾Ð´ÐºÑƒ': (49.0777, 30.9697),
    'ÐºÐ¾Ñ€ÑÑƒÐ½ÑŒ-ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': (49.4186, 31.2581),
    'ÐºÐ¾Ñ€ÑÑƒÐ½ÑŒ-ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ': (49.4186, 31.2581),
    'ÐºÐ¾Ñ€ÑÑƒÐ½ÑŒ-ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¼': (49.4186, 31.2581),
    'Ð³Ð¾Ñ€Ð¾Ð´Ð¸Ñ‰Ðµ': (49.2886, 31.4547),
    'Ð³Ð¾Ñ€Ð¾Ð´Ð¸Ñ‰Ñ–': (49.2886, 31.4547),
    'Ð³Ð¾Ñ€Ð¾Ð´Ð¸Ñ‰ÐµÑŽ': (49.2886, 31.4547),
    'Ñ…Ñ€Ð¸ÑÑ‚Ð¸Ð½Ñ–Ð²ÐºÐ°': (48.8122, 29.9806),
    'Ñ…Ñ€Ð¸ÑÑ‚Ð¸Ð½Ñ–Ð²Ñ†Ñ–': (48.8122, 29.9806),
    'Ñ…Ñ€Ð¸ÑÑ‚Ð¸Ð½Ñ–Ð²ÐºÑƒ': (48.8122, 29.9806),
    'Ð¼Ð¾Ð½Ð°ÑÑ‚Ð¸Ñ€Ð¸Ñ‰Ðµ': (48.9905, 29.8036),
    'Ð¼Ð¾Ð½Ð°ÑÑ‚Ð¸Ñ€Ð¸Ñ‰Ñ–': (48.9905, 29.8036),
    'Ð¼Ð¾Ð½Ð°ÑÑ‚Ð¸Ñ€Ð¸Ñ‰ÐµÐ¼': (48.9905, 29.8036),
    'Ñ‚Ð°Ð»ÑŒÐ½Ðµ': (48.8803, 30.6872),
    'Ñ‚Ð°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ': (48.8803, 30.6872),
    'Ñ‚Ð°Ð»ÑŒÐ½Ð¸Ð¼': (48.8803, 30.6872),
    'Ð¶Ð°ÑˆÐºÑ–Ð²': (49.2431, 30.1122),  # already present
    'Ð¶Ð°ÑˆÐºÐ¾Ð²Ñ–': (49.2431, 30.1122),
    'Ð¶Ð°ÑˆÐºÐ¾Ð²Ñ–?': (49.2431, 30.1122),
    'Ð»Ð¸ÑÑÐ½ÐºÐ°': (49.2547, 30.8294),
    'Ð»Ð¸ÑÑÐ½Ñ†Ñ–': (49.2547, 30.8294),
    'Ð»Ð¸ÑÑÐ½ÐºÑƒ': (49.2547, 30.8294),
    'Ñ‡Ð¸Ð³Ð¸Ñ€Ð¸Ð½': (49.0797, 32.6572),
    'Ñ‡Ð¸Ð³Ð¸Ñ€Ð¸Ð½Ñ–': (49.0797, 32.6572),
    'Ñ‡Ð¸Ð³Ð¸Ñ€Ð¸Ð½Ð¾Ð¼': (49.0797, 32.6572),
    'ÐºÐ°Ð¼\'ÑÐ½ÐºÐ°': (49.0310, 32.1050),
    'ÐºÐ°Ð¼ÑÐ½ÐºÐ°': (49.0310, 32.1050),
    'ÐºÐ°Ð¼\'ÑÐ½Ñ†Ñ–': (49.0310, 32.1050),
    'ÐºÐ°Ð¼ÑÐ½Ñ†Ñ–': (49.0310, 32.1050),
    'ÐºÐ°Ð¼\'ÑÐ½ÐºÑƒ': (49.0310, 32.1050),
    'Ð²Ð°Ñ‚ÑƒÑ‚Ñ–Ð½Ðµ': (48.7500, 30.1833),
    'Ð²Ð°Ñ‚ÑƒÑ‚Ñ–Ð½Ð¾Ð¼Ñƒ': (48.7500, 30.1833),
    'Ð²Ð°Ñ‚ÑƒÑ‚Ñ–Ð½Ð¸Ð¼': (48.7500, 30.1833),
    'ÑˆÐ¿Ð¾Ð»Ð°': (49.0132, 31.3942),
    'ÑˆÐ¿Ð¾Ð»Ñ–': (49.0132, 31.3942),
    'ÑˆÐ¿Ð¾Ð»Ñƒ': (49.0132, 31.3942),
    'ÐºÐ°Ñ‚ÐµÑ€Ð¸Ð½Ð¾Ð¿Ñ–Ð»ÑŒ': (48.9889, 30.9633),
    'ÐºÐ°Ñ‚ÐµÑ€Ð¸Ð½Ð¾Ð¿Ð¾Ð»Ñ–': (48.9889, 30.9633),
    'ÐºÐ°Ñ‚ÐµÑ€Ð¸Ð½Ð¾Ð¿Ð¾Ð»ÑŽ': (48.9889, 30.9633),
    'Ð´Ñ€Ð°Ð±Ñ–Ð²': (49.9700, 32.1490),
    'Ð´Ñ€Ð°Ð±Ð¾Ð²Ñ–': (49.9700, 32.1490),
    'Ð´Ñ€Ð°Ð±Ð¾Ð²Ð¾Ð¼': (49.9700, 32.1490),
    'Ð¼Ð°Ð½ÑŒÐºÑ–Ð²ÐºÐ°': (48.9900, 30.3500),
    'Ð¼Ð°Ð½ÑŒÐºÑ–Ð²Ñ†Ñ–': (48.9900, 30.3500),
    'Ð¼Ð°Ð½ÑŒÐºÑ–Ð²ÐºÑƒ': (48.9900, 30.3500),
    'ÑÑ‚ÐµÐ±Ð»Ñ–Ð²': (49.3860, 31.0550),
    'ÑÑ‚ÐµÐ±Ð»ÐµÐ²Ñ–': (49.3860, 31.0550),
    'Ñ”Ñ€ÐºÐ¸': (49.0950, 30.9817),
    'Ñ”Ñ€ÐºÐ°Ñ…': (49.0950, 30.9817),
}

for _ck_name, _ck_coords in CHERKASY_CITY_COORDS.items():
    CITY_COORDS.setdefault(_ck_name, _ck_coords)

# Lviv (Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°) Oblast cities & key settlements.
LVIV_CITY_COORDS = {
    'Ð±Ñ–Ð±Ñ€ÐºÐ°': (49.6353, 24.2614),
    'Ð±Ñ–Ð±Ñ€ÐºÑƒ': (49.6353, 24.2614),
    'Ð±Ñ–Ð±Ñ€Ñ†Ñ–': (49.6353, 24.2614),
    'Ð±Ð¾Ñ€Ð¸ÑÐ»Ð°Ð²': (49.2897, 23.4267),
    'Ð±Ð¾Ñ€Ð¸ÑÐ»Ð°Ð²Ñ–': (49.2897, 23.4267),
    'Ð±Ð¾Ñ€Ð¸ÑÐ»Ð°Ð²Ñƒ': (49.2897, 23.4267),
    'Ð±Ñ€Ð¾Ð´Ð¸': (50.0869, 25.1531),
    'Ð±Ñ€Ð¾Ð´Ð°Ñ…': (50.0869, 25.1531),
    'Ð±Ñ€Ð¾Ð´Ð°Ð¼': (50.0869, 25.1531),
    'Ð²Ð¸Ð½Ð½Ð¸ÐºÐ¸': (49.8097, 24.1431),
    'Ð²Ð¸Ð½Ð½Ð¸ÐºÐ°Ñ…': (49.8097, 24.1431),
    'Ð²Ð¸Ð½Ð½Ð¸ÐºÐ¸Ì': (49.8097, 24.1431),
    'Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ðº': (49.6889, 23.6514),
    'Ð³Ð¾Ñ€Ð¾Ð´ÐºÑƒ': (49.6889, 23.6514),
    'Ð³Ð¾Ñ€Ð¾Ð´ÐºÐ¾Ð¼': (49.6889, 23.6514),
    'Ð´Ñ€Ð¾Ð³Ð¾Ð±Ð¸Ñ‡': (49.3425, 23.5075),
    'Ð´Ñ€Ð¾Ð³Ð¾Ð±Ð¸Ñ‡Ñ–': (49.3425, 23.5075),
    'Ð´Ñ€Ð¾Ð³Ð¾Ð±Ð¸Ñ‡ÐµÐ¼': (49.3425, 23.5075),
    'Ð¶Ð¸Ð´Ð°Ñ‡Ñ–Ð²': (49.8744, 24.1403),
    'Ð¶Ð¸Ð´Ð°Ñ‡Ð¾Ð²Ñ–': (49.8744, 24.1403),
    'Ð¶Ð¸Ð´Ð°Ñ‡ÐµÐ²Ð¾Ð¼': (49.8744, 24.1403),
    'Ð¶Ð¾Ð²ÐºÐ²Ð°': (49.9731, 23.9719),
    'Ð¶Ð¾Ð²ÐºÐ²Ñ–': (49.9731, 23.9719),
    'Ð¶Ð¾Ð²ÐºÐ²Ñƒ': (49.9731, 23.9719),
    'Ð·Ð¾Ð»Ð¾Ñ‡Ñ–Ð²': (49.8072, 24.8906),
    'Ð·Ð¾Ð»Ð¾Ñ‡ÐµÐ²Ñ–': (49.8072, 24.8906),
    'Ð·Ð¾Ð»Ð¾Ñ‡ÐµÐ²Ð¾Ð¼': (49.8072, 24.8906),
    'ÐºÐ°Ð¼\'ÑÐ½ÐºÐ°-Ð±ÑƒÐ·ÑŒÐºÐ°': (50.0914, 24.0361),
    'ÐºÐ°Ð¼\'ÑÐ½Ñ†Ñ–-Ð±ÑƒÐ·ÑŒÐºÑ–Ð¹': (50.0914, 24.0361),
    'ÐºÐ°Ð¼\'ÑÐ½ÐºÑƒ-Ð±ÑƒÐ·ÑŒÐºÑƒ': (50.0914, 24.0361),
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²': (46.9750, 31.9946),  # ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ (ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».)
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ”Ð²Ñ–': (46.9750, 31.9946),
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ”Ð²Ð¾Ð¼': (46.9750, 31.9946),
    'Ð¼Ð¾ÑÑ‚Ð¸ÑÑŒÐºÐ°': (49.7956, 23.1533),
    'Ð¼Ð¾ÑÑ‚Ð¸ÑÑŒÐºÐ°Ñ…': (49.7956, 23.1533),
    'Ð¼Ð¾ÑÑ‚Ð¸ÑÑŒÐºÑƒ': (49.7956, 23.1533),
    'Ð½Ð¾Ð²Ð¸Ð¹ Ñ€Ð¾Ð·Ð´Ñ–Ð»': (49.4761, 24.4506),
    'Ð½Ð¾Ð²Ð¾Ð¼Ñƒ Ñ€Ð¾Ð·Ð´Ñ–Ð»Ñ–': (49.4761, 24.4506),
    'Ð½Ð¾Ð²Ð¸Ð¼ Ñ€Ð¾Ð·Ð´Ñ–Ð»Ð¾Ð¼': (49.4761, 24.4506),
    'Ð¿ÐµÑ€ÐµÐ¼Ð¸ÑˆÐ»ÑÐ½Ð¸': (49.6708, 24.6311),
    'Ð¿ÐµÑ€ÐµÐ¼Ð¸ÑˆÐ»ÑÐ½Ð°Ñ…': (49.6708, 24.6311),
    'Ð¿ÐµÑ€ÐµÐ¼Ð¸ÑˆÐ»ÑÐ½Ð°Ð¼Ð¸': (49.6708, 24.6311),
    'Ð¿ÑƒÑÑ‚Ð¾Ð¼Ð¸Ñ‚Ð¸': (49.7256, 24.1172),
    'Ð¿ÑƒÑÑ‚Ð¾Ð¼Ð¸Ñ‚Ð°Ñ…': (49.7256, 24.1172),
    'Ð¿ÑƒÑÑ‚Ð¾Ð¼Ð¸Ñ‚Ð°Ð¼Ð¸': (49.7256, 24.1172),
    'Ñ€Ð°Ð´ÐµÑ…Ñ–Ð²': (50.2831, 24.6411),
    'Ñ€Ð°Ð´ÐµÑ…Ð¾Ð²Ñ–': (50.2831, 24.6411),
    'Ñ€Ð°Ð´ÐµÑ…ÐµÐ²Ð¾Ð¼': (50.2831, 24.6411),
    'ÑÐ°Ð¼Ð±Ñ–Ñ€': (49.5117, 23.2019),
    'ÑÐ°Ð¼Ð±Ð¾Ñ€Ñ–': (49.5117, 23.2019),
    'ÑÐ°Ð¼Ð±Ð¾Ñ€Ð¾Ð¼': (49.5117, 23.2019),
    'ÑÐ¾ÐºÐ°Ð»ÑŒ': (50.4656, 24.2728),
    'ÑÐ¾ÐºÐ°Ð»Ñ–': (50.4656, 24.2728),
    'ÑÐ¾ÐºÐ°Ð»ÐµÐ¼': (50.4656, 24.2728),
    'ÑÑ‚Ð°Ñ€Ð¸Ð¹ ÑÐ°Ð¼Ð±Ñ–Ñ€': (49.4389, 23.0006),
    'ÑÑ‚Ð°Ñ€Ð¾Ð¼Ñƒ ÑÐ°Ð¼Ð±Ð¾Ñ€Ñ–': (49.4389, 23.0006),
    'ÑÑ‚Ð°Ñ€Ð¸Ð¼ ÑÐ°Ð¼Ð±Ð¾Ñ€Ð¾Ð¼': (49.4389, 23.0006),
    'ÑÑ‚Ñ€Ð¸Ð¹': (49.2622, 23.8603),
    'ÑÑ‚Ñ€Ð¸ÑŽ': (49.2622, 23.8603),
    'ÑÑ‚Ñ€Ð¸Ñ”Ð¼': (49.2622, 23.8603),
    'Ñ‚Ñ€ÑƒÑÐºÐ°Ð²ÐµÑ†ÑŒ': (49.2786, 23.5064),
    'Ñ‚Ñ€ÑƒÑÐºÐ°Ð²Ñ†Ñ–': (49.2786, 23.5064),
    'Ñ‚Ñ€ÑƒÑÐºÐ°Ð²Ñ†ÐµÐ¼': (49.2786, 23.5064),
    'Ñ‚ÑƒÑ€ÐºÐ°': (49.1528, 23.0306),
    'Ñ‚ÑƒÑ€Ñ†Ñ–': (49.1528, 23.0306),
    'Ñ‚ÑƒÑ€ÐºÐ¾ÑŽ': (49.1528, 23.0306),
    'ÑÐ²Ð¾Ñ€Ñ–Ð²': (49.9358, 23.3917),
    'ÑÐ²Ð¾Ñ€Ð¾Ð²Ñ–': (49.9358, 23.3917),
    'ÑÐ²Ð¾Ñ€Ð¾Ð²Ð¾Ð¼': (49.9358, 23.3917),
}

for _lv_name, _lv_coords in LVIV_CITY_COORDS.items():
    CITY_COORDS.setdefault(_lv_name, _lv_coords)

# Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ - ÑƒÑÑ– Ð¾ÑÐ½Ð¾Ð²Ð½Ñ– Ð¼Ñ–ÑÑ‚Ð° Ñ‚Ð° Ñ€Ð°Ð¹Ñ†ÐµÐ½Ñ‚Ñ€Ð¸
VINNYTSIA_CITY_COORDS = {
    # ÐžÐ±Ð»Ð°ÑÐ½Ð¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ': (49.2331, 28.4682),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ–': (49.2331, 28.4682),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŽ': (49.2331, 28.4682),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÐµÑŽ': (49.2331, 28.4682),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÐµÐ¹': (49.2331, 28.4682),
    
    # ÐœÑ–ÑÑ‚Ð° Ð¾Ð±Ð»Ð°ÑÐ½Ð¾Ð³Ð¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ
    'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½': (49.7167, 28.8333),
    'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½Ñ–': (49.7167, 28.8333),
    'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½Ñƒ': (49.7167, 28.8333),
    'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸Ðº': (49.5500, 27.9500),
    'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸ÐºÑƒ': (49.5500, 27.9500),
    'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸ÐºÐ°': (49.5500, 27.9500),
    'Ð»Ð°Ð´Ð¸Ð¶Ð¸Ð½': (48.6833, 29.2333),
    'Ð»Ð°Ð´Ð¸Ð¶Ð¸Ð½Ñ–': (48.6833, 29.2333),
    'Ð»Ð°Ð´Ð¸Ð¶Ð¸Ð½Ñƒ': (48.6833, 29.2333),
    'Ð¼Ð¾Ð³Ð¸Ð»Ñ–Ð²-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¸Ð¹': (48.4500, 27.7833),
    'Ð¼Ð¾Ð³Ð¸Ð»Ñ–Ð²-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¾Ð¼Ñƒ': (48.4500, 27.7833),
    'Ð¼Ð¾Ð³Ð¸Ð»Ñ–Ð²-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¾Ð³Ð¾': (48.4500, 27.7833),
    
    # Ð Ð°Ð¹Ñ†ÐµÐ½Ñ‚Ñ€Ð¸
    'Ð±Ð°Ñ€': (49.0667, 27.6833),
    'Ð±Ð°Ñ€Ñƒ': (49.0667, 27.6833),
    'Ð±Ð°Ñ€Ð°': (49.0667, 27.6833),
    'Ð±ÐµÑ€Ð´Ð¸Ñ‡Ñ–Ð²': (49.8978, 28.6011),  # Ð²Ð¶Ðµ Ñ” Ð² Ð¾ÑÐ½Ð¾Ð²Ð½Ñ–Ð¹ Ð±Ð°Ð·Ñ–
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŒ': (48.3667, 29.5167),  # Ð‘ÐµÑ€ÑˆÐ°Ð´ÑŒ - Ñ€Ð°Ð¹Ñ†ÐµÐ½Ñ‚Ñ€ Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¾Ñ— Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–
    'Ð±ÐµÑ€ÑˆÐ°Ð´Ñ–': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŽ': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÑŒÑŽ': (48.3667, 29.5167),
    'Ð±ÐµÑ€ÑˆÐ°Ð´ÐµÐ¹': (48.3667, 29.5167),
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (49.2331, 28.4682),
    'Ð³Ð°Ð¹ÑÐ¸Ð½': (49.4167, 29.3833),
    'Ð³Ð°Ð¹ÑÐ¸Ð½Ñ–': (49.4167, 29.3833),
    'Ð³Ð°Ð¹ÑÐ¸Ð½Ñƒ': (49.4167, 29.3833),
    'Ð¶Ð¼ÐµÑ€Ð¸Ð½ÐºÐ°': (49.0333, 28.1167),
    'Ð¶Ð¼ÐµÑ€Ð¸Ð½Ñ†Ñ–': (49.0333, 28.1167),
    'Ð¶Ð¼ÐµÑ€Ð¸Ð½ÐºÑƒ': (49.0333, 28.1167),
    'Ñ–Ð»Ð»Ñ–Ð½Ñ†Ñ–': (49.1000, 29.2167),
    'Ñ–Ð»Ð»Ñ–Ð½Ñ†ÑÑ…': (49.1000, 29.2167),
    'Ñ–Ð»Ð»Ñ–Ð½Ñ†Ð°Ð¼': (49.1000, 29.2167),
    'ÐºÐ°Ð»Ð¸Ð½Ñ–Ð²ÐºÐ°': (49.4500, 28.5167),
    'ÐºÐ°Ð»Ð¸Ð½Ñ–Ð²Ñ†Ñ–': (49.4500, 28.5167),
    'ÐºÐ°Ð»Ð¸Ð½Ñ–Ð²ÐºÑƒ': (49.4500, 28.5167),
    'ÐºÐ¾Ð·ÑÑ‚Ð¸Ð½ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (49.7167, 28.8333),
    'ÐºÑ€Ð¸Ð¶Ð¾Ð¿Ñ–Ð»ÑŒ': (48.3833, 28.8667),
    'ÐºÑ€Ð¸Ð¶Ð¾Ð¿Ð¾Ð»Ñ–': (48.3833, 28.8667),
    'ÐºÑ€Ð¸Ð¶Ð¾Ð¿Ð¾Ð»ÑŽ': (48.3833, 28.8667),
    'Ð»Ð¸Ð¿Ð¾Ð²ÐµÑ†ÑŒ': (49.2167, 29.1833),
    'Ð»Ð¸Ð¿Ð¾Ð²Ñ†Ñ–': (49.2167, 29.1833),
    'Ð»Ð¸Ð¿Ð¾Ð²ÐµÑ†ÑŒ': (49.2167, 29.1833),
    'Ð»Ñ–Ñ‚Ð¸Ð½': (49.7167, 28.0667),
    'Ð»Ñ–Ñ‚Ð¸Ð½Ñ–': (49.7167, 28.0667),
    'Ð»Ñ–Ñ‚Ð¸Ð½Ñƒ': (49.7167, 28.0667),
    'Ð¼ÑƒÑ€Ð¾Ð²Ð°Ð½Ñ– ÐºÑƒÑ€Ð¸Ð»Ñ–Ð²Ñ†Ñ–': (48.6667, 29.2667),
    'Ð¼ÑƒÑ€Ð¾Ð²Ð°Ð½Ð¸Ñ… ÐºÑƒÑ€Ð¸Ð»Ð¾Ð²Ñ†ÑÑ…': (48.6667, 29.2667),
    'Ð¼ÑƒÑ€Ð¾Ð²Ð°Ð½Ð¸Ð¼Ð¸ ÐºÑƒÑ€Ð¸Ð»Ð¾Ð²Ñ†ÑÐ¼Ð¸': (48.6667, 29.2667),
    'Ð½ÐµÐ¼Ð¸Ñ€Ñ–Ð²': (49.0833, 28.8333),
    'Ð½ÐµÐ¼Ð¸Ñ€Ð¾Ð²Ñ–': (49.0833, 28.8333),
    'Ð½ÐµÐ¼Ð¸Ñ€Ð¾Ð²Ñƒ': (49.0833, 28.8333),
    'Ð¾Ñ€Ð°Ñ‚Ñ–Ð²': (48.9333, 29.5167),
    'Ð¾Ñ€Ð°Ð½Ñ–Ð²Ð²Ê¼': (48.9333, 29.5167),
    'Ð¾Ñ€Ð°Ñ‚Ð¾Ð²Ñ–': (48.9333, 29.5167),
    'Ð¿Ñ–Ñ‰Ð°Ð½ÐºÐ°': (49.5833, 29.0833),
    'Ð¿Ñ–Ñ‰Ð°Ð½Ñ†Ñ–': (49.5833, 29.0833),
    'Ð¿Ñ–Ñ‰Ð°Ð½ÐºÑƒ': (49.5833, 29.0833),
    'Ð¿Ð¾Ð³Ñ€ÐµÐ±Ð¸Ñ‰Ðµ': (49.4833, 29.2667),
    'Ð¿Ð¾Ð³Ñ€ÐµÐ±Ð¸Ñ‰Ñ–': (49.4833, 29.2667),
    'Ð¿Ð¾Ð³Ñ€ÐµÐ±Ð¸Ñ‰Ðµ': (49.4833, 29.2667),
    'Ñ‚ÐµÐ¿Ð»Ð¸Ðº': (48.6667, 29.6667),
    'Ñ‚ÐµÐ¿Ð»Ð¸ÐºÑƒ': (48.6667, 29.6667),
    'Ñ‚ÐµÐ¿Ð»Ñ–ÐºÐ°': (48.6667, 29.6667),
    'Ñ‚Ð¾Ð¼Ð°ÑˆÐ¿Ñ–Ð»ÑŒ': (48.5333, 28.5167),
    'Ñ‚Ð¾Ð¼Ð°ÑˆÐ¿Ð¾Ð»Ñ–': (48.5333, 28.5167),
    'Ñ‚Ð¾Ð¼Ð°ÑˆÐ¿Ð¾Ð»ÑŽ': (48.5333, 28.5167),
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ': (48.8167, 29.0167),  # Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ð¾ÑÐ½Ð¾Ð²Ð½Ð¸Ð¹ Ð² Ð±Ð°Ð·Ñ– - Ð¡ÑƒÐ¼ÑÑŒÐºÐ°)
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½Ñ†Ñ–': (48.8167, 29.0167),
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.8167, 29.0167),
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (48.8167, 29.0167),
    'Ñ‚ÑƒÐ»ÑŒÑ‡Ð¸Ð½': (48.6783, 28.8486),
    'Ñ‚ÑƒÐ»ÑŒÑ‡Ð¸Ð½Ñ–': (48.6783, 28.8486),
    'Ñ‚ÑƒÐ»ÑŒÑ‡Ð¸Ð½Ñƒ': (48.6783, 28.8486),
    'Ñ‚Ð¸Ð²Ñ€Ñ–Ð²': (49.4000, 28.3167),
    'Ñ‚Ð¸Ð²Ñ€Ñ–Ð²Ñ–': (49.4000, 28.3167),
    'Ñ‚Ð¸Ð²Ñ€Ð¾Ð²Ñƒ': (49.4000, 28.3167),
    'Ñ…Ð¼Ñ–Ð»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½': (49.5500, 27.9500),
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–': (49.4167, 27.7333),  # Ð½Ðµ Ð¿Ð»ÑƒÑ‚Ð°Ñ‚Ð¸ Ð· Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†ÑÐ¼Ð¸ (Ñ†ÐµÐ½Ñ‚Ñ€ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–)
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†ÑÑ…': (49.4167, 27.7333),
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†ÑÐ¼': (49.4167, 27.7333),
    'Ñ‡ÐµÑ‡ÐµÐ»ÑŒÐ½Ð¸Ðº': (48.2167, 28.1833),
    'Ñ‡ÐµÑ‡ÐµÐ»ÑŒÐ½Ð¸ÐºÑƒ': (48.2167, 28.1833),
    'Ñ‡ÐµÑ‡ÐµÐ»ÑŒÐ½Ð¸ÐºÐ°': (48.2167, 28.1833),
    'ÑˆÐ°Ñ€Ð³Ð¾Ñ€Ð¾Ð´': (48.7333, 28.0833),
    'ÑˆÐ°Ñ€Ð³Ð¾Ñ€Ð¾Ð´Ñ–': (48.7333, 28.0833),
    'ÑˆÐ°Ñ€Ð³Ð¾Ñ€Ð¾Ð´Ñƒ': (48.7333, 28.0833),
    'ÑÐ¼Ð¿Ñ–Ð»ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833),  # Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ð¾ÑÐ½Ð¾Ð²Ð½Ð¸Ð¹ Ð² Ð±Ð°Ð·Ñ– - Ð¡ÑƒÐ¼ÑÑŒÐºÐ°)
    'ÑÐ¼Ð¿Ð¾Ð»ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833),
    'ÑÐ¼Ð¿Ñ–Ð»ÑŒ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (48.1333, 28.2833),
    'ÑÐ¼Ð¿Ð¾Ð»Ñ– Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833),
    'ÑÐ¼Ð¿Ð¾Ð»ÑŽ Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°': (48.1333, 28.2833),
    
    # Ð¡ÐµÐ»Ð¸Ñ‰Ð° Ð¼Ñ–ÑÑŒÐºÐ¾Ð³Ð¾ Ñ‚Ð¸Ð¿Ñƒ Ñ‚Ð° Ð²Ð°Ð¶Ð»Ð¸Ð²Ñ– ÑÐµÐ»Ð°
    'Ð±Ñ€Ð°Ñ—Ð»Ñ–Ð²': (49.0500, 28.2000),
    'Ð±Ñ€Ð°Ñ—Ð»Ð¾Ð²Ñ–': (49.0500, 28.2000),
    'Ð±Ñ€Ð°Ñ—Ð»Ð¾Ð²Ñƒ': (49.0500, 28.2000),
    'Ð²Ð°Ð¿Ð½ÑÑ€ÐºÐ°': (49.0333, 28.4500),
    'Ð²Ð°Ð¿Ð½ÑÑ€Ñ†Ñ–': (49.0333, 28.4500),
    'Ð²Ð°Ð¿Ð½ÑÑ€ÐºÑƒ': (49.0333, 28.4500),
    'Ð³Ð½Ñ–Ð²Ð°Ð½ÑŒ': (49.2833, 28.9167),
    'Ð³Ð½Ñ–Ð²Ð°Ð½Ñ–': (49.2833, 28.9167),
    'Ð³Ð½Ñ–Ð²Ð°Ð½ÑŒ': (49.2833, 28.9167),
    'Ð´Ð°ÑˆÑ–Ð²': (48.9000, 29.4333),
    'Ð´Ð°ÑˆÐµÐ²Ñ–': (48.9000, 29.4333),
    'Ð´Ð°ÑˆÐ¾Ð²Ñƒ': (48.9000, 29.4333),
    'Ð´ÐµÑ€Ð°Ð¶Ð½Ñ': (50.0500, 27.2667),
    'Ð´ÐµÑ€Ð°Ð¶Ð½Ñ–': (50.0500, 27.2667),
    'Ð´ÐµÑ€Ð°Ð¶Ð½ÑŽ': (50.0500, 27.2667),
    'Ð´Ð¶ÑƒÐ»Ð¸Ð½ÐºÐ°': (49.2500, 28.7000),
    'Ð´Ð¶ÑƒÐ»Ð¸Ð½Ñ†Ñ–': (49.2500, 28.7000),
    'Ð´Ð¶ÑƒÐ»Ð¸Ð½ÐºÑƒ': (49.2500, 28.7000),
    'ÐºÑ€Ð¸Ð¶Ð¾Ð¿Ñ–Ð»ÑŒ': (48.3833, 28.8667),
    'Ð»ÑƒÐºÐ°-Ð¼ÐµÐ»ÐµÑˆÐºÑ–Ð²ÑÑŒÐºÐ°': (48.6333, 29.1167),
    'Ð»ÑƒÑ†Ñ–-Ð¼ÐµÐ»ÐµÑˆÐºÑ–Ð²ÑÑŒÐºÑ–Ð¹': (48.6333, 29.1167),
    'Ð»ÑƒÐºÑƒ-Ð¼ÐµÐ»ÐµÑˆÐºÑ–Ð²ÑÑŒÐºÑƒ': (48.6333, 29.1167),
    'Ð¼ÑƒÑ€Ð°Ñ„Ð°': (49.1833, 28.7833),
    'Ð¼ÑƒÑ€Ð°Ñ„Ñ–': (49.1833, 28.7833),
    'Ð¼ÑƒÑ€Ð°Ñ„Ñƒ': (49.1833, 28.7833),
    'Ð¾Ñ…Ð¼Ð°Ñ‚Ñ–Ð²': (49.7500, 29.2167),
    'Ð¾Ñ…Ð¼Ð°Ñ‚Ð¾Ð²Ñ–': (49.7500, 29.2167),
    'Ð¾Ñ…Ð¼Ð°Ñ‚Ð¾Ð²Ñƒ': (49.7500, 29.2167),
    'Ð¿ÐµÑ‡ÐµÑ€Ð°': (49.6167, 28.8167),
    'Ð¿ÐµÑ‡ÐµÑ€Ñ–': (49.6167, 28.8167),
    'Ð¿ÐµÑ‡ÐµÑ€Ñƒ': (49.6167, 28.8167),
    'ÑÐ»Ð°Ð²ÑƒÑ‚Ð°': (50.3000, 26.8500),  # Ñ‚ÐµÑ…Ð½Ñ–Ñ‡Ð½Ð¾ Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ°, Ð°Ð»Ðµ Ñ‡Ð°ÑÑ‚Ð¾ Ð·Ð³Ð°Ð´ÑƒÑ”Ñ‚ÑŒÑÑ Ð· Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ¾ÑŽ
    'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸Ðº': (49.0333, 28.2167),
    'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸ÐºÑƒ': (49.0333, 28.2167),
    'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸ÐºÐ°': (49.0333, 28.2167),
    'ÑÑ‚Ñ€Ð¸Ð¶Ð°Ð²ÐºÐ°': (49.6833, 28.6000),
    'ÑÑ‚Ñ€Ð¸Ð¶Ð°Ð²Ñ†Ñ–': (49.6833, 28.6000),
    'ÑÑ‚Ñ€Ð¸Ð¶Ð°Ð²ÐºÑƒ': (49.6833, 28.6000),
    'Ñ‡Ð¾Ñ€Ð½Ð¸Ð¹ Ð¾ÑÑ‚Ñ€Ñ–Ð²': (49.7167, 28.6167),
    'Ñ‡Ð¾Ñ€Ð½Ð¾Ð¼Ñƒ Ð¾ÑÑ‚Ñ€Ð¾Ð²Ñ–': (49.7167, 28.6167),
    'Ñ‡Ð¾Ñ€Ð½Ð¸Ð¹ Ð¾ÑÑ‚Ñ€Ñ–Ð²': (49.7167, 28.6167),
}

for _vn_name, _vn_coords in VINNYTSIA_CITY_COORDS.items():
    CITY_COORDS.setdefault(_vn_name, _vn_coords)

# Volyn Oblast settlements (auto-generated from city_ukraine.json)
VOLYN_CITY_COORDS = {
    'Ð°Ð´Ð°Ð¼Ñ‡ÑƒÐºÐ¸': (50.7472, 25.3254),
    'Ð°Ð´Ð°Ð¼Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð°Ð¼Ð±ÑƒÐºÑ–Ð²': (50.7472, 25.3254),
    'Ð°Ð½Ñ‚Ð¾Ð½Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð°Ñ€ÑÐµÐ½Ð¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ð°Ð±Ð°Ñ†Ñ–': (50.7472, 25.3254),
    'Ð±Ð°Ð¹ÐºÑ–Ð²Ñ†Ñ–': (50.7472, 25.3254),
    'Ð±Ð°ÐºÑ–Ð²Ñ†Ñ–': (50.7472, 25.3254),
    'Ð±Ð°Ñ€Ð²Ñ–Ð½Ð¾Ðº': (50.7472, 25.3254),
    'Ð±Ð°Ñ…Ñ–Ð²': (50.7472, 25.3254),
    'Ð±Ð°ÑˆÐ»Ð¸ÐºÐ¸': (50.7472, 25.3254),
    'Ð±Ð°ÑˆÐ¾Ð²Ð°': (50.7472, 25.3254),
    'Ð±Ð°Ñ—Ð²': (50.7472, 25.3254),
    'Ð±ÐµÐ³ÐµÑ‚Ð°': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ³Ð¾Ð²Ðµ': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ¶Ð°Ð½ÐºÐ°': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ¶Ð½Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ¶Ñ†Ñ–': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð°': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð½Ð° Ð²Ð¾Ð»Ñ': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð½Ð¸ÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÐ·Ð¾Ð»ÑƒÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÑÑ‚ÐµÑ‡ÐºÐ¾': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÑÑ‚ÑÐ½Ðµ': (50.7472, 25.3254),
    'Ð±ÐµÑ€ÐµÑÑŒÐº': (50.7472, 25.3254),
    'Ð±ÐµÑ€Ð¼ÐµÑˆÑ–Ð²': (50.7472, 25.3254),
    'Ð±Ð¸ÑÑ‚Ñ€Ð¾Ð²Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð±Ð¸Ñ‚ÐµÐ½ÑŒ': (50.7472, 25.3254),
    'Ð±Ð¸Ñ…Ñ–Ð²': (50.7472, 25.3254),
    'Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ‚Ð½Ðµ': (50.7472, 25.3254),
    'Ð±Ð»Ð°Ð¶ÐµÐ½Ð¸Ðº': (50.7472, 25.3254),
    'Ð±Ð¾Ð±Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ð¾Ð±Ð»Ð¸': (50.7472, 25.3254),
    'Ð±Ð¾Ð³Ð¾Ð»ÑŽÐ±Ð¸': (50.7472, 25.3254),
    'Ð±Ð¾Ð³ÑƒÐ½Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð±Ð¾Ð³ÑƒÑˆÑ–Ð²ÐºÐ°': (50.7472, 25.3254),
    "Ð±Ð¾Ð³ÑƒÑˆÑ–Ð²ÑÑŒÐºÐ° Ð¼Ð°Ñ€'ÑÐ½Ñ–Ð²ÐºÐ°": (50.7472, 25.3254),
    'Ð±Ð¾Ð´ÑÑ‡Ñ–Ð²': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð°Ñ‚Ð¸Ð½': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€ÐµÐ¼Ñ‰Ð¸Ð½Ð°': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð·Ð¾Ð²Ð°': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¸ÑÐºÐ¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ð²Ðµ': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ð²Ð½Ðµ': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ð²ÑƒÑ…Ð°': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ñ…Ñ–Ð²': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ð¾Ñ‡Ð¸Ñ‡Ðµ': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ñ‚Ð½Ñ–Ð²': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ñ‚ÑÑ…Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð±Ð¾Ñ€Ñ‰Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð±Ð¾Ñ‚Ð¸Ð½': (50.7472, 25.3254),
    'Ð±Ñ€Ð°Ð½Ð¸': (50.7472, 25.3254),
    'Ð±Ñ€Ð¸Ñ‰Ðµ': (50.7472, 25.3254),
    'Ð±Ñ€Ð¾Ð´Ð¸': (50.7472, 25.3254),
    'Ð±Ñ€Ð¾Ð´ÑÑ‚Ð¸Ð½Ðµ': (50.7472, 25.3254),
    'Ð±Ñ€Ð¾Ð½Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð±Ñ€ÑƒÐ½ÐµÑ‚Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð±Ñ€ÑƒÑ…Ð¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ñ€Ñ–Ð´ÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÑƒÐ±Ð½Ñ–Ð²': (50.7472, 25.3254),
    'Ð±ÑƒÐ´Ð¸Ñ‰Ðµ': (50.7472, 25.3254),
    'Ð±ÑƒÐ´ÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÑƒÐ´Ð½Ð¸ÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÑƒÐ´ÑÑ‚Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±ÑƒÐ¶Ð°Ð½Ð¸': (50.7472, 25.3254),
    'Ð±ÑƒÐ¶Ð°Ð½ÐºÐ°': (50.7472, 25.3254),
    'Ð±ÑƒÐ¶ÐºÐ¾Ð²Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±ÑƒÐ·Ð°ÐºÐ¸': (50.7472, 25.3254),
    'Ð±ÑƒÐºÑ–Ð²': (50.7472, 25.3254),
    'Ð±ÑƒÑ€ÐºÐ°Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±ÑƒÑ†Ð¸Ð½ÑŒ': (50.7472, 25.3254),
    'Ð±ÑƒÑ‡Ð¸Ð½': (50.7472, 25.3254),
    'Ð±ÑƒÑÐ½Ð¸': (50.7472, 25.3254),
    'Ð±Ñ–Ð»Ð°ÑˆÑ–Ð²': (50.7472, 25.3254),
    'Ð±Ñ–Ð»Ð¸Ð½': (50.7472, 25.3254),
    'Ð±Ñ–Ð»Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð±Ñ–Ð»Ð¾Ð¿Ñ–Ð»ÑŒ': (50.7472, 25.3254),
    'Ð±Ñ–Ð»Ð¾ÑÑ‚Ð¾Ðº': (50.7472, 25.3254),
    'Ð±Ñ–Ñ€ÐºÐ¸': (50.7472, 25.3254),
    "Ð²'ÑÐ·Ñ–Ð²Ð½Ðµ": (50.7472, 25.3254),
    "Ð²Ð°Ð»ÐµÑ€'ÑÐ½Ñ–Ð²ÐºÐ°": (50.7472, 25.3254),
    'Ð²Ð°Ñ‚Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð°Ñ‚Ð¸Ð½ÐµÑ†ÑŒ': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²ÐµÐ´Ð¼ÐµÐ¶ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð³Ð»ÑƒÑˆÐ°': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð¾ÑÐ½Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ° ÑÐ±Ð»ÑƒÐ½ÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ ÐºÑƒÑ€Ñ–Ð½ÑŒ': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð¾Ð±Ð·Ð¸Ñ€': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð¾ÐºÐ¾Ñ€ÑÑŒÐº': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð¾Ð¼ÐµÐ»ÑÐ½Ð¸Ðº': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¹ Ð¿Ð¾Ñ€ÑÑŒÐº': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸Ð¼Ñ‡Ðµ': (50.7472, 25.3254),
    'Ð²ÐµÐ»Ð¸Ñ†ÑŒÐº': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ð±Ð°': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ð±Ð°Ñ—Ð²': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ð±Ð¸Ñ‡Ð½Ðµ': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ð±ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ð±Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ñ…Ð¸': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ñ…Ð½Ñ–Ð²': (50.7472, 25.3254),
    'Ð²ÐµÑ€Ñ…Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÑÐµÐ»Ðµ': (50.7472, 25.3254),
    'Ð²ÐµÑÐ½ÑÐ½ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÐµÑ‚Ð»Ð¸': (50.7472, 25.3254),
    'Ð²Ð¸Ð³Ð½Ð°Ð½ÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¸Ð³ÑƒÑ€Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð²Ð¸Ð´ÐµÑ€Ñ‚Ð°': (50.7472, 25.3254),
    'Ð²Ð¸Ð´Ñ€Ð°Ð½Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð²Ð¸Ð´Ñ€Ð¸Ñ‡Ñ–': (50.7472, 25.3254),
    'Ð²Ð¸Ð¶Ð³Ñ–Ð²': (50.7472, 25.3254),
    'Ð²Ð¸Ð¶Ð¸Ñ‡Ð½Ð¾': (50.7472, 25.3254),
    'Ð²Ð¸Ð½Ñ–Ð¼Ð¾Ðº': (50.7472, 25.3254),
    'Ð²Ð¸ÑÐ¾ÐºÐµ': (50.7472, 25.3254),
    'Ð²Ð¸ÑÐ¾Ñ†ÑŒÐº': (50.7472, 25.3254),
    'Ð²Ð¸ÑÐ¾Ñ‡Ð½Ðµ': (50.7472, 25.3254),
    'Ð²Ð¸Ñ‚ÐµÐ½ÑŒ': (50.7472, 25.3254),
    'Ð²Ð¸Ñ‚ÑƒÐ»Ðµ': (50.7472, 25.3254),
    'Ð²Ð¸ÑˆÐµÐ½ÑŒÐºÐ¸': (50.7472, 25.3254),
    'Ð²Ð¸ÑˆÐ½Ñ–Ð²': (50.7472, 25.3254),
    'Ð²Ð¸ÑˆÐ½Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð²Ñ‡Ð¸Ñ†ÑŒÐº': (50.7472, 25.3254),
    'Ð²Ð¾Ð¹Ð½Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¸Ñ†Ñ-Ð´Ñ€ÑƒÐ¶ÐºÐ¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¸Ñ†Ñ-Ð»Ð¾Ð±Ð°Ñ‡Ñ–Ð²ÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¸Ñ†Ñ-Ð¼Ð¾Ñ€Ð¾Ð·Ð¾Ð²Ð¸Ñ†ÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€-Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ¸Ð¹': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¾Ð´Ð¸Ð¼Ð¸Ñ€Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ð¾ÑˆÐºÐ¸': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ñ': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ñ-ÐºÐ¾Ð²ÐµÐ»ÑŒÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ñ-Ð»ÑŽÐ±Ð¸Ñ‚Ñ–Ð²ÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ð»Ñ-ÑÐ²Ñ–Ð¹Ñ‡Ñ–Ð²ÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ð¾Ñ€Ð¾ÐºÐ¾Ð¼Ð»Ðµ': (50.7472, 25.3254),
    'Ð²Ð¾Ñ€Ð¾Ð½Ð°': (50.7472, 25.3254),
    'Ð²Ð¾Ñ€Ð¾Ð½Ñ‡Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð¾Ñ€Ð¾Ñ‚Ð½Ñ–Ð²': (50.7472, 25.3254),
    'Ð²Ð¾Ñ€Ñ‡Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð¾Ñ‰Ð°Ñ‚Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð¾ÑŽÑ‚Ð¸Ð½': (50.7472, 25.3254),
    'Ð²Ð¾Ñ”Ð³Ð¾Ñ‰Ð°': (50.7472, 25.3254),
    'Ð²ÑÐµÐ²Ð¾Ð»Ð¾Ð´Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²ÑƒÐ¶Ð¸ÑÑŒÐº': (50.7472, 25.3254),
    'Ð²Ñ–Ð²Ñ‡Ð¸Ñ†ÑŒÐº': (50.7472, 25.3254),
    'Ð²Ñ–Ð´ÑƒÑ‚Ð¸': (50.7472, 25.3254),
    'Ð²Ñ–Ð¹Ð½Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð²Ñ–ÐºÑ‚Ð¾Ñ€ÑÐ½Ð¸': (50.7472, 25.3254),
    'Ð²Ñ–Ð»': (50.7472, 25.3254),
    'Ð²Ñ–Ð»Ð¸Ñ†Ñ': (50.7472, 25.3254),
    'Ð²Ñ–Ð»ÑŒÐºÐ°-Ð¿Ñ–Ð´Ð³Ð¾Ñ€Ð¾Ð´Ð½ÐµÐ½ÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ñ–Ð»ÑŒÐºÐ°-ÑÐ°Ð´Ñ–Ð²ÑÑŒÐºÐ°': (50.7472, 25.3254),
    'Ð²Ñ–Ð»ÑŒÑ…Ñ–Ð²ÐºÐ°': (50.7472, 25.3254),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½ÐºÐ°': (50.7472, 25.3254),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð¸': (48.4667, 32.2667),  # Ð’Ñ–Ð»ÑŒÑˆÐ°Ð½Ð¸, ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ð¼': (48.4667, 32.2667),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ñ…': (48.4667, 32.2667),
    'Ð²Ñ–Ñ‚Ð¾Ð½Ñ–Ð¶': (50.7472, 25.3254),
    'Ð²Ñ–Ñ‡Ð¸Ð½Ñ–': (50.7472, 25.3254),
    # ... (truncated for brevity - 1087 total settlements)
    'Ñ–Ð·Ð¾Ð²': (50.7472, 25.3254),
}

for _volyn_name, _volyn_coords in VOLYN_CITY_COORDS.items():
    CITY_COORDS.setdefault(_volyn_name, _volyn_coords)

# Kherson Oblast Cities - Adding specific variants for Ð³Ð¾Ñ€Ð¾Ð´Ð° with oblast designation
KHERSON_CITY_COORDS = {
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ° Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°': (46.64, 32.88),
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ° (Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°)': (46.64, 32.88),
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ° Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (46.64, 32.88),
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ° Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (46.64, 32.88),
    'Ð±Ñ–Ð»Ð¾Ð·ÐµÑ€ÐºÐ° Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': (46.64, 32.88),
}

for _ks_name, _ks_coords in KHERSON_CITY_COORDS.items():
    CITY_COORDS.setdefault(_ks_name, _ks_coords)

# Additional missing settlements from large UAV course messages
MISSING_SETTLEMENTS = {
    # Vinnytsia Oblast
    'Ð¿ÐµÐ½ÑŒÐºÑ–Ð²ÐºÐ°': (49.1667, 28.5500), 'Ð¿ÐµÐ½ÑŒÐºÑ–Ð²ÐºÑƒ': (49.1667, 28.5500), 'Ð¿ÐµÐ½ÑŒÐºÑ–Ð²Ñ†Ñ–': (49.1667, 28.5500),
    'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸Ðº': (49.0333, 28.2167), 'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸ÐºÑƒ': (49.0333, 28.2167), 'ÑÑ‚Ð°Ð½Ñ–ÑÐ»Ð°Ð²Ñ‡Ð¸ÐºÐ¸': (49.0333, 28.2167),
    'Ð²ÐµÐ½Ð´Ð¸Ñ‡Ð°Ð½Ð¸': (48.4167, 27.9500), 'Ð²ÐµÐ½Ð´Ð¸Ñ‡Ð°Ð½Ð°Ñ…': (48.4167, 27.9500), 'Ð²ÐµÐ½Ð´Ð¸Ñ‡Ð°Ð½Ð¸Ì': (48.4167, 27.9500),
    'Ð¼Ð°Ð·ÑƒÑ€Ñ–Ð²ÐºÐ°': (49.1000, 28.8500), 'Ð¼Ð°Ð·ÑƒÑ€Ñ–Ð²ÐºÑƒ': (49.1000, 28.8500), 'Ð¼Ð°Ð·ÑƒÑ€Ñ–Ð²Ñ†Ñ–': (49.1000, 28.8500),
    # Odesa Oblast
    'ÑˆÐ¸Ñ€ÑÑ”Ð²Ðµ': (46.6167, 30.1667), 'ÑˆÐ¸Ñ€ÑÑ”Ð²Ð¾Ð¼Ñƒ': (46.6167, 30.1667), 'ÑˆÐ¸Ñ€ÑÑ”Ð²Ð°': (46.6167, 30.1667),
    # Kirovohrad Oblast
    'ÑÐ²Ñ–Ñ‚Ð»Ð¾Ð²Ð¾Ð´ÑÑŒÐº': (49.0556, 33.2433),  # Ð¡Ð²Ñ–Ñ‚Ð»Ð¾Ð²Ð¾Ð´ÑÑŒÐº, ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
    'Ð³Ð°Ð¹Ð´Ð°Ð¼Ð°Ñ†ÑŒÐºÐµ': (48.7833, 32.4333), 'Ð³Ð°Ð¹Ð´Ð°Ð¼Ð°Ñ†ÑŒÐºÐ¾Ð¼': (48.7833, 32.4333), 'Ð³Ð°Ð¹Ð´Ð°Ð¼Ð°Ñ†ÑŒÐºÐ¾Ð³Ð¾': (48.7833, 32.4333),
    'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð¸': (48.4667, 32.2667), 'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ð¼': (48.4667, 32.2667), 'Ð²Ñ–Ð»ÑŒÑˆÐ°Ð½Ð°Ñ…': (48.4667, 32.2667),
    # Poltava Oblast  
    'Ð²ÐµÐ»Ð¸ÐºÑ– ÑÐ¾Ñ€Ð¾Ñ‡Ð¸Ð½Ñ†Ñ–': (50.0667, 34.2833), 'Ð²ÐµÐ»Ð¸ÐºÐ¸Ñ… ÑÐ¾Ñ€Ð¾Ñ‡Ð¸Ð½Ñ†ÑÑ…': (50.0667, 34.2833), 'Ð²ÐµÐ»Ð¸ÐºÐ¸Ð¼Ð¸ ÑÐ¾Ñ€Ð¾Ñ‡Ð¸Ð½Ñ†ÑÐ¼Ð¸': (50.0667, 34.2833),
    'Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ': (49.3833, 33.2667), 'Ð³Ð»Ð¾Ð±Ð¸Ð½Ð¾Ð¼Ñƒ': (49.3833, 33.2667), 'Ð³Ð»Ð¾Ð±Ð¸Ð½Ð°': (49.3833, 33.2667),
    # Sumy Oblast
    'ÑÑ‚ÐµÐ¿Ð°Ð½Ñ–Ð²ÐºÐ°': (50.7833, 34.5500), 'ÑÑ‚ÐµÐ¿Ð°Ð½Ñ–Ð²ÐºÑƒ': (50.7833, 34.5500), 'ÑÑ‚ÐµÐ¿Ð°Ð½Ñ–Ð²Ñ†Ñ–': (50.7833, 34.5500),
    'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°': (51.1167, 34.4500), 'Ð»Ð¸Ð¿Ð¾Ð²Ñ–Ð¹ Ð´Ð¾Ð»Ð¸Ð½Ñ–': (51.1167, 34.4500), 'Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ñƒ': (51.1167, 34.4500),
    # Chernihiv Oblast
    'Ð³Ð¾Ð½Ñ‡Ð°Ñ€Ñ–Ð²ÑÑŒÐºÐµ': (51.3667, 31.7833), 'Ð³Ð¾Ð½Ñ‡Ð°Ñ€Ñ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ': (51.3667, 31.7833), 'Ð³Ð¾Ð½Ñ‡Ð°Ñ€Ñ–Ð²ÑÑŒÐºÐ¾Ð³Ð¾': (51.3667, 31.7833),
    # Kyiv Oblast
    'ÐºÑ€Ð°ÑÑÑ‚Ð¸Ñ‡Ñ–': (50.3167, 30.0500), 'ÐºÑ€Ð°ÑÑÑ‚Ð¸Ñ‡Ð°Ñ…': (50.3167, 30.0500), 'ÐºÑ€Ð°ÑÑÑ‚Ð¸Ñ‡Ð°Ð¼Ð¸': (50.3167, 30.0500),
    # Zhytomyr Oblast
    'Ð½Ð¾Ð²Ñ– Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ñ–': (51.3833, 27.7167), 'Ð½Ð¾Ð²Ð¸Ñ… Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ð°Ñ…': (51.3833, 27.7167), 'Ð½Ð¾Ð²Ð¸Ð¼Ð¸ Ð±Ñ–Ð»Ð¾ÐºÐ¾Ñ€Ð¾Ð²Ð¸Ñ‡Ð°Ð¼Ð¸': (51.3833, 27.7167),
    'Ñ‡ÐµÑ€Ð½ÑÑ…Ñ–Ð²': (50.0667, 28.8833), 'Ñ‡ÐµÑ€Ð½ÑÑ…Ð¾Ð²Ñ–': (50.0667, 28.8833), 'Ñ‡ÐµÑ€Ð½ÑÑ…Ð¾Ð²Ð¾Ð¼': (50.0667, 28.8833),
    'Ð°Ð½Ð´Ñ€ÑƒÑˆÑ–Ð²ÐºÐ°': (50.0833, 29.8167), 'Ð°Ð½Ð´Ñ€ÑƒÑˆÑ–Ð²ÐºÑƒ': (50.0833, 29.8167), 'Ð°Ð½Ð´Ñ€ÑƒÑˆÑ–Ð²Ñ†Ñ–': (50.0833, 29.8167),
    'Ð»ÑŽÐ±Ð°Ñ€': (49.9167, 27.5333), 'Ð»ÑŽÐ±Ð°Ñ€Ñ–': (49.9167, 27.5333), 'Ð»ÑŽÐ±Ð°Ñ€Ñƒ': (49.9167, 27.5333),
    # Khmelnytskyi Oblast
    'Ð°Ð´Ð°Ð¼Ð¿Ñ–Ð»ÑŒ': (49.7667, 26.9667), 'Ð°Ð´Ð°Ð¼Ð¿Ð¾Ð»Ñ–': (49.7667, 26.9667), 'Ð°Ð´Ð°Ð¼Ð¿Ð¾Ð»ÑŽ': (49.7667, 26.9667),
    # Rivne Oblast
    'Ð´ÐµÑ€Ð°Ð¶Ð½Ðµ': (50.9167, 25.7500), 'Ð´ÐµÑ€Ð°Ð¶Ð½Ð¾Ð¼Ñƒ': (50.9167, 25.7500), 'Ð´ÐµÑ€Ð°Ð¶Ð½Ð¾Ð³Ð¾': (50.9167, 25.7500),
    'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ðµ': (50.3167, 26.1500), 'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ð¾Ð¼Ñƒ': (50.3167, 26.1500), 'Ñ€Ð¾ÐºÐ¸Ñ‚Ð½Ð¾Ð³Ð¾': (50.3167, 26.1500),
    'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†Ñ': (51.5667, 26.5667), 'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†ÑŽ': (51.5667, 26.5667), 'Ð´ÑƒÐ±Ñ€Ð¾Ð²Ð¸Ñ†Ñ–': (51.5667, 26.5667),
    # Volyn Oblast
    'ÐºÐ°Ð¼Ñ–Ð½ÑŒ-ÐºÐ°ÑˆÐ¸Ñ€ÑÑŒÐºÐ¸Ð¹': (51.6167, 24.9667), 'ÐºÐ°Ð¼ÐµÐ½ÑŽ-ÐºÐ°ÑˆÐ¸Ñ€ÑÑŒÐºÐ¾Ð¼Ñƒ': (51.6167, 24.9667), 'ÐºÐ°Ð¼ÐµÐ½ÐµÐ¼-ÐºÐ°ÑˆÐ¸Ñ€ÑÑŒÐºÐ¸Ð¼': (51.6167, 24.9667),
}

for _ms_name, _ms_coords in MISSING_SETTLEMENTS.items():
    CITY_COORDS.setdefault(_ms_name, _ms_coords)

# Mapping city -> oblast stem (lowercase stems used earlier) for disambiguation when region already detected.
# Minimal subset; extend as needed.
CITY_TO_OBLAST = {
    'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²',
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²',
    'ÐºÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²',
    'Ð»ÑŒÐ²Ñ–Ð²': 'Ð»ÑŒÐ²Ñ–Ð²',
    'ÑÑ‚Ñ€Ð¸Ð¹': 'Ð»ÑŒÐ²Ñ–Ð²',
    'Ð´Ñ€Ð¾Ð±Ð¾Ð±Ð¸Ñ‡': 'Ð»ÑŒÐ²Ñ–Ð²',
    'ÐºÐ¸Ñ—Ð²': 'ÐºÐ¸Ñ—Ð²',
    'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°': 'ÐºÐ¸Ñ—Ð²',
    'Ð±Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ': 'ÐºÐ¸Ñ—Ð²',
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²',
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²',
    'Ð²ÐµÐ»Ð¸ÐºÐ° Ð±Ð°Ð³Ð°Ñ‡ÐºÐ°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²',
    'Ð³Ð°Ð´ÑÑ‡': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²',
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€': 'Ð¶Ð¸Ñ‚Ð¾Ð¼',
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸': 'Ñ‡ÐµÑ€ÐºÐ°',
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³',
    'ÑÑƒÐ¼Ð¸': 'ÑÑƒÐ¼',
    'Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°': 'ÑÑƒÐ¼',
    'Ñ‚Ñ€Ð¾ÑÑ‚ÑÐ½ÐµÑ†ÑŒ': 'ÑÑƒÐ¼',
    'Ð»ÐµÐ±ÐµÐ´Ð¸Ð½': 'ÑÑƒÐ¼',
    'ÑƒÐ»ÑÐ½Ñ–Ð²ÐºÐ°': 'ÑÑƒÐ¼',
    'Ð¾Ð´ÐµÑÐ°': 'Ð¾Ð´ÐµÑ',
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°',
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–': 'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†',
    'Ñ€Ñ–Ð²Ð½Ðµ': 'Ñ€Ñ–Ð²Ð½',
    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ': 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿',
    'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´': 'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´',
    'Ð»ÑƒÑ†ÑŒÐº': 'Ð²Ð¾Ð»Ð¸Ð½',
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ': 'Ð·Ð°Ð¿Ð¾Ñ€',
    'Ñ…Ð°Ñ€ÐºÑ–Ð²': 'Ñ…Ð°Ñ€ÐºÑ–Ð²',
    'Ð°Ñ…Ñ‚Ð¸Ñ€ÐºÐ°': 'ÑÑƒÐ¼',
}

OBLAST_CENTERS = {
    'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': (48.0433, 37.7974), 'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð¸': (48.0433, 37.7974), 'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ñƒ': (48.0433, 37.7974), 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.0433, 37.7974),
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': (48.4500, 34.9830), 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸': (48.4500, 34.9830), 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.4500, 34.9830),
    'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ‰Ð¸Ð½Ð°': (48.5132, 32.2597), 'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ‰Ð¸Ð½Ð¸': (48.5132, 32.2597), 'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.5132, 32.2597),
    'Ð´Ð½ÐµÐ¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': (48.4500, 34.9830), 'Ð´Ð½ÐµÐ¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñ‹': (48.4500, 34.9830),
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': (51.4982, 31.2893), 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸': (51.4982, 31.2893),
    'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': (49.9935, 36.2304), 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð¸': (49.9935, 36.2304)
    , 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (48.4500, 34.9830), 'Ð´Ð½ÐµÐ¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð».': (48.4500, 34.9830)
    , 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (51.4982, 31.2893), 'Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð».': (51.4982, 31.2893)
    , 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.9935, 36.2304), 'Ñ…Ð°Ñ€ÑŒÐºÐ¾Ð²ÑÐºÐ°Ñ Ð¾Ð±Ð».': (49.9935, 36.2304)
    , 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': (50.9077, 34.7981), 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð¸': (50.9077, 34.7981), 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñƒ': (50.9077, 34.7981), 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (50.9077, 34.7981), 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.9077, 34.7981), 'ÑÑƒÐ¼ÑÐºÐ°Ñ Ð¾Ð±Ð».': (50.9077, 34.7981)
    , 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': (49.5883, 34.5514), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð¸': (49.5883, 34.5514), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ñƒ': (49.5883, 34.5514), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.5883, 34.5514), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.5883, 34.5514)
    , 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': (50.4501, 30.5234), 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð¸': (50.4501, 30.5234), 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ñƒ': (50.4501, 30.5234), 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (50.4501, 30.5234), 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.4501, 30.5234)
    , 'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°': (49.8397, 24.0297), 'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð¸': (49.8397, 24.0297), 'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ñƒ': (49.8397, 24.0297), 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.8397, 24.0297), 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.8397, 24.0297)
    , 'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°': (49.4444, 32.0598), 'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð¸': (49.4444, 32.0598), 'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ñƒ': (49.4444, 32.0598), 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.4444, 32.0598), 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.4444, 32.0598)
    , 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': (50.2547, 28.6587), 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð¸': (50.2547, 28.6587), 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ñƒ': (50.2547, 28.6587), 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (50.2547, 28.6587), 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.2547, 28.6587)
    , 'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': (46.6354, 32.6169), 'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð¸': (46.6354, 32.6169), 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÑƒ': (46.6354, 32.6169), 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (46.6354, 32.6169), 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (46.6354, 32.6169)
    , 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': (46.9750, 31.9946), 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð¸': (46.9750, 31.9946), 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÑƒ': (46.9750, 31.9946), 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (46.9750, 31.9946), 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (46.9750, 31.9946)
    , 'Ð¾Ð´ÐµÑÑ‰Ð¸Ð½Ð°': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑÑŒÑ‡Ð¸Ð½Ð°': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑÑŒÑ‰Ð¸Ð½Ð¸': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑÑŒÐºÑƒ': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð».': (46.4825, 30.7233)
    , 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð¸': (46.4825, 30.7233), 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ñƒ': (46.4825, 30.7233)
    , 'Ð²Ð¾Ð»Ð¸Ð½ÑŒ': (50.7472, 25.3254), 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (50.7472, 25.3254), 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.7472, 25.3254)
    , 'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°': (50.6199, 26.2516), 'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð¸': (50.6199, 26.2516), 'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ñƒ': (50.6199, 26.2516), 'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (50.6199, 26.2516), 'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.6199, 26.2516)
    , 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð°': (49.5535, 25.5948), 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð¸': (49.5535, 25.5948), 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ñƒ': (49.5535, 25.5948), 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.5535, 25.5948), 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ° Ð¾Ð±Ð».': (49.5535, 25.5948)
    , 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': (49.4229, 26.9871), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð¸': (49.4229, 26.9871), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ñƒ': (49.4229, 26.9871), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.4229, 26.9871), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð».': (49.4229, 26.9871)
    , 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': (49.2331, 28.4682), 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð¸': (49.2331, 28.4682), 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ñƒ': (49.2331, 28.4682), 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (49.2331, 28.4682), 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð».': (49.2331, 28.4682)
    , 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ð¸Ð½Ð°': (49.2331, 28.4682), 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ð¸Ð½Ð¸': (49.2331, 28.4682)
    , 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚Ñ': (48.6208, 22.2879), 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.6208, 22.2879), 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ° Ð¾Ð±Ð».': (48.6208, 22.2879)
    , 'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ‡Ñ‡Ð¸Ð½Ð°': (48.2921, 25.9358), 'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.2921, 25.9358), 'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð».': (48.2921, 25.9358)
    , 'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': (48.5740, 39.3078), 'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð¸': (48.5740, 39.3078), 'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ñƒ': (48.5740, 39.3078), 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.5740, 39.3078), 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (48.5740, 39.3078)
}

# Add no-dot variants for keys ending with ' Ð¾Ð±Ð».' (common source variation without the dot)
_no_dot_variants = {}
for _k,_v in list(OBLAST_CENTERS.items()):
    if _k.endswith(' Ð¾Ð±Ð».'):
        nd = _k[:-1]  # remove trailing '.' only
        if nd not in OBLAST_CENTERS:
            _no_dot_variants[nd] = _v
OBLAST_CENTERS.update(_no_dot_variants)

# Canonical forms for geocoding queries (region headers -> '<adj> Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ')
REGION_GEOCODE_CANON = {
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°':'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°':'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°':'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°':'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
    'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°':'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°':'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°':'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°':'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
    'Ð²Ð¾Ð»Ð¸Ð½ÑŒ':'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð¾Ð´ÐµÑÑ‰Ð¸Ð½Ð°':'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð¾Ð´ÐµÑÑŒÑ‡Ð¸Ð½Ð°':'Ð¾Ð´ÐµÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°':'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°':'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°':'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°':'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð°':'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ‡Ñ‡Ð¸Ð½Ð°':'Ñ‡ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚Ñ':'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°':'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°':'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
    'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°':'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ð¸Ð½Ð°':'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ'
}

# Explicit (city, oblast form) overrides to disambiguate duplicate settlement names across oblasts.
# Key: (normalized_city, normalized_region_hint as appears in message)
OBLAST_CITY_OVERRIDES = {
    ('Ð±Ð¾Ñ€Ð¾Ð²Ð°', 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».'): (49.3743, 37.6179),  # Ð‘Ð¾Ñ€Ð¾Ð²Ð° (Ð†Ð·ÑŽÐ¼ÑÑŒÐºÐ¸Ð¹ Ñ€-Ð½, Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°)
}

# Ð Ð°Ð¹Ð¾Ð½ (district) fallback centers (Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÑ‚ÑŒ). ÐšÐ»ÑŽÑ‡Ð¸ Ð² Ð½Ð¸Ð¶Ð½ÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ðµ Ð±ÐµÐ· ÑÐ»Ð¾Ð²Ð° 'Ñ€Ð°Ð¹Ð¾Ð½'.
RAION_FALLBACK = {
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': (48.2767, 37.1763),  # ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐº (Ð”Ð¾Ð½ÐµÑ†ÑŒÐºÐ°)
    'Ð¿Ð¾ÐºÑ€Ð¾Ð²ÑÐºÐ¸Ð¹': (48.2767, 37.1763),
    'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ¸Ð¹': (48.5350, 35.8700),  # ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´
    'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÐºÐ¸Ð¹': (48.5350, 35.8700),
    'Ð¿Ð¾Ð»Ð¾Ð³Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (47.4840, 36.2536),  # ÐŸÐ¾Ð»Ð¾Ð³Ð¸ (approx center of Polohivskyi raion)
    'Ð¿Ð¾Ð»Ð¾Ð³Ð¾Ð²ÑÐºÐ¸Ð¹': (47.4840, 36.2536),
    'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐºÐ¸Ð¹': (48.7389, 37.5848),
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ¸Ð¹': (46.9750, 31.9946),  # Mykolaivskyi raion (approx Mykolaiv city center)
    'Ð½Ð¸ÐºÐ¾Ð»Ð°ÐµÐ²ÑÐºÐ¸Ð¹': (46.9750, 31.9946),
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°ÐµÐ²ÑÐºÐ¸Ð¹': (46.9750, 31.9946),
    'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÐºÐ¸Ð¹': (48.7389, 37.5848),
    'Ð±Ð°Ñ…Ð¼ÑƒÑ‚ÑÑŒÐºÐ¸Ð¹': (48.5941, 38.0021),
    'Ð±Ð°Ñ…Ð¼ÑƒÑ‚ÑÐºÐ¸Ð¹': (48.5941, 38.0021),
    'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ¸Ð¹': (49.4444, 32.0598),
    'Ñ‡ÐµÑ€ÐºÐ°ÑÑÐºÐ¸Ð¹': (49.4444, 32.0598),
    'Ð¾Ð´ÐµÑÑŒÐºÐ¸Ð¹': (46.4825, 30.7233),
    'Ð¾Ð´ÐµÑÑÐºÐ¸Ð¹': (46.4825, 30.7233),
    'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': (49.9935, 36.2304),
    'Ñ…Ð°Ñ€ÑŒÐºÐ¾Ð²ÑÐºÐ¸Ð¹': (49.9935, 36.2304),
    # ÐÐ¾Ð²Ñ‹Ðµ Ñ€Ð°Ð¹Ð¾Ð½Ñ‹ Ð´Ð»Ñ Ð¼Ð½Ð¾Ð³Ð¾ÐºÑ€Ð°Ñ‚Ð½Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
    'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹': (51.2375, 33.2020), 'ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÐºÐ¸Ð¹': (51.2375, 33.2020),
    'ÑÑƒÐ¼ÑÑŒÐºÐ¸Ð¹': (50.8500, 34.9500), 'ÑÑƒÐ¼ÑÐºÐ¸Ð¹': (50.8500, 34.9500),  # Shifted SE from Sumy city center to represent district area
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (51.4982, 31.2893), 'Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²ÑÐºÐ¸Ð¹': (51.4982, 31.2893),
    'Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´ÑÑŒÐºÐ¸Ð¹': (50.5850, 30.4915), 'Ð²Ñ‹ÑˆÐ³Ð¾Ñ€Ð¾Ð´ÑÐºÐ¸Ð¹': (50.5850, 30.4915),
    'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹': (51.9874, 33.2620), 'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÐµÐ²ÐµÑ€ÑÐºÐ¸Ð¹': (51.9874, 33.2620),
    'Ñ‡ÑƒÐ³ÑƒÑ—Ð²ÑÑŒÐºÐ¸Ð¹': (49.8353, 36.6880), 'Ñ‡ÑƒÐ³ÐµÐ²ÑÐºÐ¸Ð¹': (49.8353, 36.6880), 'Ñ‡ÑƒÐ³ÐµÐ²ÑÑŒÐºÐ¸Ð¹': (49.8353, 36.6880), 'Ñ‡ÑƒÐ³ÑƒÐµÐ²ÑÐºÐ¸Ð¹': (49.8353, 36.6880)
    , 'ÑÐ¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': (48.3167, 36.5000), 'ÑÐ¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²ÑÐºÐ¸Ð¹': (48.3167, 36.5000)
    # Zaporizkyi raion (shifted off exact city center to represent wider district)
    , 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ¸Ð¹': (47.9000, 35.2500), 'Ð·Ð°Ð¿Ð¾Ñ€Ð¾Ð¶ÑÐºÐ¸Ð¹': (47.9000, 35.2500)
    , 'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': (46.1871, 30.3410), 'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÐºÐ¾Ð³Ð¾': (46.1871, 30.3410), 'Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾': (46.1871, 30.3410)
    # Dnipro oblast & Dnipro city internal districts (to avoid fallback to generic city center)
    , 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': (48.4500, 35.1000), 'Ð´Ð½ÐµÐ¿Ñ€Ð¾Ð²ÑÐºÐ¸Ð¹': (48.4500, 35.1000)  # Dnipro Raion (approx centroid)
    , 'ÑÐ°Ð¼Ð°Ñ€ÑÑŒÐºÐ¸Ð¹': (48.5380, 35.1500), 'ÑÐ°Ð¼Ð°Ñ€ÑÐºÐ¸Ð¹': (48.5380, 35.1500), 'ÑÐ°Ð¼Ð°Ñ€Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (48.5380, 35.1500)  # Samarskyi (approx east bank)
    , 'Ð¼Ð¸Ñ€Ð³Ð¾Ñ€Ð¾Ð´ÑÑŒÐºÐ¸Ð¹': (49.9640, 33.6121), 'Ð¼Ð¸Ñ€Ð³Ð¾Ñ€Ð¾Ð´ÑÐºÐ¸Ð¹': (49.9640, 33.6121)
    , 'Ð±ÐµÑ€Ð¸ÑÐ»Ð°Ð²ÑÑŒÐºÐ¸Ð¹': (46.8367, 33.4281), 'Ð±ÐµÑ€Ð¸ÑÐ»Ð°Ð²ÑÐºÐ¸Ð¹': (46.8367, 33.4281)
    # Added batch (air alarm coverage) â€” approximate district administrative centers
    , 'ÑˆÐµÐ¿ÐµÑ‚Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (50.1822, 27.0637), 'ÑˆÐµÐ¿ÐµÑ‚Ð¾Ð²ÑÐºÐ¸Ð¹': (50.1822, 27.0637)  # Shepetivka
    , 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ¸Ð¹': (49.5883, 34.5514), 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÐºÐ¸Ð¹': (49.5883, 34.5514)    # Poltava (raion)
    , 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (49.4229, 26.9871), 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÐºÐ¸Ð¹': (49.4229, 26.9871)  # Khmelnytskyi raion (city)
    , 'ÐºÑƒÐ¿\'ÑÐ½ÑÑŒÐºÐ¸Ð¹': (49.7106, 37.6156), 'ÐºÑƒÐ¿ÑÐ½ÑÑŒÐºÐ¸Ð¹': (49.7106, 37.6156)   # Kupiansk raion (Kharkiv oblast)
    , 'Ñ€Ð¾Ð¼ÐµÐ½ÑÑŒÐºÐ¸Ð¹': (50.7515, 33.4746), 'Ñ€Ð¾Ð¼ÐµÐ½ÑÐºÐ¸Ð¹': (50.7515, 33.4746)      # Romny
    , 'Ð¾Ñ…Ñ‚Ð¸Ñ€ÑÑŒÐºÐ¸Ð¹': (50.3103, 34.8988), 'Ð°Ñ…Ñ‚Ñ‹Ñ€ÑÐºÐ¸Ð¹': (50.3103, 34.8988)      # Okhtyrka translit variant
    , 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': (49.9935, 36.2304), 'Ñ…Ð°Ñ€ÑŒÐºÐ¾Ð²ÑÐºÐ¸Ð¹': (49.9935, 36.2304)   # ensure duplication above
    , 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (48.3833, 30.4500), 'Ð³Ð¾Ð»Ð¾Ð²Ð°Ð½ÐµÐ²ÑÐºÐ¸Ð¹': (48.3833, 30.4500) # Holovanivsk
    , 'Ð»ÑƒÐ±ÐµÐ½ÑÑŒÐºÐ¸Ð¹': (50.0165, 32.9969), 'Ð»ÑƒÐ±ÐµÐ½ÑÐºÐ¸Ð¹': (50.0165, 32.9969)      # Lubny
    , 'ÑˆÐ¾ÑÑ‚ÐºÐ¸Ð½ÑÑŒÐºÐ¸Ð¹': (51.8736, 33.4806), 'ÑˆÐ¾ÑÑ‚ÐºÐ¸Ð½ÑÐºÐ¸Ð¹': (51.8736, 33.4806)  # Shostka
    , 'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÑ†ÑŒÐºÐ¸Ð¹': (49.0631, 33.4030), 'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐ³ÑÐºÐ¸Ð¹': (49.0631, 33.4030) # Kremenchuk
    , "ÐºÐ°Ð¼'ÑÐ½ÐµÑ†ÑŒ-Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¸Ð¹": (48.6845, 26.5853), 'ÐºÐ°Ð¼ÑÐ½ÐµÑ†-Ð¿Ð¾Ð´Ð¾Ð»ÑŒÑÐºÐ¸Ð¹': (48.6845, 26.5853)
    , 'Ð±Ð¾Ð³Ð¾Ð´ÑƒÑ…Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (50.1643, 35.5272), 'Ð±Ð¾Ð³Ð¾Ð´ÑƒÑ…Ð¾Ð²ÑÐºÐ¸Ð¹': (50.1643, 35.5272) # Bohodukhiv
    , 'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹': (48.5079, 32.2623), 'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÐºÐ¸Ð¹': (48.5079, 32.2623)   # Kropyvnytskyi raion center
    , 'ÑÐ°Ñ€Ð½ÐµÐ½ÑÑŒÐºÐ¸Ð¹': (51.3373, 26.6019), 'ÑÐ°Ñ€Ð½ÐµÐ½ÑÐºÐ¸Ð¹': (51.3373, 26.6019)       # Sarny
    , 'Ð»Ð¾Ð·Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (48.8926, 36.3172), 'Ð»Ð¾Ð·Ð¾Ð²ÑÐºÐ¸Ð¹': (48.8926, 36.3172)         # Lozova
    , 'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹': (48.3174, 31.5167), 'Ð½Ð¾Ð²Ð¾ÑƒÐºÑ€Ð°Ð¸Ð½ÑÐºÐ¸Ð¹': (48.3174, 31.5167) # Novoukrainka
    , 'Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ð¹ÑÑŒÐºÐ¸Ð¹': (48.6696, 33.1176), 'Ð°Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ð¸Ð¹ÑÐºÐ¸Ð¹': (48.6696, 33.1176) # Oleksandriia
    , 'Ð±ÐµÑ€ÐµÐ·Ñ–Ð²ÑÑŒÐºÐ¸Ð¹': (46.8183, 31.3972), 'Ð±ÐµÑ€ÐµÐ·Ð¾Ð²ÑÐºÐ¸Ð¹': (46.8183, 31.3972) # Berezivka (Odesa Oblast)
    , 'Ð¾Ñ…Ñ‚Ð¸Ñ€ÑÐºÐ¸Ð¹': (50.3103, 34.8988)  # Russian variant explicit
    # Potential typo in feed: 'Ð±ÐµÑ€ÐµÑÑ‚Ð¸Ð½ÑÑŒÐºÐ¸Ð¹' (if meant 'Ð‘ÐµÑ€Ð¸ÑÐ»Ð°Ð²ÑÑŒÐºÐ¸Ð¹' already covered). Placeholder guess -> skip precise to avoid misplot.
}

# Known external launch / airfield / training ground coordinates for Shahed (and similar) launch detection
# Keys are normalized (lowercase, hyphen instead of spaces). Approximate coordinates.
LAUNCH_SITES = {
    'Ð½Ð°Ð²Ð»Ñ': (52.8300, 34.4900),              # Navlya (Bryansk Oblast training area approx)
    'Ð¿Ð¾Ð»Ñ–Ð³Ð¾Ð½ Ð½Ð°Ð²Ð»Ñ': (52.8300, 34.4900),
    'Ð¿Ð¾Ð»Ð¸Ð³Ð¾Ð½ Ð½Ð°Ð²Ð»Ñ': (52.8300, 34.4900),
    'ÑˆÐ°Ñ‚Ð°Ð»Ð¾Ð²Ð¾': (54.0500, 32.2900),            # Shatalovo (Smolensk Oblast)
    'Ð¾Ñ€ÐµÐ»-Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹': (52.9340, 36.0020),      # Orel South (Oryol Yuzhny)
    'Ð¾Ñ€Ñ‘Ð»-ÑŽÐ¶Ð½Ñ‹Ð¹': (52.9340, 36.0020),
    'Ð¾Ñ€ÐµÐ»-ÑŽÐ¶Ð½Ñ‹Ð¹': (52.9340, 36.0020),
    'Ð¾Ñ€Ñ‘Ð» ÑŽÐ¶Ð½Ñ‹Ð¹': (52.9340, 36.0020),
    'Ð¾Ñ€ÐµÐ» ÑŽÐ¶Ð½Ñ‹Ð¹': (52.9340, 36.0020),
    'Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÑŒÐº-Ð°Ñ…Ñ‚Ð°Ñ€ÑÑŒÐº': (46.0420, 38.1700),  # Primorsko-Akhtarsk (Krasnodar Krai)
    'Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÐº-Ð°Ñ…Ñ‚Ð°Ñ€ÑÐº': (46.0420, 38.1700),
    'Ñ…Ð°Ð»Ñ–Ð½Ð¾': (51.7500, 36.2950),              # Khalino (Kursk)
    'Ñ…Ð°Ð»Ð¸Ð½Ð¾': (51.7500, 36.2950),
    'Ð¼Ñ–Ð»Ð»ÐµÑ€Ð¾Ð²Ð¾': (48.9250, 40.4000),           # Millerovo (Rostov Oblast) approximate airbase
    'Ð¼Ð¸Ð»Ð»ÐµÑ€Ð¾Ð²Ð¾': (48.9250, 40.4000),
    # Newly added occupied launch / training areas
    'Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÑŒÐº': (46.7306, 36.3456),           # Prymorsk (Zaporizhzhia oblast, occupied coastal area)
    'Ð¿Ð¾Ð»Ñ–Ð³Ð¾Ð½ Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÑŒÐº': (46.7306, 36.3456),
    'Ð¿Ð¾Ð»Ð¸Ð³Ð¾Ð½ Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÐº': (46.7306, 36.3456),
    'Ñ‡Ð°ÑƒÐ´Ð°': (45.0710, 36.1320),               # Chauda range (Crimea)
    'Ð¿Ð¾Ð»Ñ–Ð³Ð¾Ð½ Ñ‡Ð°ÑƒÐ´Ð°': (45.0710, 36.1320),
    'Ð¿Ð¾Ð»Ð¸Ð³Ð¾Ð½ Ñ‡Ð°ÑƒÐ´Ð°': (45.0710, 36.1320),
}

# Active raion (district) air alarms: raion_base -> dict(place, lat, lng, since)
RAION_ALARMS = {}

# Territorial hromada fallback centers (selected). Keys lower-case without word 'Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ð°Ð»ÑŒÐ½Ð° Ð³Ñ€Ð¾Ð¼Ð°Ð´Ð°'.
HROMADA_FALLBACK = {
    'Ñ…Ð¾Ñ‚Ñ–Ð½ÑÑŒÐºÐ°': (51.0825, 34.5860),  # Ð¥Ð¾Ñ‚Ñ–Ð½ÑÑŒÐºÐ° Ð³Ñ€Ð¾Ð¼Ð°Ð´Ð° (approx center, Sumy raion near border)
    'Ñ…Ð¾Ñ‚Ñ–Ð½ÑŒ': (51.0825, 34.5860),  # Ñ. Ð¥Ð¾Ñ‚Ñ–Ð½ÑŒ (explicit to avoid fallback to Ð¡ÑƒÐ¼Ð¸ center)
}

# Specific settlement fallback for mis-localized parsing
SETTLEMENT_FALLBACK = {
    'ÐºÐ¸Ð¿Ñ‚Ð¸': (51.2833, 31.2167),  # Russian / simplified spelling â†’ 'ÐºÑ–Ð¿Ñ‚Ñ–'
}

SETTLEMENTS_FILE = os.getenv('SETTLEMENTS_FILE', 'settlements_ua.json')
SETTLEMENTS_URL = os.getenv('SETTLEMENTS_URL')  # optional remote JSON (list of {name,lat,lng})
SETTLEMENTS_MAX = int(os.getenv('SETTLEMENTS_MAX', '150000'))  # safety cap
SETTLEMENTS_INDEX = {}
SETTLEMENTS_ORDERED = []

# --------------- Optional Git auto-commit settings ---------------
GIT_AUTO_COMMIT = os.getenv('GIT_AUTO_COMMIT', '0') not in ('0','false','False','')
GIT_REPO_SLUG = os.getenv('GIT_REPO_SLUG')  # e.g. 'vavaika22423232/neptun'

# ----------- Ukrainian place name normalization (force Ukrainian display) -----------
EN_UA_PLACE_MAP = {k.lower(): v for k,v in [
    ('kyiv','ÐšÐ¸Ñ—Ð²'),('kiev','ÐšÐ¸Ñ—Ð²'),('kharkiv','Ð¥Ð°Ñ€ÐºÑ–Ð²'),('kharkov','Ð¥Ð°Ñ€ÐºÑ–Ð²'),('odesa','ÐžÐ´ÐµÑÐ°'),('odessa','ÐžÐ´ÐµÑÐ°'),
    ('lviv','Ð›ÑŒÐ²Ñ–Ð²'),('dnipro','Ð”Ð½Ñ–Ð¿Ñ€Ð¾'),('zaporizhzhia','Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ'),('zaporizhia','Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ'),('mykolaiv','ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²'),('nikolaev','ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²'),
    ('chernihiv','Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²'),('poltava','ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°'),('sumy','Ð¡ÑƒÐ¼Ð¸'),('kherson','Ð¥ÐµÑ€ÑÐ¾Ð½'),('rivne','Ð Ñ–Ð²Ð½Ðµ'),('ternopil','Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ'),
    ('ivano-frankivsk','Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº'),('chernivtsi','Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–'),('uzhhorod','Ð£Ð¶Ð³Ð¾Ñ€Ð¾Ð´'),('kropyvnytskyi','ÐšÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹'),
    ('kryvyi rih','ÐšÑ€Ð¸Ð²Ð¸Ð¹ Ð Ñ–Ð³'),('kryvyi-rih','ÐšÑ€Ð¸Ð²Ð¸Ð¹ Ð Ñ–Ð³'),('sloviansk','Ð¡Ð»Ð¾Ð²Ê¼ÑÐ½ÑÑŒÐº'),('slavyansk','Ð¡Ð»Ð¾Ð²Ê¼ÑÐ½ÑÑŒÐº'),
    ('bakhmut','Ð‘Ð°Ñ…Ð¼ÑƒÑ‚'),('mariupol','ÐœÐ°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ'),('berdyansk','Ð‘ÐµÑ€Ð´ÑÐ½ÑÑŒÐº'),('melitopol','ÐœÐµÐ»Ñ–Ñ‚Ð¾Ð¿Ð¾Ð»ÑŒ'),
    ('pavlohrad','ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´'),('pavlograd','ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´'),('pokrovsk','ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐº'),('sevastopol','Ð¡ÐµÐ²Ð°ÑÑ‚Ð¾Ð¿Ð¾Ð»ÑŒ'),('simferopol','Ð¡Ñ–Ð¼Ñ„ÐµÑ€Ð¾Ð¿Ð¾Ð»ÑŒ')
]}

def ensure_ua_place(name: str) -> str:
    if not name or not isinstance(name,str):
        return name
    n = name.strip()
    # Already contains Ukrainian-specific letters
    if re.search(r'[Ñ–Ñ—Ñ”Ò‘Ê¼Ð†Ð‡Ð„Ò]', n):
        return n
    low = n.lower()
    if low in EN_UA_PLACE_MAP:
        return EN_UA_PLACE_MAP[low]
    # Basic transliteration fallback for ascii-only names
    if re.fullmatch(r'[a-zA-Z\-\s]+', n):
        s = low
        # multi-char sequences first
        repl = [
            ('shch','Ñ‰'),('sch','Ñ‰'),('kh','Ñ…'),('ch','Ñ‡'),('sh','Ñˆ'),('ya','Ñ'),('yu','ÑŽ'),('ye','Ñ”'),('yi','Ñ—'),('zh','Ð¶'),('ii','Ñ–Ñ—'),
            ('ie','Ñ”'),('jo','Ð¹Ð¾'),('yo','Ð¹Ð¾')
        ]
        for a,b in repl:
            s = re.sub(a,b,s)
        single = {
            'a':'Ð°','b':'Ð±','c':'Ðº','d':'Ð´','e':'Ðµ','f':'Ñ„','g':'Ð³','h':'Ð³','i':'Ñ–','j':'Ð¹','k':'Ðº','l':'Ð»','m':'Ð¼','n':'Ð½','o':'Ð¾','p':'Ð¿',
            'q':'Ðº','r':'Ñ€','s':'Ñ','t':'Ñ‚','u':'Ñƒ','v':'Ð²','w':'Ð²','x':'ÐºÑ','y':'Ð¸','z':'Ð·','Ê¼':'Ê¼','-':'-',' ':' '
        }
        out = ''.join(single.get(ch,ch) for ch in s)
        # Capitalize first letter and letters after dash/space
        def cap_tokens(txt):
            # Use raw regex to avoid invalid escape sequence warning for \s
            parts = re.split(r'([-\s])', txt)
            return ''.join(p.capitalize() if i%2==0 else p for i,p in enumerate(parts))
        return cap_tokens(out)
    return n

# -------- Persistent visit tracking (SQLite) to survive redeploys --------
VISITS_DB = os.getenv('VISITS_DB','visits.db')
_SQLITE_PRAGMAS = [
    "PRAGMA journal_mode=WAL;",
    "PRAGMA synchronous=NORMAL;",
    "PRAGMA foreign_keys=ON;"
]
def _visits_db_conn():
    conn = sqlite3.connect(VISITS_DB, timeout=5, check_same_thread=False)
    try:
        # Apply pragmas every time (cheap) to ensure durability/performance settings even after restart
        for p in _SQLITE_PRAGMAS:
            try:
                conn.execute(p)
            except Exception:
                pass
    except Exception:
        pass
    return conn

_RECENT_SEEDED = False
def _seed_recent_from_sql():
    """If rolling recent visits file missing/outdated or lost after redeploy, rebuild from SQLite so
    Day / Week counts remain stable across deployments."""
    global _RECENT_SEEDED
    if _RECENT_SEEDED:
        return
    try:
        data = _load_recent_visits() or {}
        tz = pytz.timezone('Europe/Kyiv')
        now_dt = datetime.now(tz)
        today_str = now_dt.strftime('%Y-%m-%d')
        week_cut = now_dt - timedelta(days=7)
        today_start = tz.localize(datetime.strptime(today_str, '%Y-%m-%d')).timestamp()
        week_start_ts = week_cut.timestamp()
        with _visits_db_conn() as conn:
            cur_day = conn.execute("SELECT id FROM visits WHERE last_seen >= ?", (today_start,))
            day_ids = [r[0] for r in cur_day.fetchall()]
            cur_week = conn.execute("SELECT id FROM visits WHERE last_seen >= ?", (week_start_ts,))
            week_ids = [r[0] for r in cur_week.fetchall()]
        need_seed = False
        # Conditions to trigger seeding: empty/missing file, day mismatch, or counts smaller than SQL (lost state)
        if not data:
            need_seed = True
        else:
            if data.get('day') != today_str:
                need_seed = True
            elif len(set(data.get('today_ids', []))) < len(day_ids):
                need_seed = True
            elif len(set(data.get('week_ids', []))) < len(week_ids):
                need_seed = True
        if need_seed:
            data = {
                'day': today_str,
                'today_ids': list(dict.fromkeys(day_ids)),  # preserve order unique
                'week_ids': list(dict.fromkeys(week_ids)),
                'week_start': week_cut.strftime('%Y-%m-%d')  # informational; rolling window logic tolerates
            }
            _save_recent_visits(data)
            log.info(f"recent visits seeded from SQL: day={len(day_ids)} week={len(week_ids)}")
        _RECENT_SEEDED = True
    except Exception as e:
        log.warning(f"recent visits seeding failed: {e}")

def init_visits_db():
    try:
        with _visits_db_conn() as conn:
            conn.execute("CREATE TABLE IF NOT EXISTS visits (id TEXT PRIMARY KEY, first_seen REAL, last_seen REAL)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_visits_first ON visits(first_seen)")
            # Helpful for fast lookups of currently active users by recent activity window
            conn.execute("CREATE INDEX IF NOT EXISTS idx_visits_last ON visits(last_seen)")
    except Exception as e:
        log.warning(f"visits db init failed: {e}")

# --------------- Persistent comments (SQLite) ---------------
def init_comments_db():
    """Create comments table if missing. Uses same SQLite DB as visits for simplicity."""
    try:
        with _visits_db_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS comments (
                    id TEXT PRIMARY KEY,
                    text TEXT,
                    ts   TEXT,
                    epoch REAL
                )
            """)
            # Enhanced reactions table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS comment_reactions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    comment_id TEXT NOT NULL,
                    emoji TEXT NOT NULL,
                    user_ip TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    UNIQUE(comment_id, emoji, user_ip)
                )
            """)
            # Migration: ensure reply_to column exists
            cur = conn.execute("PRAGMA table_info(comments)")
            cols = [r[1] for r in cur.fetchall()]
            if 'reply_to' not in cols:
                try:
                    conn.execute("ALTER TABLE comments ADD COLUMN reply_to TEXT")
                    log.info('comments table migrated: added reply_to column')
                except Exception as me:
                    log.warning(f'failed adding reply_to column: {me}')
            # Create indexes (individually wrapped)
            for idx_sql in [
                "CREATE INDEX IF NOT EXISTS idx_comments_epoch ON comments(epoch)",
                "CREATE INDEX IF NOT EXISTS idx_comments_reply ON comments(reply_to)",
                "CREATE INDEX IF NOT EXISTS idx_reactions_comment ON comment_reactions(comment_id)",
                "CREATE INDEX IF NOT EXISTS idx_reactions_user ON comment_reactions(user_ip)"
            ]:
                try:
                    conn.execute(idx_sql)
                except Exception as ie:
                    log.debug(f'index create skipped: {ie}')
    except Exception as e:
        log.warning(f"comments db init failed: {e}")

def save_comment_record(item:dict):
    try:
        with _visits_db_conn() as conn:
            conn.execute("INSERT OR REPLACE INTO comments (id,text,ts,epoch,reply_to) VALUES (?,?,?,?,?)",
                         (item.get('id'), item.get('text'), item.get('ts'), item.get('epoch'), item.get('reply_to')))
    except Exception as e:
        log.warning(f"save_comment_record failed: {e}")

def load_recent_comments(limit:int=80)->list[dict]:
    rows = []
    try:
        with _visits_db_conn() as conn:
            try:
                cur = conn.execute("SELECT id,text,ts,reply_to FROM comments ORDER BY epoch DESC LIMIT ?", (limit,))
                fetched = cur.fetchall()
            except Exception as sel_err:
                # Fallback legacy schema (no reply_to); try to migrate then retry
                log.warning(f'comments select fallback (legacy schema): {sel_err}')
                try:
                    conn.execute("ALTER TABLE comments ADD COLUMN reply_to TEXT")
                    cur = conn.execute("SELECT id,text,ts,reply_to FROM comments ORDER BY epoch DESC LIMIT ?", (limit,))
                    fetched = cur.fetchall()
                except Exception as mig_err:
                    log.warning(f'comments migration select failed: {mig_err}')
                    # Last resort: select without reply_to
                    try:
                        cur = conn.execute("SELECT id,text,ts FROM comments ORDER BY epoch DESC LIMIT ?", (limit,))
                        fetched = [(*r, None) for r in cur.fetchall()]
                    except Exception:
                        fetched = []
            for rid, text, ts, reply_to in fetched:
                d={'id': rid, 'text': text, 'ts': ts}
                if reply_to: d['reply_to']=reply_to
                
                # Load reactions for this comment
                try:
                    reactions = load_comment_reactions(rid, conn)
                    if reactions:
                        d['reactions'] = reactions
                except Exception:
                    pass  # Non-critical, skip reactions if failed
                
                rows.append(d)
    except Exception as e:
        log.warning(f"load_recent_comments failed: {e}")
    return list(reversed(rows))  # reverse so oldest of the slice first

def load_comment_reactions(comment_id: str, conn=None) -> dict:
    """Load reaction counts for a specific comment."""
    try:
        if conn:
            cur = conn.execute("""
                SELECT emoji, COUNT(*) as count 
                FROM comment_reactions 
                WHERE comment_id = ? 
                GROUP BY emoji
            """, (comment_id,))
            
            reactions = {}
            for emoji, count in cur.fetchall():
                reactions[emoji] = count
            return reactions
        else:
            with _visits_db_conn() as use_conn:
                cur = use_conn.execute("""
                    SELECT emoji, COUNT(*) as count 
                    FROM comment_reactions 
                    WHERE comment_id = ? 
                    GROUP BY emoji
                """, (comment_id,))
                
                reactions = {}
                for emoji, count in cur.fetchall():
                    reactions[emoji] = count
                return reactions
    except Exception as e:
        log.debug(f"load_comment_reactions failed: {e}")
        return {}

def toggle_comment_reaction(comment_id: str, emoji: str, user_ip: str) -> dict:
    """Toggle a reaction on a comment. Returns updated reaction counts."""
    try:
        with _visits_db_conn() as conn:
            # Check if reaction already exists
            cur = conn.execute("""
                SELECT id FROM comment_reactions 
                WHERE comment_id = ? AND emoji = ? AND user_ip = ?
            """, (comment_id, emoji, user_ip))
            
            existing = cur.fetchone()
            
            if existing:
                # Remove existing reaction
                conn.execute("DELETE FROM comment_reactions WHERE id = ?", (existing[0],))
                action = 'removed'
            else:
                # Add new reaction
                conn.execute("""
                    INSERT INTO comment_reactions (comment_id, emoji, user_ip, timestamp)
                    VALUES (?, ?, ?, ?)
                """, (comment_id, emoji, user_ip, time.time()))
                action = 'added'
            
            conn.commit()
            
            # Return updated counts
            reactions = load_comment_reactions(comment_id, conn)
            return {'action': action, 'reactions': reactions}
            
    except Exception as e:
        log.warning(f"toggle_comment_reaction failed: {e}")
        return {'action': 'error', 'reactions': {}}

def load_comment_reactions(comment_id: str, conn=None) -> dict:
    """Load reaction counts for a specific comment."""
    try:
        use_conn = conn
        if not use_conn:
            use_conn = _visits_db_conn()
            
        with use_conn as c:
            cur = c.execute("""
                SELECT emoji, COUNT(*) as count 
                FROM comment_reactions 
                WHERE comment_id = ? 
                GROUP BY emoji
            """, (comment_id,))
            
            reactions = {}
            for emoji, count in cur.fetchall():
                reactions[emoji] = count
            return reactions
    except Exception as e:
        log.debug(f"load_comment_reactions failed: {e}")
        return {}

def toggle_comment_reaction(comment_id: str, emoji: str, user_ip: str) -> dict:
    """Toggle a reaction on a comment. Returns updated reaction counts."""
    try:
        with _visits_db_conn() as conn:
            # Check if reaction already exists
            cur = conn.execute("""
                SELECT id FROM comment_reactions 
                WHERE comment_id = ? AND emoji = ? AND user_ip = ?
            """, (comment_id, emoji, user_ip))
            
            existing = cur.fetchone()
            
            if existing:
                # Remove existing reaction
                conn.execute("DELETE FROM comment_reactions WHERE id = ?", (existing[0],))
                action = 'removed'
            else:
                # Add new reaction
                conn.execute("""
                    INSERT INTO comment_reactions (comment_id, emoji, user_ip, timestamp)
                    VALUES (?, ?, ?, ?)
                """, (comment_id, emoji, user_ip, time.time()))
                action = 'added'
            
            conn.commit()
            
            # Return updated counts
            reactions = load_comment_reactions(comment_id, conn)
            return {'action': action, 'reactions': reactions}
            
    except Exception as e:
        log.warning(f"toggle_comment_reaction failed: {e}")
        return {'action': 'error', 'reactions': {}}

def record_visit_sql(id_:str, now_ts:float):
    if not id_:
        return
    try:
        with _visits_db_conn() as conn:
            # Use upsert pattern to avoid race between SELECT and INSERT under concurrent requests
            conn.execute("INSERT OR IGNORE INTO visits (id,first_seen,last_seen) VALUES (?,?,?)", (id_, now_ts, now_ts))
            conn.execute("UPDATE visits SET last_seen=? WHERE id=?", (now_ts, id_))
    except Exception as e:
        log.warning(f"record_visit_sql failed: {e}")

def sql_unique_counts():
    try:
        with _visits_db_conn() as conn:
            tz = pytz.timezone('Europe/Kyiv')
            now_dt = datetime.now(tz)
            today_start = tz.localize(datetime.strptime(now_dt.strftime('%Y-%m-%d'), '%Y-%m-%d'))
            week_start = now_dt - timedelta(days=7)
            today_ts = today_start.timestamp()
            week_ts = week_start.timestamp()
            cur1 = conn.execute("SELECT COUNT(*) FROM visits WHERE last_seen >= ?", (today_ts,))
            day = cur1.fetchone()[0]
            cur2 = conn.execute("SELECT COUNT(*) FROM visits WHERE last_seen >= ?", (week_ts,))
            week = cur2.fetchone()[0]
            return day, week
    except Exception as e:
        log.warning(f"sql_unique_counts failed: {e}")
    return None, None

def _active_sessions_from_db(ttl:int)->list[dict]:
    """Return list of active sessions (id, first_seen, last_seen) from persistent DB within ttl seconds."""
    cutoff = time.time() - ttl
    out = []
    try:
        with _visits_db_conn() as conn:
            cur = conn.execute("SELECT id, first_seen, last_seen FROM visits WHERE last_seen >= ?", (cutoff,))
            for row in cur.fetchall():
                try:
                    out.append({'id': row[0], 'first': float(row[1] or 0), 'last': float(row[2] or 0)})
                except Exception:
                    continue
    except Exception as e:
        log.warning(f"active sessions db query failed: {e}")
    return out

# Initialize DB at import
init_visits_db()
init_comments_db()
init_alarms_db()
init_alarm_events_db()
# Restore persisted active alarms
try:
    _obl,_r = load_active_alarms(APP_ALARM_TTL_MINUTES*60)
    if _obl: ACTIVE_OBLAST_ALARMS.update(_obl)
    if _r: ACTIVE_RAION_ALARMS.update(_r)
except Exception as _e_rec:
    log.debug(f'alarm restore failed: {_e_rec}')
# Preload recent comments into in-memory cache so first GET can serve quickly without hitting DB again
try:
    COMMENTS = load_recent_comments(limit=COMMENTS_MAX)
except Exception as _e:
    log.debug(f'preload comments failed: {_e}')
GIT_SYNC_TOKEN = os.getenv('GIT_SYNC_TOKEN')  # GitHub PAT (classic or fine-grained) with repo write
GIT_COMMIT_INTERVAL = int(os.getenv('GIT_COMMIT_INTERVAL', '180'))  # seconds between commits
_last_git_commit = 0

# Delay before first Telegram connect (helps Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÑÐµÑ‡ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ€Ð¾Ð³Ð¾ Ð¸ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ° Ð¿Ñ€Ð¸ Ð´ÐµÐ¿Ð»Ð¾Ðµ)
FETCH_START_DELAY = int(os.getenv('FETCH_START_DELAY', '0'))  # seconds

def maybe_git_autocommit():
    """If enabled, commit & push updated messages.json back to GitHub.
    Requirements:
      - Set GIT_AUTO_COMMIT=1
      - Provide GIT_REPO_SLUG (owner/repo)
      - Provide GIT_SYNC_TOKEN (PAT with repo write)
    The container build must include git (Render base images do).
    Commits throttled by GIT_COMMIT_INTERVAL seconds.
    """
    global _last_git_commit
    if not GIT_AUTO_COMMIT or not GIT_REPO_SLUG or not GIT_SYNC_TOKEN:
        return
    now = time.time()
    if now - _last_git_commit < GIT_COMMIT_INTERVAL:
        return
    if not os.path.isdir('.git'):
        raise RuntimeError('Not a git repo')
    # Configure user (once)
    def run(cmd):
        return subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    run('git config user.email "bot@local"')
    run('git config user.name "Auto Sync Bot"')
    # Set remote URL embedding token (avoid logging token!)
    safe_remote = f'https://x-access-token:{GIT_SYNC_TOKEN}@github.com/{GIT_REPO_SLUG}.git'
    # Do not print safe_remote (contains secret)
    # Update origin only if needed
    remotes = run('git remote -v').stdout
    if 'origin' not in remotes or GIT_REPO_SLUG not in remotes:
        run('git remote remove origin')
        run(f'git remote add origin "{safe_remote}"')
    # Stage & commit if there is a change
    run(f'git add {MESSAGES_FILE}')
    status = run('git status --porcelain').stdout
    if MESSAGES_FILE not in status:
        return  # no actual diff
    commit_msg = f'Update {MESSAGES_FILE} (auto)'  # no secrets
    run(f'git commit -m "{commit_msg}"')
    push_res = run('git push origin HEAD:main')
    if push_res.returncode == 0:
        _last_git_commit = now
    else:
        # If push fails (e.g., diverged), attempt pull+rebase then push
        run('git fetch origin')
        run('git rebase origin/main || git rebase --abort')
        push_res2 = run('git push origin HEAD:main')
        if push_res2.returncode == 0:
            _last_git_commit = now
        # else: give up silently to avoid spamming logs

def _download_settlements():
    if not SETTLEMENTS_URL or os.path.exists(SETTLEMENTS_FILE):
        return False
    try:
        import requests
        r = requests.get(SETTLEMENTS_URL, timeout=30)
        if r.status_code == 200:
            with open(SETTLEMENTS_FILE, 'wb') as f:
                f.write(r.content)
            log.info(f'Downloaded settlements file from {SETTLEMENTS_URL}')
            return True
        else:
            log.warning(f'Failed to download settlements ({r.status_code}) from {SETTLEMENTS_URL}')
    except Exception as e:
        log.warning(f'Error downloading settlements: {e}')
    return False

def _load_settlements():
    global SETTLEMENTS_INDEX, SETTLEMENTS_ORDERED
    if not os.path.exists(SETTLEMENTS_FILE):
        _download_settlements()
    if not os.path.exists(SETTLEMENTS_FILE):
        log.info('No settlements file present; only basic CITY_COORDS will be used.')
        return
    try:
        with open(SETTLEMENTS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        count = 0
        for item in data:
            if count >= SETTLEMENTS_MAX:
                break
            try:
                name = item.get('name') or item.get('n')
                if not name:
                    continue
                lat_raw = item.get('lat')
                lng_raw = item.get('lng') or item.get('lon')
                if lat_raw is None or lng_raw is None:
                    continue
                lat = float(lat_raw)
                lng = float(lng_raw)
                key = name.strip().lower()
                if key and key not in SETTLEMENTS_INDEX:
                    SETTLEMENTS_INDEX[key] = (lat, lng)
                    count += 1
            except Exception:
                continue
        SETTLEMENTS_ORDERED = sorted(SETTLEMENTS_INDEX.keys(), key=len, reverse=True)[:SETTLEMENTS_MAX]
        log.info(f'Loaded settlements: {len(SETTLEMENTS_INDEX)} (cap {SETTLEMENTS_MAX})')
    except Exception as e:
        log.warning(f'Failed to load settlements file {SETTLEMENTS_FILE}: {e}')

_load_settlements()

# ---- External comprehensive cities/settlements file merge (user-provided) ----
# You supplied an external file with full coordinates of Ukrainian cities / settlements.
# Set EXT_CITIES_FILE env var (default 'city_ukraine.json') and place the file in the app working directory.
# Accepted JSON shapes:
#   1) List[ { name|city|settlement: str, lat|latitude: float, lng|lon|long|longitude: float } ]
#   2) List[ [ name, lat, lon ] ]
#   3) Dict[str, { lat: x, lng: y }] or Dict[str, [lat, lon]]
# Fields may also appear in Ukrainian/Russian ("Ð½Ð°Ð·Ð²Ð°","ÑˆÐ¸Ñ€Ð¾Ñ‚Ð°","Ð´Ð¾Ð²Ð³Ð¾Ñ‚Ð°","Ð´Ð¾Ð»Ð³Ð¾Ñ‚Ð°").
EXT_CITIES_FILE = os.getenv('EXT_CITIES_FILE', 'city_ukraine.json')

def _load_external_cities():
    global CITY_COORDS, SETTLEMENTS_INDEX, SETTLEMENTS_ORDERED
    path = EXT_CITIES_FILE
    if not path or not os.path.exists(path):
        return
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        log.warning(f"Failed reading {path}: {e}")
        return
    added = 0
    def add_entry(name_raw, lat_raw, lon_raw):
        nonlocal added
        try:
            if name_raw is None: return
            name = str(name_raw).strip().lower()
            if not name or len(name) < 2: return
            lat = float(lat_raw); lon = float(lon_raw)
            # Basic sanity bounds for Ukraine region (approx) to skip corrupt rows
            if not (43.0 <= lat <= 53.5 and 21.0 <= lon <= 41.5):
                return
            if name not in CITY_COORDS:
                CITY_COORDS[name] = (lat, lon)
            if name not in SETTLEMENTS_INDEX:
                SETTLEMENTS_INDEX[name] = (lat, lon)
                added += 1
        except Exception:
            return
    if isinstance(data, dict):
        # Expect mapping name -> {lat,lng} or name -> [lat,lon]
        for k,v in data.items():
            if isinstance(v, dict):
                lat = v.get('lat') or v.get('latitude') or v.get('ÑˆÐ¸Ñ€Ð¾Ñ‚Ð°')
                lon = v.get('lng') or v.get('lon') or v.get('long') or v.get('longitude') or v.get('Ð´Ð¾Ð²Ð³Ð¾Ñ‚Ð°') or v.get('Ð´Ð¾Ð»Ð³Ð¾Ñ‚Ð°')
                add_entry(k, lat, lon)
            elif isinstance(v, (list, tuple)) and len(v) >= 2:
                add_entry(k, v[0], v[1])
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                name = item.get('name') or item.get('city') or item.get('settlement') or item.get('Ð½Ð°ÑÐµÐ»ÐµÐ½Ð½Ñ‹Ð¹ Ð¿ÑƒÐ½ÐºÑ‚') or item.get('Ð½Ð°ÑÐµÐ»ÐµÐ½Ð¸Ð¹ Ð¿ÑƒÐ½ÐºÑ‚') or item.get('Ð½Ð°Ð·Ð²Ð°')
                lat = item.get('lat') or item.get('latitude') or item.get('ÑˆÐ¸Ñ€Ð¾Ñ‚Ð°')
                lon = item.get('lng') or item.get('lon') or item.get('long') or item.get('longitude') or item.get('Ð´Ð¾Ð²Ð³Ð¾Ñ‚Ð°') or item.get('Ð´Ð¾Ð»Ð³Ð¾Ñ‚Ð°')
                add_entry(name, lat, lon)
            elif isinstance(item, (list, tuple)) and len(item) >= 3:
                add_entry(item[0], item[1], item[2])
    # Rebuild ordered list (largest names first to prefer longer multi-word matches)
    if added:
        try:
            SETTLEMENTS_ORDERED = sorted(SETTLEMENTS_INDEX.keys(), key=len, reverse=True)[:SETTLEMENTS_MAX]
        except Exception:
            pass
        log.info(f"Merged external cities file {path}: +{added} settlements (total {len(SETTLEMENTS_INDEX)})")
    else:
        log.info(f"External cities file {path} parsed; no new settlements added (maybe already present)")

_load_external_cities()

def geocode_opencage(place: str):
    if not OPENCAGE_API_KEY:
        return None
    
    # Skip if known negative
    if neg_geocode_check(place):
        return None
    
    # Block general directional terms that don't represent specific places
    place_lower = place.lower().strip()
    directional_terms = [
        'Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº', 'Ð½Ð°Ð¿Ñ€ÑÐ¼', 'Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸', 'Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ',
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹',
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹', 'ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹', 'Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹',
        'nord', 'south', 'east', 'west', 'northeast', 'northwest', 'southeast', 'southwest'
    ]
    
    if any(term in place_lower for term in directional_terms):
        # Add to negative cache to avoid repeated attempts
        neg_geocode_add(place, 'directional')
        return None
    
    cache = _load_opencage_cache()
    key = place.strip().lower()
    now = int(datetime.utcnow().timestamp())
    if key in cache:
        entry = cache[key]
        if now - entry.get('ts', 0) < OPENCAGE_TTL:
            return tuple(entry['coords']) if entry['coords'] else None
    import requests
    try:
        resp = requests.get('https://api.opencagedata.com/geocode/v1/json', params={
            'q': place,
            'key': OPENCAGE_API_KEY,
            'language': 'uk',
            'limit': 1,
            'countrycode': 'ua'
        }, timeout=6)
        if resp.status_code == 200:
            data = resp.json()
            if data.get('results'):
                g = data['results'][0]['geometry']
                coords = (g['lat'], g['lng'])
                cache[key] = {'ts': now, 'coords': coords}
                _save_opencage_cache()
                return coords
        # negative (no results or non-200)
        cache[key] = {'ts': now, 'coords': None}
        _save_opencage_cache(); neg_geocode_add(place,'nocode')
        return None
    except Exception as e:
        log.warning(f"OpenCage error for '{place}': {e}")
        cache[key] = {'ts': now, 'coords': None}
        _save_opencage_cache(); neg_geocode_add(place,'error')
        return None

def calculate_projected_path(source_lat, source_lng, target_lat, target_lng, speed_kmh=50):
    """
    Calculate projected path from source to target with intermediate points
    
    Args:
        source_lat, source_lng: Current/source coordinates
        target_lat, target_lng: Target coordinates  
        speed_kmh: Estimated speed in km/h (default: 50 km/h for UAVs)
    
    Returns:
        dict with path_points, estimated_arrival, total_distance
    """
    try:
        # Calculate distance using Haversine formula
        R = 6371  # Earth's radius in km
        
        lat1_rad = math.radians(source_lat)
        lon1_rad = math.radians(source_lng)
        lat2_rad = math.radians(target_lat)
        lon2_rad = math.radians(target_lng)
        
        dlat = lat2_rad - lat1_rad
        dlon = lon2_rad - lon1_rad
        
        a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
        distance_km = R * c
        
        # Calculate estimated travel time
        travel_time_hours = distance_km / speed_kmh
        travel_time_minutes = travel_time_hours * 60
        
        # Generate intermediate points along the path (every ~10km or 10 points max)
        num_points = min(10, max(2, int(distance_km / 10)))
        path_points = []
        
        for i in range(num_points + 1):
            fraction = i / num_points
            
            # Linear interpolation for simple path
            lat = source_lat + (target_lat - source_lat) * fraction
            lng = source_lng + (target_lng - source_lng) * fraction
            
            # Calculate ETA for this point
            point_travel_time = travel_time_minutes * fraction
            
            path_points.append({
                'lat': lat,
                'lng': lng,
                'eta_minutes': point_travel_time,
                'fraction': fraction
            })
        
        return {
            'path_points': path_points,
            'total_distance_km': distance_km,
            'estimated_arrival_minutes': travel_time_minutes,
            'speed_kmh': speed_kmh
        }
        
    except Exception as e:
        print(f"ERROR calculating projected path: {e}")
        return None

def create_eta_circles(center_lat, center_lng, time_minutes, speed_kmh=50):
    """
    Create ETA circles showing possible positions after given time
    
    Args:
        center_lat, center_lng: Center coordinates
        time_minutes: Time in minutes
        speed_kmh: Speed in km/h
        
    Returns:
        List of circle definitions for different confidence levels
    """
    try:
        # Calculate distance that can be covered in given time
        max_distance_km = (speed_kmh * time_minutes) / 60
        
        # Create circles with different confidence levels
        circles = []
        
        # 90% confidence circle (slightly smaller radius)
        circles.append({
            'center_lat': center_lat,
            'center_lng': center_lng,
            'radius_km': max_distance_km * 0.9,
            'confidence': 90,
            'color': '#ff4444',
            'opacity': 0.3,
            'stroke_color': '#cc0000',
            'stroke_width': 2
        })
        
        # 50% confidence circle (even smaller)
        circles.append({
            'center_lat': center_lat,
            'center_lng': center_lng,
            'radius_km': max_distance_km * 0.6,
            'confidence': 50,
            'color': '#ffaa00',
            'opacity': 0.4,
            'stroke_color': '#ff8800',
            'stroke_width': 2
        })
        
        return circles
        
    except Exception as e:
        print(f"ERROR creating ETA circles: {e}")
        return []

def _create_directional_trajectory_markers(text, mid, date_str, channel):
    """
    Create trajectory markers for directional movement messages
    Instead of showing destination marker, show projected path and ETA circles
    """
    import re
    from datetime import datetime, timedelta
    
    try:
        text_lower = text.lower()
        
        # Extract target city from directional patterns
        target_city = None
        source_direction = None
        
        # Patterns to extract target city
        target_patterns = [
            r'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s|$|Ð·)',
            r'Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s|$|Ð·)',
            r'ÐºÑƒÑ€Ñ Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s|$|Ð·)',
            r'Ð¿Ñ€ÑÐ¼ÑƒÑ” Ð´Ð¾\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s|$|Ð·)'
        ]
        
        for pattern in target_patterns:
            match = re.search(pattern, text_lower)
            if match:
                target_city = match.group(1).strip()
                break
        
        # Extract source direction
        direction_patterns = [
            r'Ð·\s+(Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾?-?ÑÑ…Ð¾Ð´Ñƒ?)',
            r'Ð·\s+(Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾?-?Ð·Ð°Ñ…Ð¾Ð´Ñƒ?)', 
            r'Ð·\s+(Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾?-?Ð·Ð°Ñ…Ð¾Ð´Ñƒ?)',
            r'Ð·\s+(Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾?-?ÑÑ…Ð¾Ð´Ñƒ?)',
            r'Ð·\s+(Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–)',
            r'Ð·\s+(Ð¿Ñ–Ð²Ð´Ð½Ñ)',
            r'Ð·\s+(Ð·Ð°Ñ…Ð¾Ð´Ñƒ)',
            r'Ð·\s+(ÑÑ…Ð¾Ð´Ñƒ)'
        ]
        
        for pattern in direction_patterns:
            match = re.search(pattern, text_lower)
            if match:
                source_direction = match.group(1)
                break
        
        if not target_city:
            return []
        
        # Normalize target city name and get coordinates
        target_city_normalized = target_city.lower().strip()
        
        # Try to find coordinates for target city
        target_coords = None
        
        # Check in CITY_COORDS
        if target_city_normalized in CITY_COORDS:
            target_coords = CITY_COORDS[target_city_normalized]
        else:
            # Try common variations and declensions
            common_variations = {
                'Ð´Ð½Ñ–Ð¿Ñ€Ð¾': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾',
                'ÐºÐ¸ÐµÐ²': 'ÐºÐ¸Ñ—Ð²', 
                'ÐºÐ¸Ñ—Ð²Ð°': 'ÐºÐ¸Ñ—Ð²',
                'Ñ…Ð°Ñ€ÐºÐ¾Ð²': 'Ñ…Ð°Ñ€ÐºÑ–Ð²',
                'Ñ…Ð°Ñ€ÐºÐ¾Ð²Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²',
                'Ð¾Ð´ÐµÑÑÐ°': 'Ð¾Ð´ÐµÑÐ°',
                'Ð¾Ð´ÐµÑÐ¸': 'Ð¾Ð´ÐµÑÐ°'
            }
            
            for variant, canonical in common_variations.items():
                if variant in target_city_normalized or target_city_normalized in variant:
                    if canonical in CITY_COORDS:
                        target_coords = CITY_COORDS[canonical]
                        break
            
            # If still not found, try removing common endings (declensions)
            if not target_coords:
                endings_to_try = ['Ð°', 'Ñƒ', 'Ð¾Ð¼', 'Ñ–', 'Ñ–Ð²', 'Ð°Ð¼Ð¸']
                for ending in endings_to_try:
                    if target_city_normalized.endswith(ending) and len(target_city_normalized) > len(ending) + 2:
                        base_form = target_city_normalized[:-len(ending)]
                        # Special case for ÐºÐ¸Ñ—Ð² + Ð° = ÐºÐ¸Ñ”Ð²Ð° -> ÐºÐ¸Ñ—Ð²  
                        if base_form + ending == 'ÐºÐ¸Ñ”Ð²Ð°':
                            base_form = 'ÐºÐ¸Ñ—Ð²'
                        if base_form in CITY_COORDS:
                            target_coords = CITY_COORDS[base_form]
                            break
        
        if not target_coords:
            # Fallback - return empty if we can't find target coordinates
            return []
        
        target_lat, target_lng = target_coords
        
        # Estimate source coordinates based on direction
        source_lat, source_lng = _estimate_source_coordinates(target_lat, target_lng, source_direction)
        
        # Create projected path
        projected_path = calculate_projected_path(source_lat, source_lng, target_lat, target_lng)
        
        if not projected_path:
            return []
        
        # Create trajectory markers
        markers = []
        
        # Add path markers (intermediate points)
        for i, point in enumerate(projected_path['path_points'][1:-1], 1):  # Skip first and last
            if i % 2 == 0:  # Only show every other point to avoid clutter
                continue
                
            markers.append({
                'id': f"{mid}_path_{i}",
                'place': f"Ð¢Ñ€Ð°Ñ”ÐºÑ‚Ð¾Ñ€Ñ–Ñ ({int(point['eta_minutes'])}Ñ…Ð²)",
                'lat': point['lat'],
                'lng': point['lng'],
                'threat_type': 'trajectory',
                'text': f"ÐŸÑ€Ð¾Ð¼Ñ–Ð¶Ð½Ð° Ñ‚Ð¾Ñ‡ÐºÐ° Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ñƒ Ð´Ð¾ {target_city.title()}",
                'date': date_str,
                'channel': channel,
                'marker_icon': 'trajectory.png',
                'source_match': 'projected_path',
                'eta_minutes': point['eta_minutes'],
                'marker_type': 'trajectory_point',
                'opacity': 0.7
            })
        
        # Add ETA circles around target
        eta_circles = create_eta_circles(target_lat, target_lng, projected_path['estimated_arrival_minutes'])
        
        # Create main target marker with trajectory info
        markers.append({
            'id': f"{mid}_target",
            'place': f"{target_city.title()} (Ñ†Ñ–Ð»ÑŒ)",
            'lat': target_lat,
            'lng': target_lng,
            'threat_type': 'trajectory_target',
            'text': f"Ð¦Ñ–Ð»ÑŒ: {target_city.title()} (ETA: {int(projected_path['estimated_arrival_minutes'])}Ñ…Ð²)",
            'date': date_str,
            'channel': channel,
            'marker_icon': 'target.png',
            'source_match': 'trajectory_target',
            'eta_minutes': projected_path['estimated_arrival_minutes'],
            'distance_km': projected_path['total_distance_km'],
            'marker_type': 'trajectory_target',
            'eta_circles': eta_circles,
            'projected_path': projected_path['path_points']
        })
        
        return markers
        
    except Exception as e:
        print(f"ERROR creating directional trajectory markers: {e}")
        return []

def _estimate_source_coordinates(target_lat, target_lng, direction):
    """Estimate source coordinates based on target and direction"""
    
    # Default distance for estimation (50km)
    distance_km = 50
    
    # Direction offsets (approximate)
    direction_offsets = {
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ': (-0.45, 0.45),
        'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾-Ð·Ð°Ñ…Ð¾Ð´Ñƒ': (0.45, -0.45),
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾-Ð·Ð°Ñ…Ð¾Ð´Ñƒ': (-0.45, -0.45), 
        'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ': (0.45, 0.45),
        'Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–': (-0.45, 0),
        'Ð¿Ñ–Ð²Ð´Ð½Ñ': (0.45, 0),
        'Ð·Ð°Ñ…Ð¾Ð´Ñƒ': (0, -0.45),
        'ÑÑ…Ð¾Ð´Ñƒ': (0, 0.45)
    }
    
    # Get offset or default to east
    lat_offset, lng_offset = direction_offsets.get(direction, (0, 0.45))
    
    # Apply offset (rough approximation: 1 degree â‰ˆ 111km)
    source_lat = target_lat + lat_offset
    source_lng = target_lng + lng_offset
    
    return source_lat, source_lng

def process_message(text, mid, date_str, channel, _disable_multiline=False):  # type: ignore
    import re
    
    # Helper function to clean text from subscription prompts
    def clean_text(text_to_clean):
        if not text_to_clean:
            return text_to_clean
        import re as re_import
        cleaned = []
        for ln in text_to_clean.splitlines():
            ln2 = ln.strip()
            if not ln2:
                continue
            # Remove invisible/unicode spaces and normalize
            ln2 = re_import.sub(r'[\u200B-\u200D\uFEFF\u3164\u2060\u00A0\u1680\u180E\u2000-\u200F\u202A-\u202E\u2028\u2029\u205F\u3000]+', ' ', ln2)
            ln2 = ln2.strip()
            
            # Check if line ends with subscription text after meaningful content (including bold **text**)
            subscription_match = re_import.search(r'^(.+?)\s+[âž¡â†’>â¬‡â¬†â¬…â¬Œâ†—â†˜â†™â†–]\s*(\*\*)?Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ(\*\*)?\s*$', ln2, re_import.IGNORECASE)
            if subscription_match:
                # Extract the part before the subscription text
                main_content = subscription_match.group(1).strip()
                if main_content and len(main_content) > 5:  # Only keep if meaningful content
                    cleaned.append(main_content)
                continue
                
            # remove any line that is ONLY a subscribe CTA (including bold)
            if re_import.search(r'^[âž¡â†’>â¬‡â¬†â¬…â¬Œâ†—â†˜â†™â†–]?\s*(\*\*)?Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ(\*\*)?\s*$', ln2, re_import.IGNORECASE):
                continue
            
            # Remove URLs and links from text
            ln2 = re_import.sub(r'https?://[^\s]+', '', ln2)  # Remove http/https links
            ln2 = re_import.sub(r'www\.[^\s]+', '', ln2)      # Remove www links
            ln2 = re_import.sub(r't\.me/[^\s]+', '', ln2)     # Remove Telegram links
            ln2 = re_import.sub(r'@[a-zA-Z0-9_]+', '', ln2)  # Remove @mentions
            ln2 = re_import.sub(r'_+', '', ln2)  # Remove leftover underscores
            ln2 = re_import.sub(r'[âœ™âœš]+[^âœ™âœš]*âœ™[^âœ™âœš]*âœ™', '', ln2)  # Remove âœ™...âœ™ patterns
            
            # Remove card numbers and bank details
            ln2 = re_import.sub(r'\d{4}\s*\d{4}\s*\d{4}\s*\d{4}', '', ln2)  # Card numbers
            ln2 = re_import.sub(r'[â€”-]\s*ÐšÐ°Ñ€Ñ‚ÐºÐ°:', '', ln2)  # Card labels
            ln2 = re_import.sub(r'[â€”-]\s*Ð‘Ð°Ð½ÐºÐ°:', '', ln2)   # Bank labels
            ln2 = re_import.sub(r'[â€”-]\s*ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚:', '', ln2) # Envelope labels
            
            # Clean up multiple spaces and trim
            ln2 = re_import.sub(r'\s+', ' ', ln2).strip()
            
            # Skip empty lines after cleaning
            if not ln2:
                continue
                
            cleaned.append(ln2)
        return '\n'.join(cleaned)
    
    # PRIORITY: Check for trajectory patterns FIRST (before any processing)
    # Pattern: "Ð· [source_region] Ð½Ð° [target_region(s)]" - trajectory, not multi-target
    trajectory_pattern = r'(\d+)?\s*ÑˆÐ°Ñ…ÐµÐ´[Ñ–Ñ—Ð²Ñ‹Ð¸Ñ”]*\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+(Ñ‰Ð¸Ð½|Ñ‡Ñ‡Ð¸Ð½)[Ð°ÑƒÐ¸Ñ–])\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘/]+(Ñ‰Ð¸Ð½|Ñ‡Ñ‡Ð¸Ð½)[Ð°ÑƒÐ¸Ñ–Ñƒ])'
    trajectory_match = re.search(trajectory_pattern, text.lower(), re.IGNORECASE)
    
    if trajectory_match:
        count_str = trajectory_match.group(1)
        source_region = trajectory_match.group(2)
        target_regions = trajectory_match.group(4)
        
        print(f"DEBUG: Trajectory detected - {count_str or ''}ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð· {source_region} Ð½Ð° {target_regions}")
        return []
    
    # EARLY FILTERS: Check for messages that should be completely filtered out
    def _is_russian_strategic_aviation(t: str) -> bool:
        """Suppress messages about Russian strategic aviation (Tu-95, etc.) from Russian airbases"""
        t_lower = t.lower()
        
        # Check for Russian strategic bombers
        russian_bombers = ['Ñ‚Ñƒ-95', 'tu-95', 'Ñ‚Ñƒ-160', 'tu-160', 'Ñ‚Ñƒ-22', 'tu-22']
        has_bomber = any(bomber in t_lower for bomber in russian_bombers)
        
        # Check for Russian airbases and regions
        russian_airbases = ['ÐµÐ½Ð³ÐµÐ»ÑŒÑ', 'engels', 'ÑÐ½Ð³ÐµÐ»ÑŒÑ', 'ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²', 'Ñ€ÑÐ·Ð°Ð½ÑŒ', 'Ð¼ÑƒÑ€Ð¾Ð¼', 'ÑƒÐºÑ€Ð°Ñ–Ð½ÐºÐ°', 'ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÐ°']
        has_russian_airbase = any(airbase in t_lower for airbase in russian_airbases)
        
        # Check for Russian regions/areas
        russian_regions = ['ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²ÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–', 'ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²ÑÐºÐ¾Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸', 'Ñ‚ÑƒÐ»ÑŒÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–', 'Ñ€ÑÐ·Ð°Ð½ÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–']
        has_russian_region = any(region in t_lower for region in russian_regions)
        
        # Check for terms indicating Russian territory/airbases
        russian_territory_terms = ['Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð· Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ñƒ', 'Ñ Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð¼ÐµÑ‚Ð° Ð²Ð¸Ð»ÑŒÐ¾Ñ‚Ñƒ Ð½ÐµÐ²Ñ–Ð´Ð¾Ð¼Ð°', 'Ñ†ÐµÐ»ÑŒ Ð²Ñ‹Ð»ÐµÑ‚Ð° Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°']
        has_russian_territory = any(term in t_lower for term in russian_territory_terms)
        
        # Check for generic relocation/transfer terms without specific threats
        relocation_terms = ['Ð¿ÐµÑ€ÐµÐ´Ð¸ÑÐ»Ð¾ÐºÐ°Ñ†Ñ–Ñ', 'Ð¿ÐµÑ€ÐµÐ´Ð¸ÑÐ»Ð¾ÐºÐ°Ñ†Ð¸Ñ', 'Ð¿ÐµÑ€ÐµÐ»Ñ–Ñ‚', 'Ð¿ÐµÑ€ÐµÐ»ÐµÑ‚', 'Ð²Ñ–Ð´Ð¼Ñ–Ñ‡ÐµÐ½Ð¾', 'Ð¾Ñ‚Ð¼ÐµÑ‡ÐµÐ½Ð¾']
        has_relocation = any(term in t_lower for term in relocation_terms)
        
        # Suppress if it's about Russian bombers from Russian territory
        if has_bomber and (has_russian_airbase or has_russian_territory or has_russian_region):
            return True
            
        # Suppress relocation/transfer messages between Russian airbases
        if has_relocation and has_bomber and (has_russian_airbase or has_russian_region):
            return True
            
        # Also suppress general strategic aviation reports without specific Ukrainian targets
        if ('Ð±Ð¾Ñ€Ñ‚' in t_lower or 'Ð±Ð¾Ñ€Ñ‚Ð¸' in t_lower) and ('Ð¼ÐµÑ‚Ð° Ð²Ð¸Ð»ÑŒÐ¾Ñ‚Ñƒ Ð½ÐµÐ²Ñ–Ð´Ð¾Ð¼Ð°' in t_lower or 'Ñ†ÐµÐ»ÑŒ Ð²Ñ‹Ð»ÐµÑ‚Ð° Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°' in t_lower):
            return True
            
        return False

    def _is_general_warning_without_location(t: str) -> bool:
        """Suppress general warnings without specific locations or threat details"""
        t_lower = t.lower()
        
        # Check for general warning phrases
        warning_phrases = [
            'Ð¿Ñ€Ð¾Ñ‚ÑÐ³Ð¾Ð¼ Ð½Ð¾Ñ‡Ñ– ÑƒÐ²Ð°Ð¶Ð½Ð¸Ð¼ Ð±ÑƒÑ‚Ð¸',
            'Ð¿Ñ€Ð¾Ñ‚ÑÐ³Ð¾Ð¼ Ð´Ð½Ñ ÑƒÐ²Ð°Ð¶Ð½Ð¸Ð¼ Ð±ÑƒÑ‚Ð¸', 
            'ÑƒÐ²Ð°Ð¶Ð½Ð¸Ð¼ Ð±ÑƒÑ‚Ð¸',
            'Ð·Ð°Ð³Ð°Ð»ÑŒÐ½Ðµ Ð¿Ð¾Ð¿ÐµÑ€ÐµÐ´Ð¶ÐµÐ½Ð½Ñ',
            'Ð¾Ð±Ñ‰ÐµÐµ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ðµ'
        ]
        has_general_warning = any(phrase in t_lower for phrase in warning_phrases)
        
        # Check for alert messages that should only be in events, not on map
        alert_phrases = [
            'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸',
            'Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°',
            'Ð²Ñ–Ð´Ð±Ð¾Ð¹ Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ð¸',
            'Ð²Ð¾Ð·Ð´ÑƒÑˆÐ½Ð°Ñ Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ð°'
        ]
        has_alert_message = any(phrase in t_lower for phrase in alert_phrases)
        
        # Suppress alert messages - they should only be in events
        if has_alert_message:
            return True
        
        # Check for tactical threat messages first - these should NEVER be filtered
        tactical_phrases = [
            'Ð±Ð¿Ð»Ð°',
            'ÐºÑ€Ð¸Ð»Ð°Ñ‚Ñ– Ñ€Ð°ÐºÐµÑ‚Ð¸',
            'Ñ€Ð°ÐºÐµÑ‚',
            'Ñ€Ð°ÐºÐµÑ‚Ð°',
            'Ñ€Ð°ÐºÐµÑ‚Ð¸',
            'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°',
            'ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°',
            'Ð½Ð°Ð±Ð»Ð¸Ð¶Ð°Ñ”Ñ‚ÑŒÑÑ',
            'Ð¿Ð¾Ð²Ð·',
            'Ð¿Ð¾Ð±Ð»Ð¸Ð·Ñƒ',
            'Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº',
            'Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ',
            'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ',
            'ÐºÐ² ÑˆÐ°Ñ…ÐµÐ´',
            'ÑˆÐ°Ñ…ÐµÐ´',
            'ÐºÐ°Ð±',
            'ÑƒÐ¼Ð¿',
            'Ñ–ÑÐºÐ°Ð½Ð´ÐµÑ€'
        ]
        has_tactical_info = any(phrase in t_lower for phrase in tactical_phrases)
        
        # Check for informational/historical messages that should be filtered
        # even if they contain tactical terms
        informational_phrases = [
            'Ð¿Ñ€Ð¾Ð»ÐµÑ‚Ñ–Ð²',
            'Ð²Ñ–Ð´Ð²ÐµÑ€Ð½ÑƒÐ²',
            'Ð·Ð´Ñ–Ð¹ÑÐ½Ð¸Ð² Ð¿Ð¾ÑÐ°Ð´ÐºÑƒ',
            'Ð¿Ð¾ÑÐ°Ð´ÐºÑƒ Ð½Ð° Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼',
            'Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ Ð±Ð¾Ñ€Ñ‚Ñ–Ð²',
            'Ð±ÑƒÐ´Ñƒ Ð¾Ð½Ð¾Ð²Ð»ÑŽÐ²Ð°Ñ‚Ð¸',
            'Ð² Ñ€Ð°Ð·Ñ– Ð´Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ð¾Ñ— Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ—',
            'Ð½Ð°Ñ€Ð°Ð·Ñ– Ñ†Ðµ Ñ”Ð´Ð¸Ð½Ð°',
            'Ñ„Ð°ÐºÑ‚Ð¸Ñ‡Ð½Ð° Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ'
        ]
        has_informational_content = any(phrase in t_lower for phrase in informational_phrases)
        
        # Check for short tactical messages that are likely informational updates
        # These patterns suggest updates rather than active threats
        brief_tactical_patterns = [
            'Ð±Ð¿Ð»Ð° ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°',
            'Ð±Ð¿Ð»Ð° Ð½Ð° ',
            'Ð¿Ð¾Ð²Ð· .* ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°'
        ]
        # But only filter if the message is relatively short (likely an update, not detailed threat)
        is_brief_message = len(t.strip()) < 100
        has_brief_tactical = any(re.search(pattern, t_lower) for pattern in brief_tactical_patterns)
        
        # Check if this is actually a current location message (not brief update)
        current_location_phrases = [
            'Ð½Ð°Ð´',
            'Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ–',
            'Ð°Ñ‚Ð°ÐºÑƒÐ²Ð°Ð²',
            'Ð²Ð¸Ð±ÑƒÑ…Ð¸ Ð²',
            'Ð²Ð»ÑƒÑ‡Ð°Ð½Ð½Ñ Ð²',
            'Ð·Ð±Ð¸Ñ‚Ð¾ Ð²',
            'Ð·Ð½Ð¸Ñ‰ÐµÐ½Ð¾ Ð²'
        ]
        has_current_location = any(phrase in t_lower for phrase in current_location_phrases)
        
        # Filter brief tactical messages - these are often status updates
        # BUT not if they describe current location/events
        if is_brief_message and has_brief_tactical and not has_current_location:
            return True
        
        # Check for general status messages that contain tactical terms but are informational
        status_phrases = [
            'ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹ | Ð¿Ð¿Ð¾ÑˆÐ½Ð¸Ðº',
            'ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¸Ð¹|Ð¿Ð¿Ð¾ÑˆÐ½Ð¸Ðº',
            'Ð¿Ð¾Ð´Ñ–Ð» Ð»ÑƒÐº\'ÑÐ½Ñ–Ð²ÐºÐ°'
        ]
        has_status_message = any(phrase in t_lower for phrase in status_phrases)
        
        # Check for route/location listing messages (format: "city â€” city1/city2 | region:")
        route_listing_pattern = r'ÐºÐ¸Ñ—Ð².*â€”.*Ð¶ÑƒÐ»ÑÐ½Ð¸.*Ð²Ð¸ÑˆÐ½ÐµÐ²Ðµ.*ÐºÐ¸Ñ—Ð²'
        has_route_listing = re.search(route_listing_pattern, t_lower, re.IGNORECASE)
        
        # Filter route listing messages as they are informational
        if has_route_listing:
            return True
        
        # If message is informational/historical, filter it out
        if has_informational_content:
            return True
            
        # If message is a general status update with tactical info, filter it out
        if has_status_message and has_tactical_info:
            return True
        
        # If message contains tactical information and is not informational, do NOT filter it
        if has_tactical_info:
            return False
        
        # Check for donation/fundraising messages (use more specific phrases)
        donation_phrases = [
            'Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ð¹Ñ‚Ðµ Ð¼ÐµÐ½Ðµ',
            'Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ ÐºÐ°Ð½Ð°Ð»',
            'Ñ€ÐµÐºÐ²Ñ–Ð·Ð¸Ñ‚Ð¸',
            'ÐºÐ°Ñ€Ñ‚ÐºÐ°:',
            'Ð±Ð°Ð½ÐºÐ°:',
            'Ð³Ñ€Ð½ Ð½Ð° ÐºÐ°Ð²Ñƒ',
            'Ð½Ð° ÐºÐ°Ð²Ñƒ Ñ‚Ð° ÐµÐ½ÐµÑ€Ð³ÐµÑ‚Ð¸ÐºÐ¸',
            'Ð¿Ð¾ Ð±Ð°Ð¶Ð°Ð½Ð½ÑŽ',
            'Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÐ° Ñ‚Ñ–Ð»ÑŒÐºÐ¸',
            'monobank.ua',
            'privat24.ua',
            'send.monobank',
            'www.privat24',
            'Ð´Ð¾Ð½Ð°Ñ‚Ð¸',
            'Ð´Ð¾Ð½Ð°Ñ‚',
            'Ð´ÑƒÐ¶Ðµ Ð²Ð´ÑÑ‡Ð½Ð¸Ð¹',
            'Ð²Ð´ÑÑ‡Ð½Ð¸Ð¹ Ð²Ð°Ð¼ Ð·Ð° Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÑƒ',
            'Ð·Ð° Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÑƒ',
            'Ð´ÑÐºÑƒÑŽ Ð·Ð° Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÑƒ'
        ]
        has_donation_message = any(phrase in t_lower for phrase in donation_phrases)
        
        # Suppress donation messages
        if has_donation_message:
            return True
        
        # Check for channel promotion messages
        promotion_phrases = [
            'Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ ÐºÐ°Ð½Ð°Ð»',
            'ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÑƒ',
            'Ð¿Ð¾Ð´Ð¿Ð¸ÑÑ‹Ð²Ð°Ð¹Ñ‚ÐµÑÑŒ',
            'Ð½Ð°Ñˆ ÐºÐ°Ð½Ð°Ð»',
            'Ð½Ð°Ñˆ Ñ‚ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼'
        ]
        has_promotion_message = any(phrase in t_lower for phrase in promotion_phrases)
        
        # Suppress promotion messages
        if has_promotion_message:
            return True
        
        # Check for general informational messages without threats
        info_phrases = [
            'Ð½Ð°Ñ€Ð°Ð·Ñ– Ñ†Ðµ Ñ”Ð´Ð¸Ð½Ð°',
            'Ñ„Ð°ÐºÑ‚Ð¸Ñ‡Ð½Ð° Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ',
            'Ð±ÑƒÐ´Ñƒ Ð¾Ð½Ð¾Ð²Ð»ÑŽÐ²Ð°Ñ‚Ð¸',
            'Ð² Ñ€Ð°Ð·Ñ– Ð´Ð¾Ð´Ð°Ñ‚ÐºÐ¾Ð²Ð¾Ñ— Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ—',
            'Ð·Ð´Ñ–Ð¹ÑÐ½Ð¸Ð² Ð¿Ð¾ÑÐ°Ð´ÐºÑƒ',
            'Ð¿Ð¾ÑÐ°Ð´ÐºÑƒ Ð½Ð° Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼',
            'Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ Ð±Ð¾Ñ€Ñ‚Ñ–Ð²'
        ]
        has_info_message = any(phrase in t_lower for phrase in info_phrases)
        
        # Suppress general info messages
        if has_info_message:
            return True
        
        # Check for very broad regions without specific cities
        broad_regions = [
            'ÐºÐ¸Ñ”Ð²Ñƒ, ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð° Ñ– Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° ÑƒÐºÑ€Ð°Ñ—Ð½Ð°',
            'ÐºÐ¸Ñ—Ð², ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð° Ñ– Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° ÑƒÐºÑ€Ð°Ñ—Ð½Ð°', 
            'Ñ†ÐµÐ½Ñ‚Ñ€ Ñ– Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', 
            'Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ Ñ– ÑÑ…Ñ–Ð´'
        ]
        has_broad_region = any(region in t_lower for region in broad_regions)
        
        # Suppress if it's a general warning with broad regions
        if has_general_warning and has_broad_region:
            return True
            
        # Also suppress very short messages that are just general alerts
        if len(t.strip()) < 50 and has_general_warning:
            return True
            
        return False

    # Apply early filters
    if _is_russian_strategic_aviation(text):
        return []
        
    if _is_general_warning_without_location(text):
        return []
    
    # PRIORITY: Handle directional movement patterns (Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ, Ð² Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸)
    # These should show trajectory/direction, not markers at destination
    def _is_directional_movement_message(t: str) -> bool:
        """Check if message describes movement towards a destination"""
        t_lower = t.lower()
        
        # Patterns indicating movement toward destination, not presence at location
        directional_patterns = [
            'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ',
            'Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', 
            'Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº',
            'Ñ€ÑƒÑ…Ð°Ñ”Ñ‚ÑŒÑÑ Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ',
            'Ð»ÐµÑ‚Ð¸Ñ‚ÑŒ Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ',
            'ÐºÑƒÑ€Ñ Ð½Ð°',
            'Ð¿Ñ€ÑÐ¼ÑƒÑ” Ð´Ð¾'
        ]
        
        # Additional context that suggests this is about movement, not current location
        movement_context = [
            'Ð· Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ',
            'Ð· Ð¿Ñ–Ð²Ð´Ð½Ñ',
            'Ð· Ð·Ð°Ñ…Ð¾Ð´Ñƒ',
            'Ð· ÑÑ…Ð¾Ð´Ñƒ',  
            'Ñ€ÑƒÑ…Ð°Ñ”Ñ‚ÑŒÑÑ',
            'Ð»ÐµÑ‚Ð¸Ñ‚ÑŒ',
            'Ð¿Ñ€ÑÐ¼ÑƒÑ”'
        ]
        
        has_directional = any(pattern in t_lower for pattern in directional_patterns)
        has_movement_context = any(context in t_lower for context in movement_context)
        
        return has_directional and has_movement_context
    
    # Handle directional movement messages - create projected path instead of filtering
    if _is_directional_movement_message(text):
        return _create_directional_trajectory_markers(text, mid, date_str, channel)
    
    # PRIORITY: Try SpaCy enhanced processing first
    if SPACY_AVAILABLE:
        try:
            spacy_results = spacy_enhanced_geocoding(text)
            if spacy_results:
                # Convert SpaCy results to the format expected by the rest of the system
                threat_markers = []
                
                # Process cities with coordinates first
                cities_with_coords = [city for city in spacy_results if city['coords']]
                
                for spacy_city in cities_with_coords:
                    lat, lng = spacy_city['coords']
                    
                    # Determine threat type using our classify function
                    threat_type, icon = classify(text, spacy_city['name'])
                    
                    # Create a proper place label
                    place_label = spacy_city['name'].title()
                    if spacy_city['region']:
                        place_label += f" [{spacy_city['region'].title()}]"
                    
                    marker = {
                        'id': f"{mid}_spacy_{len(threat_markers)+1}",
                        'place': place_label,
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': clean_text(text)[:500],
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': f'spacy_{spacy_city["source"]}',
                        'count': 1,
                        'confidence': spacy_city['confidence']
                    }
                    threat_markers.append(marker)
                    
                    add_debug_log(f"SPACY: Created marker for {spacy_city['name']} -> {spacy_city['normalized']} "
                                f"(case: {spacy_city.get('case', 'unknown')}, confidence: {spacy_city['confidence']})", 
                                "spacy_integration")
                
                if threat_markers:
                    add_debug_log(f"SPACY: Successfully processed message with {len(threat_markers)} markers", "spacy_integration")
                    return threat_markers
                    
        except Exception as e:
            add_debug_log(f"SPACY: Error processing message: {e}", "spacy_integration")
            # Continue with fallback processing
    
    # FALLBACK: Original regex-based processing continues below
    
    # PRIORITY: Handle "[city] Ð½Ð° [region]" patterns early to avoid misprocessing
    regional_city_match = re.search(r'(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”]+Ñ‰Ð¸Ð½Ñ–?)', text.lower()) if text else None
    if regional_city_match:
        count_str = regional_city_match.group(1)
        city_raw = regional_city_match.group(2).strip()
        region_raw = regional_city_match.group(3).strip()
        
        # Use context-aware resolution
        coords = ensure_city_coords_with_message_context(city_raw, text)
        if coords:
            lat, lng, approx = coords
            add_debug_log(f"PRIORITY: Regional city pattern - {city_raw} Ð½Ð° {region_raw} -> ({lat}, {lng})", "priority_regional_city")
            
            result_entry = {
                'id': f"{mid}_priority_regional",
                'place': f"{city_raw.title()} Ð½Ð° {region_raw.title()}",
                'lat': lat, 'lng': lng,
                'type': 'shahed', 'count': int(count_str),
                'timestamp': date_str, 'channel': channel
            }
            return [result_entry]
    
    # EARLY CHECK: General multi-line threat detection (before specific cases)
    if not _disable_multiline:
        text_lines = (text or '').split('\n')
        threat_lines = []
        
        # Look for lines that contain threats with quantities and targets
        for line in text_lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue
                
            line_lower = line_stripped.lower()
            
            # Check if line contains threat patterns with quantities and targets
            has_threat_pattern = (
                # Pattern: "N x/Ã— Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [target]"
                (re.search(r'\d+\s*[xÑ…Ã—]\s*Ð±Ð¿Ð»Ð°.*?(ÐºÑƒÑ€Ñ|Ð½Ð°)\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²/ÑˆÐ°Ñ…ÐµÐ´Ð¸ Ð½Ð° [target]" - all forms of Shahed
                (re.search(r'\d+\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²/ÑˆÐ°Ñ…ÐµÐ´Ð¸ Ð±Ñ–Ð»Ñ [target]" - near target
                (re.search(r'\d+\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð±Ñ–Ð»Ñ\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N ÑƒÐ´Ð°Ñ€Ð½Ð¸Ñ… Ð‘Ð¿Ð›Ð Ð½Ð° [target]"
                (re.search(r'\d+\s+ÑƒÐ´Ð°Ñ€Ð½.*?Ð±Ð¿Ð»Ð°.*?Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N Ð‘Ð¿Ð›Ð Ð½Ð° [target]" or "N Ð±Ð¿Ð»Ð° Ð½Ð° [target]"  
                (re.search(r'\d+\s+Ð±Ð¿Ð»Ð°.*?Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [target]" (without count)
                (re.search(r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ.*?Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ñ‡ÐµÑ€ÐµÐ· [target]" - via target  
                (re.search(r'\d+\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ñ‡ÐµÑ€ÐµÐ·\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower)) or
                # Pattern: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð· Ð±Ð¾ÐºÑƒ [target]" - from direction of target
                (re.search(r'\d+\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð·\s+Ð±Ð¾ÐºÑƒ\s+([Ð°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)', line_lower))
            )
            
            if has_threat_pattern:
                threat_lines.append(line_stripped)
        
        # If we have multiple threat lines, process them separately
        if len(threat_lines) >= 2:
            add_debug_log(f"MULTI-LINE THREAT PROCESSING: {len(threat_lines)} threat lines detected", "multi_line_threats")
            
            all_tracks = []
            for i, line in enumerate(threat_lines):
                if not line.strip():
                    continue
                    
                add_debug_log(f"Processing threat line {i+1}: {line[:100]}", "threat_line")
                
                # Process each line as a separate message with multiline disabled
                line_result = process_message(line.strip(), f"{mid}_threat_{i+1}", date_str, channel, _disable_multiline=True)
                if line_result and isinstance(line_result, list):
                    all_tracks.extend(line_result)
                    add_debug_log(f"Threat line {i+1} produced {len(line_result)} tracks", "threat_line_result")
                else:
                    add_debug_log(f"Threat line {i+1} produced no tracks", "threat_line_result")
            
            if all_tracks:
                add_debug_log(f"Multi-line threat processing complete: {len(all_tracks)} total tracks", "multi_line_threats_complete")
                return all_tracks
    
    # PRIORITY FIRST: All air alarm messages should be list-only (no map markers)
    # This must be checked BEFORE any other processing to prevent other logic from creating markers
    original_text = text or ''
    low_orig = original_text.lower()
    
    # Clear any previous priority result
    globals()['_current_priority_result'] = None
    
    # IMMEDIATE CHECK: Multi-regional UAV messages (highest priority)
    text_lines = original_text.split('\n')
    region_count = sum(1 for line in text_lines if any(region in line.lower() for region in ['Ñ‰Ð¸Ð½Ð°:', 'Ñ‰Ð¸Ð½Ð°]', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ:', 'ÐºÑ€Ð°Ð¹:']) or (
        'Ñ‰Ð¸Ð½Ð°' in line.lower() and line.lower().strip().endswith(':')
    ) or any(region in line.lower() for region in ['Ñ‰Ð¸Ð½Ð°)', 'Ñ‰Ð¸Ð½Ð¸', 'Ñ‰Ð¸Ð½Ñƒ', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°', 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°']))
    # Look for lines with emoji + UAV mentions (more flexible detection)
    uav_lines = [line for line in text_lines if 'Ð±Ð¿Ð»Ð°' in line.lower() and ('ðŸ›µ' in line or 'ðŸ›¸' in line)]
    uav_count = len(uav_lines)
    
    # NEW: Look for lines with Shahed mentions and regions (without emoji requirement)
    shahed_region_lines = [line for line in text_lines if 
                          ('ÑˆÐ°Ñ…ÐµÐ´' in line.lower() or 'shahed' in line.lower()) and 
                          ('Ñ‰Ð¸Ð½Ð°' in line.lower() or 'Ñ‰Ð¸Ð½Ñƒ' in line.lower() or 'Ñ‰Ð¸Ð½Ñ–' in line.lower())]
    shahed_count = len(shahed_region_lines)
    
    add_debug_log(f"DEBUG COUNT CHECK: {region_count} regions, {uav_count} UAV lines, {shahed_count} Shahed+region lines", "count_check")
    
    # If we have multiple Shahed lines with regions, process them separately
    if shahed_count >= 2:
        add_debug_log(f"MULTI-LINE SHAHED PROCESSING: {shahed_count} Shahed+region lines detected", "multi_shahed")
        
        all_tracks = []
        for i, line in enumerate(shahed_region_lines):
            if not line.strip():
                continue
                
            add_debug_log(f"Processing Shahed line {i+1}: {line[:100]}", "shahed_line")
            
            # Process each line as a separate message
            line_result = process_message(line.strip(), f"{mid}_shahed_{i+1}", date_str, channel, _disable_multiline=True)
            if line_result and isinstance(line_result, list):
                all_tracks.extend(line_result)
                add_debug_log(f"Shahed line {i+1} produced {len(line_result)} tracks", "shahed_line_result")
            else:
                add_debug_log(f"Shahed line {i+1} produced no tracks", "shahed_line_result")
        
        if all_tracks:
            add_debug_log(f"Multi-line Shahed processing complete: {len(all_tracks)} total tracks", "multi_shahed_complete")
            return all_tracks
    
    # If we have multiple UAV lines with emojis, process them separately even if they don't have explicit regions
    if uav_count >= 2 and (region_count >= 1 or any('Ñ€Ð°Ð¹Ð¾Ð½Ñƒ' in line.lower() or 'Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–' in line.lower() or 'Ð¾Ð±Ð».' in line.lower() for line in uav_lines)):
        add_debug_log(f"MULTI-LINE UAV PROCESSING: {uav_count} UAV lines detected", "multi_uav")
        
        all_tracks = []
        for i, line in enumerate(uav_lines):
            if not line.strip():
                continue
                
            add_debug_log(f"Processing UAV line {i+1}: {line[:100]}", "uav_line")
            
            # Process each line as a separate message
            line_result = process_message(line.strip(), f"{mid}_line_{i+1}", date_str, channel, _disable_multiline=True)
            if line_result and isinstance(line_result, list):
                all_tracks.extend(line_result)
                add_debug_log(f"Line {i+1} produced {len(line_result)} tracks", "uav_line_result")
            else:
                add_debug_log(f"Line {i+1} produced no tracks", "uav_line_result")
        
        if all_tracks:
            add_debug_log(f"Multi-line UAV processing complete: {len(all_tracks)} total tracks", "multi_uav_complete")
            return all_tracks
    
    # Legacy multi-regional detection (keep for backward compatibility)
    if region_count >= 2 and sum(1 for line in text_lines if 'Ð±Ð¿Ð»Ð°' in line.lower() and ('ÐºÑƒÑ€Ñ' in line.lower() or 'Ð½Ð° ' in line.lower())) >= 3:
        add_debug_log(f"IMMEDIATE MULTI-REGIONAL UAV: {region_count} regions, {uav_count} UAVs - ENTERING EARLY PROCESSING", "multi_regional")
        # Process directly without going through other logic
        import re
        
        # Define essential functions inline for immediate processing
        def get_city_coords_quick(city_name):
            """Quick coordinate lookup with accusative case normalization"""
            city_norm = city_name.strip().lower()
            
            # Handle specific multi-word cities in accusative case
            if city_norm == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':
                city_norm = 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°'
            elif city_norm == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð²Ð¸ÑÐºÑƒ':
                city_norm = 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð²Ð¸ÑÐºÐ°'
            elif city_norm == 'Ð¼ÐµÐ½Ñƒ':
                city_norm = 'Ð¼ÐµÐ½Ð°'
            elif city_norm == 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÑƒ':
                city_norm = 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÐ°'
            elif city_norm == 'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹':
                city_norm = 'Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹'
            elif city_norm == 'ÐºÐ¸Ñ”Ð²Ð¾Ð¼':
                city_norm = 'ÐºÐ¸Ñ—Ð²'
            
            # General accusative case endings (Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ð´ÐµÐ¶)
            elif city_norm.endswith('Ñƒ') and len(city_norm) > 3:
                city_norm = city_norm[:-1] + 'Ð°'
            elif city_norm.endswith('ÑŽ') and len(city_norm) > 3:
                city_norm = city_norm[:-1] + 'Ñ'
            elif city_norm.endswith('ÐºÑƒ') and len(city_norm) > 4:
                city_norm = city_norm[:-2] + 'ÐºÐ°'
            
            # Apply UA_CITY_NORMALIZE rules
            if city_norm in UA_CITY_NORMALIZE:
                city_norm = UA_CITY_NORMALIZE[city_norm]
            
            # Try direct lookup
            coords = CITY_COORDS.get(city_norm)
            if not coords:
                coords = ensure_city_coords_with_message_context(city_norm, text)
            
            add_debug_log(f"Coord lookup: '{city_name}' -> '{city_norm}' -> {bool(coords)}", "multi_regional")
            return coords
        
        threats = []
        processed_cities = set()  # Ð˜Ð·Ð±ÐµÐ³Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²
        
        for line in text_lines:
            line_stripped = line.strip()
            if not line_stripped or ':' in line_stripped[:20]:  # Skip region headers
                continue
            
            line_lower = line_stripped.lower()
            
            # Look for UAV course patterns
            if 'Ð±Ð¿Ð»Ð°' in line_lower and ('ÐºÑƒÑ€Ñ' in line_lower or ' Ð½Ð° ' in line_lower or 'Ð½Ð°Ð´' in line_lower or 'Ð¿Ð¾Ð²Ð·' in line_lower):
                # Extract city name from patterns - handle both plain text and markdown links
                patterns = [
                    # Pattern for markdown links: Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [Ð‘Ñ€Ð¾Ð²Ð°Ñ€Ð¸](link)
                    r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+(?:ÐºÑƒÑ€ÑÐ¾Ð¼?)?\s*(?:Ð½Ð°|Ð½Ð°Ð´)\s+\[([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘Ê¼\'\-\s]+?)\]',
                    # Pattern for plain text: Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ (improved to capture multi-word cities)
                    r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘Ê¼\'\-\s]+?)(?=\s*(?:\n|$|[,\.\!\?;]|\s+\d+[xÑ…]?\s*Ð±Ð¿Ð»Ð°|\s+[Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+Ñ‰Ð¸Ð½Ð°:))',
                    # Pattern for "Ð¿Ð¾Ð²Ð·" (e.g., "Ð‘Ð¿Ð›Ð Ð¿Ð¾Ð²Ð· Ð¡Ð»Ð°Ð²ÑƒÑ‚Ð¸Ñ‡ Ð² Ð±Ñ–Ðº Ð‘Ñ–Ð»Ð¾Ñ€ÑƒÑÑ–")
                    r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+(?:.*?)?Ð¿Ð¾Ð²Ð·\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘Ê¼\'\-\s]{3,50}?)(?=\s+(?:Ð²\s+Ð±Ñ–Ðº|Ð´Ð¾|Ð½Ð°|Ñ‡ÐµÑ€ÐµÐ·|$|[,\.\!\?;]))'
                ]
                
                # Also check for bracket city pattern like "Ð’Ð¸Ð»ÐºÐ¾Ð²Ð¾ (ÐžÐ´ÐµÑ‰Ð¸Ð½Ð°)"
                bracket_matches = re.finditer(r'([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]{3,30})\s*\(([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+Ñ‰Ð¸Ð½Ð°|[Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+Ð¾Ð±Ð»\.?)\)', line_stripped, re.IGNORECASE)
                for bmatch in bracket_matches:
                    city_clean = bmatch.group(1).strip()
                    region_info = bmatch.group(2).strip()
                    
                    city_normalized = city_clean.lower()
                    city_key = city_normalized
                    
                    # Skip if already processed
                    if city_key in processed_cities:
                        continue
                    processed_cities.add(city_key)
                    
                    # Try to get coordinates
                    coords = get_city_coords_quick(city_clean)
                    
                    if coords:
                        if len(coords) == 3:
                            lat, lng, approx = coords
                        else:
                            lat, lng = coords
                        
                        threat_id = f"{mid}_imm_bracket_{len(threats)}"
                        threats.append({
                            'id': threat_id,
                            'place': city_clean.title(),
                            'lat': lat,
                            'lng': lng,
                            'threat_type': 'shahed',
                            'text': f"{line_stripped} (bracket city)",
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': 'shahed.png',
                            'source_match': 'immediate_multi_regional_bracket',
                            'count': 1
                        })
                        
                        add_debug_log(f"Immediate Multi-regional bracket: {city_clean} -> {coords}", "multi_regional")
                    else:
                        add_debug_log(f"Immediate Multi-regional bracket: No coords for {city_clean}", "multi_regional")
                
                for pattern in patterns:
                    matches = re.finditer(pattern, line_stripped, re.IGNORECASE)
                    for match in matches:
                        if len(match.groups()) == 2:
                            count_str, city_raw = match.groups()
                        else:
                            count_str = None
                            city_raw = match.group(1)
                        
                        if not city_raw:
                            continue
                            
                        # Clean city name (remove trailing spaces)
                        city_clean = city_raw.strip()
                        
                        # Normalize city name for coordinate lookup  
                        city_normalized = city_clean.lower()
                        
                        # Normalize for display (convert accusative to nominative)
                        city_display = city_clean
                        if city_normalized == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':
                            city_display = 'Ð’ÐµÐ»Ð¸ÐºÐ° Ð”Ð¸Ð¼ÐµÑ€ÐºÐ°'
                        elif city_normalized == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð²Ð¸ÑÐºÑƒ':
                            city_display = 'Ð’ÐµÐ»Ð¸ÐºÐ° Ð’Ð¸ÑÐºÐ°'
                        elif city_normalized == 'Ð¼ÐµÐ½Ñƒ':
                            city_display = 'ÐœÐµÐ½Ð°'
                        elif city_normalized == 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÑƒ':
                            city_display = 'ÐŸÑ–ÑÐºÑ–Ð²ÐºÐ°'
                        elif city_normalized == 'ÐºÐ¸Ñ”Ð²Ð¾Ð¼':
                            city_display = 'ÐšÐ¸Ñ—Ð²'
                            city_normalized = 'ÐºÐ¸Ñ—Ð²'  # Also normalize for lookup
                        elif city_normalized.endswith('Ð¾Ð¼') and len(city_normalized) > 4:
                            # Handle other accusative masculine endings
                            city_display = city_normalized[:-2]
                            city_display = city_display.title()
                            city_normalized = city_normalized[:-2]
                        elif city_normalized.endswith('Ñƒ') and len(city_normalized) > 3:
                            city_display = city_normalized[:-1] + 'Ð°'
                            city_display = city_display.title()
                        elif city_normalized.endswith('ÑŽ') and len(city_normalized) > 3:
                            city_display = city_normalized[:-1] + 'Ñ'
                            city_display = city_display.title()
                        elif city_normalized.endswith('ÐºÑƒ') and len(city_normalized) > 4:
                            city_display = city_normalized[:-2] + 'ÐºÐ°'
                            city_display = city_display.title()
                        else:
                            city_display = city_clean.title()
                        
                        city_key = city_normalized
                        
                        # Skip if already processed
                        if city_key in processed_cities:
                            continue
                        processed_cities.add(city_key)
                        
                        # Try to get coordinates
                        coords = get_city_coords_quick(city_clean)
                        
                        if coords:
                            if len(coords) == 3:
                                lat, lng, approx = coords
                            else:
                                lat, lng = coords
                            
                            # Extract count if present
                            uav_count_num = 1
                            if count_str and count_str.isdigit():
                                uav_count_num = int(count_str)
                            
                            # Create multiple tracks for multiple drones
                            tracks_to_create = max(1, uav_count_num)
                            for i in range(tracks_to_create):
                                track_display_name = city_display
                                if tracks_to_create > 1:
                                    track_display_name += f" #{i+1}"
                                
                                # Add small coordinate offsets to prevent marker overlap
                                marker_lat = lat
                                marker_lng = lng
                                if tracks_to_create > 1:
                                    # Create a circular pattern around the target
                                    import math
                                    angle = (2 * math.pi * i) / tracks_to_create
                                    offset_distance = 0.01  # ~1km offset
                                    marker_lat += offset_distance * math.cos(angle)
                                    marker_lng += offset_distance * math.sin(angle)
                                
                                threat_id = f"{mid}_imm_multi_{len(threats)}"
                                threats.append({
                                    'id': threat_id,
                                    'place': track_display_name,  # Use numbered display name for multiple drones
                                    'lat': marker_lat,
                                    'lng': marker_lng,
                                    'threat_type': 'shahed',
                                    'text': f"{line_stripped} (Ð¼ÑƒÐ»ÑŒÑ‚Ð¸Ñ€ÐµÐ³Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ðµ)",
                                    'date': date_str,
                                    'channel': channel,
                                    'marker_icon': 'shahed.png',
                                    'source_match': f'immediate_multi_regional_uav_{uav_count_num}x',
                                    'count': 1  # Each track represents 1 drone
                                })
                            
                            add_debug_log(f"Immediate Multi-regional: {city_clean} ({uav_count_num}x) -> {tracks_to_create} tracks at {coords}", "multi_regional")
                        else:
                            add_debug_log(f"Immediate Multi-regional: No coords for {city_clean}", "multi_regional")
        
        # Also check for regional UAV references without specific cities
        for line in text_lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue
            line_lower = line_stripped.lower()
            
            # Look for UAV + region patterns without specific cities
            if 'Ð±Ð¿Ð»Ð°' in line_lower and any(region in line_lower for region in ['Ñ‰Ð¸Ð½Ð¸', 'Ñ‰Ð¸Ð½Ñƒ', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸']):
                # Skip if this specific line contains a city that was already processed
                line_has_processed_city = False
                for city in processed_cities:
                    if city in line_lower:
                        line_has_processed_city = True
                        break
                
                if line_has_processed_city:
                    continue
                
                # Special case: movement messages with direction in parentheses
                # Pattern: "Ð‘Ð¿Ð›Ð Ð½Ð° Ð¿Ñ–Ð²Ð´Ð½Ñ– Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸, Ñ€ÑƒÑ…Ð°ÑŽÑ‚ÑŒÑÑ Ð½Ð° Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ (ÐšÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°)"
                # Here (ÐšÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°) indicates direction, not location
                directional_movement = re.search(r'Ð½Ð°\s+([\w\-\s/]+?)\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ð¸|[Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ñƒ|Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸|Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð¸|Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸).*Ñ€ÑƒÑ…Ð°ÑŽÑ‚ÑŒÑÑ.*\(([^)]+)\)', line_lower)
                if directional_movement:
                    direction = directional_movement.group(1).strip()
                    region_raw = directional_movement.group(2).strip()
                    target_direction = directional_movement.group(3).strip()
                    
                    # Map region to oblast center (current location, not target)
                    region_coords = None
                    if 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²' in region_raw:
                        region_coords = (48.45, 35.0)
                        region_name = 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸'
                    elif 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²' in region_raw:
                        region_coords = (51.4982, 31.3044)
                        region_name = 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸'
                    elif 'Ð¾Ð´ÐµÑ' in region_raw:
                        region_coords = (46.5197, 30.7495)
                        region_name = 'ÐžÐ´ÐµÑ‰Ð¸Ð½Ð¸'
                    
                    if region_coords:
                        # Apply directional offset for current location
                        lat, lng = region_coords
                        if 'Ð¿Ñ–Ð²Ð´Ð½' in direction or 'ÑŽÐ¶Ð½' in direction:
                            lat -= 0.5
                        elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡' in direction or 'ÑÐµÐ²ÐµÑ€' in direction:
                            lat += 0.5
                        elif 'Ð·Ð°Ñ…Ñ–Ð´' in direction or 'Ð·Ð°Ð¿Ð°Ð´' in direction:
                            lng -= 0.8
                        elif 'ÑÑ…Ñ–Ð´' in direction or 'Ð²Ð¾ÑÑ‚Ð¾Ðº' in direction:
                            lng += 0.8
                        
                        direction_label = direction.replace('Ð¿Ñ–Ð²Ð´Ð½', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½').replace('Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½')
                        place_name = f"{region_name} ({direction_label}Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°) â†’ {target_direction}"
                        
                        threat_id = f"{mid}_imm_regional_movement_{len(threats)}"
                        threats.append({
                            'id': threat_id,
                            'place': place_name,
                            'lat': lat,
                            'lng': lng,
                            'threat_type': 'shahed',
                            'text': f"{line_stripped} (Ñ€ÑƒÑ… Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ {target_direction})",
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': 'shahed.png',
                            'source_match': 'immediate_multi_regional_movement',
                            'count': 1,
                            'movement_target': target_direction
                        })
                        
                        add_debug_log(f"Immediate Multi-regional movement: {place_name} -> {lat}, {lng} (target: {target_direction})", "multi_regional")
                        continue
                
                # Check if this is a directional reference like "Ð½Ð° Ð¿Ñ–Ð²Ð´Ð½Ñ– Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸"
                region_match = re.search(r'Ð½Ð°\s+([\w\-\s/]+?)\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ð¸|[Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ñƒ|Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸|Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð¸|Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸)', line_lower)
                if region_match:
                    direction = region_match.group(1).strip()
                    region_raw = region_match.group(2).strip()
                    
                    # Map region to oblast center
                    region_coords = None
                    if 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²' in region_raw:
                        region_coords = (48.45, 35.0)
                        region_name = 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸'
                    elif 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²' in region_raw:
                        region_coords = (51.4982, 31.3044)
                        region_name = 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð¸'
                    elif 'Ð¾Ð´ÐµÑ' in region_raw:
                        region_coords = (46.5197, 30.7495)
                        region_name = 'ÐžÐ´ÐµÑ‰Ð¸Ð½Ð¸'
                    
                    if region_coords:
                        # Apply directional offset
                        lat, lng = region_coords
                        if 'Ð¿Ñ–Ð²Ð´Ð½' in direction or 'ÑŽÐ¶Ð½' in direction:
                            lat -= 0.5
                        elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡' in direction or 'ÑÐµÐ²ÐµÑ€' in direction:
                            lat += 0.5
                        elif 'Ð·Ð°Ñ…Ñ–Ð´' in direction or 'Ð·Ð°Ð¿Ð°Ð´' in direction:
                            lng -= 0.8
                        elif 'ÑÑ…Ñ–Ð´' in direction or 'Ð²Ð¾ÑÑ‚Ð¾Ðº' in direction:
                            lng += 0.8
                        
                        direction_label = direction.replace('Ð¿Ñ–Ð²Ð´Ð½', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½').replace('Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½')
                        place_name = f"{region_name} ({direction_label}Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°)"
                        
                        threat_id = f"{mid}_imm_regional_{len(threats)}"
                        threats.append({
                            'id': threat_id,
                            'place': place_name,
                            'lat': lat,
                            'lng': lng,
                            'threat_type': 'shahed',
                            'text': f"{line_stripped} (Ñ€ÐµÐ³Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¸Ð¹)",
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': 'shahed.png',
                            'source_match': 'immediate_multi_regional_region',
                            'count': 1
                        })
                        
                        add_debug_log(f"Immediate Multi-regional regional: {place_name} -> {lat}, {lng}", "multi_regional")
        
        if threats:
            add_debug_log(f"IMMEDIATE MULTI-REGIONAL RESULT: {len(threats)} threats", "multi_regional")
            return threats
    
    if 'Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°' in low_orig or 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°' in low_orig or 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³' in low_orig:
        # Always event-only record (list), never create map markers for air alarms or cancellations
        place = None
        low = low_orig.lower()
        # Try to extract oblast/region info for place
        for name in ['Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'Ð¾Ð´ÐµÑÑŒÐºÐ°', 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°', 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°', 'ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°', 'ÑÑƒÐ¼ÑÑŒÐºÐ°', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ°', 'Ð²Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ°', 'Ñ€Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ°', 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°', 'Ð²Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ°', 'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ°', 'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ°', 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ°', 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ°', 'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ°', 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ°']:
            if name in low:
                place = name.title() + ' ÐžÐ±Ð».'
                break
        
        # Also try to find city names
        if not place:
            for city in ['Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'Ð¾Ð´ÐµÑÐ°', 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²', 'Ñ…ÐµÑ€ÑÐ¾Ð½', 'ÐºÐ¸Ñ—Ð²', 'Ð»ÑŒÐ²Ñ–Ð²', 'Ñ…Ð°Ñ€ÐºÑ–Ð²', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 'ÑÑƒÐ¼Ð¸', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°']:
                if city in low:
                    place = city.title()
                    break
        
        # Determine if this is alarm start or cancellation
        threat_type = 'alarm_cancel' if ('Ð²Ñ–Ð´Ð±Ñ–Ð¹' in low_orig or 'Ð¾Ñ‚Ð±Ð¾Ð¹' in low_orig) else 'alarm'
        icon = 'vidboi.png' if threat_type == 'alarm_cancel' else 'trivoga.png'
        
        # Clean subscription links from air alarm messages before returning
        import re as re_import
        cleaned_text = original_text
        if original_text:
            # remove lines containing subscription prompts
            cleaned = []
            for ln in original_text.splitlines():
                ln2 = ln.strip()
                if not ln2:
                    continue
                # remove any line that is just a subscribe CTA or starts with arrow+subscribe
                if re_import.search(r'(Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑŒ|Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ|Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ|Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ°Ñ‚ÑŒÑÑ|Ð¿Ð¾Ð´Ð¿Ð¸ÑˆÐ¸ÑÑŒ|subscribe)', ln2, re_import.IGNORECASE):
                    continue
                # remove arrow+subscribe pattern specifically
                if re_import.search(r'[âž¡â†’>]\s*Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ', ln2, re_import.IGNORECASE):
                    continue
                cleaned.append(ln2)
            cleaned_text = '\n'.join(cleaned)
        
        return [{
            'id': str(mid), 'place': place, 'lat': None, 'lng': None,
            'threat_type': threat_type, 'text': cleaned_text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': icon, 'list_only': True
        }]

    # Define classify function at the start so it's available throughout process_message
    def classify(th: str, city_context: str = ""):
        import re  # Import re module locally for pattern matching
        l = th.lower()
        
        # Add debug logging (temporarily disabled)
        # print(f"[CLASSIFY DEBUG] Input text: {th}")
        # print(f"[CLASSIFY DEBUG] Lowercase text: {l}")
        # print(f"[CLASSIFY DEBUG] City context: {city_context}")
        # print(f"[CLASSIFY DEBUG] Contains ðŸš€: {'ðŸš€' in th}")
        # print(f"[CLASSIFY DEBUG] Contains 'Ñ†Ñ–Ð»ÑŒ': {'Ñ†Ñ–Ð»ÑŒ' in l}")
        # print(f"[CLASSIFY DEBUG] Contains 'Ð²Ð¸ÑÐ¾ÐºÐ¾ÑˆÐ²Ð¸Ð´ÐºÑ–ÑÐ½': {'Ð²Ð¸ÑÐ¾ÐºÐ¾ÑˆÐ²Ð¸Ð´ÐºÑ–ÑÐ½' in l}")
        # print(f"[CLASSIFY DEBUG] Contains 'Ð±Ð¿Ð»Ð°': {'Ð±Ð¿Ð»Ð°' in l}")
        
        # PRIORITY: Artillery shelling warning (Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð» / Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ) -> use obstril.png
        # This should have priority over FPV cities when explicit shelling threat is mentioned
        if 'Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»' in l or 'Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»' in l or 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ' in l or 'ÑƒÐ³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»Ð°' in l:
            # print(f"[CLASSIFY DEBUG] Classified as artillery")
            return 'artillery', 'obstril.png'
        
        # Special override for specific cities - Kherson, Nikopol, Marhanets always get FPV icon
        city_lower = city_context.lower() if city_context else ""
        fpv_cities = ['Ñ…ÐµÑ€ÑÐ¾Ð½', 'Ð½Ð¸ÐºÐ¾Ð¿Ð¾Ð»ÑŒ', 'Ð½Ñ–ÐºÐ¾Ð¿Ð¾Ð»ÑŒ', 'Ð¼Ð°Ñ€Ð³Ð°Ð½ÐµÑ†ÑŒ', 'Ð¼Ð°Ñ€Ð³Ð°Ð½ÐµÑ†']
        
        # Check both city context and message text for FPV cities
        if any(fpv_city in city_lower for fpv_city in fpv_cities) or any(fpv_city in l for fpv_city in fpv_cities):
            return 'fpv', 'fpv.png'
        # Recon / Ñ€Ð¾Ð·Ð²Ñ–Ð´ Ð´Ñ€Ð¾Ð½Ð¸ -> use pvo icon (rozved.png) per user request - PRIORITY: check BEFORE general Ð‘ÐŸÐ›Ð
        if 'Ñ€Ð¾Ð·Ð²Ñ–Ð´' in l or 'Ñ€Ð¾Ð·Ð²Ñ–Ð´ÑƒÐ²Ð°Ð»ÑŒ' in l or 'Ñ€Ð°Ð·Ð²ÐµÐ´' in l:
            return 'rozved', 'rozved.png'
        # PRIORITY: ÐšÐÐ‘Ñ‹ (ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼Ñ‹Ðµ Ð°Ð²Ð¸Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð±Ð¾Ð¼Ð±Ñ‹) -> rszv.png - check BEFORE Ð¿ÑƒÑÐºÐ¸ to avoid misclassification
        if any(k in l for k in ['ÐºÐ°Ð±','kab','ÑƒÐ¼Ð¿Ðº','umpk','Ð¼Ð¾Ð´ÑƒÐ»ÑŒ','fab','ÑƒÐ¼Ð¿Ð±','Ñ„Ð°Ð±','ÐºÐ°Ð±Ñƒ']) or \
           ('Ð°Ð²Ñ–Ð°Ñ†Ñ–Ð¹Ð½' in l and 'Ð±Ð¾Ð¼Ð±' in l) or ('ÐºÐµÑ€Ð¾Ð²Ð°Ð½' in l and 'Ð±Ð¾Ð¼Ð±' in l):
            return 'kab', 'rszv.png'
        # Launch site detections for Shahed / UAV launches ("Ð¿ÑƒÑÐºÐ¸" + origin phrases). User wants pusk.png marker.
        # Exclude ÐšÐÐ‘ launches - they should be classified as ÐšÐÐ‘, not Ð¿ÑƒÑÐºÐ¸
        if ('Ð¿ÑƒÑÐº' in l or 'Ð¿ÑƒÑÐºÐ¸' in l) and (any(k in l for k in ['shahed','ÑˆÐ°Ñ…ÐµÐ´','ÑˆÐ°Ñ…ÐµÐ´Ñ–','ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²','Ð±Ð¿Ð»Ð°','uav','Ð´Ñ€Ð¾Ð½']) or ('Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼' in l) or ('Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼' in l)) and not any(k in l for k in ['ÐºÐ°Ð±','kab','ÑƒÐ¼Ð¿Ðº','fab','Ñ„Ð°Ð±']):
            return 'pusk', 'pusk.png'
        # Explicit launches from occupied Berdyansk airbase (Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ) should also show as pusk (not avia)
        if ('Ð¿ÑƒÑÐº' in l or 'Ð¿ÑƒÑÐºÐ¸' in l) and 'Ð±ÐµÑ€Ð´ÑÐ½' in l and ('Ð°Ð²Ñ–Ð°Ð±Ð°Ð·' in l or 'Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼' in l or 'Ð°Ð²Ð¸Ð°Ð±Ð°Ð·' in l):
            return 'pusk', 'pusk.png'
        # Air alarm start
        if ('Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°' in l or 'Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°.' in l or ('Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°' in l and 'Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€' in l)) and not ('Ð²Ñ–Ð´Ð±Ñ–Ð¹' in l or 'Ð¾Ñ‚Ð±Ð¾Ð¹' in l):
            return 'alarm', 'trivoga.png'
        # Air alarm cancellation
        if ('Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸' in l) or ('Ð¾Ñ‚Ð±Ð¾Ð¹ Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ð¸' in l):
            return 'alarm_cancel', 'vidboi.png'
        # Explosions reporting -> vibuh icon (cover broader fixation phrases)
        if ('Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÑÑŽÑ‚ÑŒ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…' in l or 'Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð¾ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…' in l or 'Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…' in l or 'Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…Ð¸' in l
            or 'Ñ„Ñ–ÐºÑÐ°Ñ†Ñ–Ñ Ð²Ð¸Ð±ÑƒÑ…' in l or 'Ñ„Ñ–ÐºÑÑƒÑŽÑ‚ÑŒ Ð²Ð¸Ð±ÑƒÑ…' in l or re.search(r'\b(Ð²Ð¸Ð±ÑƒÑ…|Ð²Ð¸Ð±ÑƒÑ…Ð¸|Ð²Ð¸Ð±ÑƒÑ…Ñ–Ð²)\b', l)):
            return 'vibuh', 'vibuh.png'
        # Alarm cancellation (Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸ / Ð¾Ñ‚Ð±Ð¾Ð¹ Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ð¸)
        if ('Ð²Ñ–Ð´Ð±Ñ–Ð¹' in l and 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³' in l) or ('Ð¾Ñ‚Ð±Ð¾Ð¹' in l and 'Ñ‚Ñ€ÐµÐ²Ð¾Ð³' in l):
            print(f"[CLASSIFY DEBUG] Classified as alarm_cancel")
            return 'alarm_cancel', 'vidboi.png'
        
        # PRIORITY: High-speed targets / missile threats with rocket emoji (ðŸš€) -> raketa.png
        # This should have priority over drones to handle missile-like threats with rocket emoji
        if 'ðŸš€' in th or any(k in l for k in ['Ñ†Ñ–Ð»ÑŒ','Ñ†Ñ–Ð»ÐµÐ¹','Ñ†Ñ–Ð»Ñ–','Ð²Ð¸ÑÐ¾ÐºÐ¾ÑˆÐ²Ð¸Ð´ÐºÑ–ÑÐ½','high-speed']):
            print(f"[CLASSIFY DEBUG] Classified as raketa (high-speed targets/rocket emoji)")
            return 'raketa', 'raketa.png'
            
        # PRIORITY: drones (Ñ‡Ð°ÑÑ‚Ð°Ñ Ð¿ÑƒÑ‚Ð°Ð½Ð¸Ñ†Ð°). Ð•ÑÐ»Ð¸ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÐ»Ð¾Ð²Ð° ÑˆÐ°Ñ…ÐµÐ´/Ð±Ð¿Ð»Ð°/Ð´Ñ€Ð¾Ð½ -> ÑÑ‚Ð¾ shahed
        if any(k in l for k in ['shahed','ÑˆÐ°Ñ…ÐµÐ´','ÑˆÐ°Ñ…ÐµÐ´Ñ–','ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²','geran','Ð³ÐµÑ€Ð°Ð½ÑŒ','Ð´Ñ€Ð¾Ð½','Ð´Ñ€Ð¾Ð½Ð¸','Ð±Ð¿Ð»Ð°','uav']):
            print(f"[CLASSIFY DEBUG] Classified as shahed (drones/UAV)")
            return 'shahed', 'shahed.png'
        # PRIORITY: Aircraft activity & tactical aviation (avia) -> avia.png (jets, tactical aviation, Ð½Ð¾ Ð‘Ð•Ð— ÐšÐÐ‘Ð¾Ð²)
        if any(k in l for k in ['Ð»Ñ–Ñ‚Ð°Ðº','ÑÐ°Ð¼Ð¾Ð»','avia','tactical','Ñ‚Ð°ÐºÑ‚Ð¸Ñ‡Ð½','fighter','Ð¸ÑÑ‚Ñ€ÐµÐ±Ð¸Ñ‚','jets']) or \
           ('Ð°Ð²Ñ–Ð°Ñ†Ñ–Ð¹Ð½' in l and ('Ð·Ð°ÑÐ¾Ð±' in l or 'ÑƒÑ€Ð°Ð¶' in l)):
            return 'avia', 'avia.png'
        # Rocket / missile attacks (Ñ€Ð°ÐºÐµÑ‚Ð°, Ñ€Ð°ÐºÐµÑ‚Ð¸) -> raketa.png
        if any(k in l for k in ['Ñ€Ð°ÐºÐµÑ‚','rocket','Ð¼Ñ–Ð¶ÐºÐ¾Ð½Ñ‚Ð¸Ð½ÐµÐ½Ñ‚','Ð¼ÐµÐ¶ÐºÐ¾Ð½Ñ‚Ð¸Ð½ÐµÐ½Ñ‚','Ð±Ð°Ð»Ñ–ÑÑ‚Ð¸Ñ‡Ð½','ÐºÑ€Ð¸Ð»Ð°Ñ‚','cruise']):
            return 'raketa', 'raketa.png'
        # Ð Ð¡Ð—Ð’ (MLRS, Ð³Ñ€Ð°Ð´, ÑƒÑ€Ð°Ð³Ð°Ð½, ÑÐ¼ÐµÑ€Ñ‡) -> rszv.png
        if any(k in l for k in ['Ñ€ÑÐ·Ð²','mlrs','Ð³Ñ€Ð°Ð´','ÑƒÑ€Ð°Ð³Ð°Ð½','ÑÐ¼ÐµÑ€Ñ‡','Ñ€ÑÑƒÐ²','tÐ¾Ñ€','tÐ¾Ñ€Ð½Ð°Ð´Ð¾','Ñ‚Ð¾Ñ€Ð½Ð°Ð´Ð¾']):
            return 'rszv', 'rszv.png'
        # Korabel (naval/ship-related threats) -> korabel.png
        if any(k in l for k in ['ÐºÐ¾Ñ€Ð°Ð±ÐµÐ»','Ñ„Ð»Ð¾Ñ‚','ÐºÐ¾Ñ€Ð°Ð±Ð»','ship','fleet','Ð¼Ð¾Ñ€ÑÑŒÐº','naval']):
            return 'korabel', 'korabel.png'
        # Artillery
        if any(k in l for k in ['Ð°Ñ€Ñ‚','artillery','Ð³Ð°Ñ€Ð¼Ð°Ñ‚','Ð³Ð°ÑƒÐ±Ð¸Ñ†','Ð¼Ð¸Ð½Ð¾Ð¼ÐµÑ‚Ð½','howitzer']):
            return 'artillery', 'artillery.png'
        # PVO (air defense activity) -> pvo.png
        if any(k in l for k in ['Ð¿Ð¿Ð¾','pvo','defense','Ð¾Ð±Ð¾Ñ€Ð¾Ð½','Ð·ÐµÐ½Ñ–Ñ‚Ð½','Ñ-','patriot']):
            return 'pvo', 'pvo.png'
        # Naval mines -> neptun
        if any(k in l for k in ['Ð¼Ñ–Ð½Ð°','Ð¼Ñ–Ð½ ','mine','neptun','Ð½ÐµÐ¿Ñ‚ÑƒÐ½','Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾ÐºÐ¾Ñ€Ð°Ð±ÐµÐ»']):
            return 'neptun', 'neptun.jpg'
        # FPV drones -> fpv.png
        if any(k in l for k in ['fpv','Ñ„Ð¿Ð²','ÐºÐ°Ð¼Ñ–ÐºÐ°Ð´Ð·','kamikaze']):
            print(f"[CLASSIFY DEBUG] Classified as fpv")
            return 'fpv', 'fpv.png'
        # General fallback for unclassified threats
        print(f"[CLASSIFY DEBUG] Using default fallback: shahed")
        return 'shahed', 'shahed.png'  # default fallback
    
    # PRIORITY CHECK: District-level UAV messages (e.g., "Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´ÑÑŒÐºÐ¸Ð¹ Ñ€-Ð½ ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».")
    # Added after classify function to ensure it's available
    lower_text = original_text.lower()
    district_pattern = re.compile(r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+ÑÑŒÐºÐ¸Ð¹|[Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+Ñ†ÑŒÐºÐ¸Ð¹)\s+Ñ€[-\s]*Ð½\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+(?:Ð¾Ð±Ð»\.?|Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ|Ñ‰Ð¸Ð½Ð°))', re.IGNORECASE)
    district_match = district_pattern.search(lower_text)
    
    if district_match and 'Ð±Ð¿Ð»Ð°' in lower_text:
        district_raw = district_match.group(1).strip()
        region_raw = district_match.group(2).strip()
        
        add_debug_log(f"DISTRICT UAV: found '{district_raw} Ñ€-Ð½ {region_raw}'", "district_uav")
        
        # Try to map district to city coordinates
        district_city = district_raw.replace('ÑÑŒÐºÐ¸Ð¹', '').replace('Ñ†ÑŒÐºÐ¸Ð¹', '').strip()
        
        # Check if we have coordinates for this district city
        coords = CITY_COORDS.get(district_city)
        if not coords and district_city in UA_CITY_NORMALIZE:
            coords = CITY_COORDS.get(UA_CITY_NORMALIZE[district_city])
        
        if coords:
            lat, lng = coords
            threat_type, icon = classify(original_text, district_city)
            
            # Create district-level marker
            district_track = {
                'id': f"{mid}_district",
                'place': f"{district_city.title()} ({district_raw} Ñ€-Ð½)",
                'lat': lat,
                'lng': lng,
                'threat_type': threat_type,
                'text': original_text[:500],
                'date': date_str,
                'channel': channel,
                'marker_icon': icon,
                'source_match': 'district_priority_uav',
                'count': 1
            }
            
            add_debug_log(f"DISTRICT UAV SUCCESS: {district_city} -> {coords}", "district_uav")
            return [district_track]
        else:
            add_debug_log(f"DISTRICT UAV: No coords for '{district_city}'", "district_uav")
    
    # PRIORITY CHECK: Single-region numbered UAV lists (Ð½.Ð¿. patterns)
    # For messages like "ÐšÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°:\nâ€¢ Ð½.Ð¿. Ð‘Ñ€Ð¾Ð²Ð°Ñ€Ð¸ - Ð¿Ð¾ÑÑ‚Ñ–Ð¹Ð½Ð° Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð‘Ð¿Ð›Ð"
    lower_text = original_text.lower()
    text_lines = original_text.split('\n')
    
    # Check if this is a single-region message with numbered Ð½.Ð¿. cities
    region_lines = [line for line in text_lines if any(region in line.lower() for region in ['Ñ‰Ð¸Ð½Ð°:', 'Ñ‰Ð¸Ð½Ð°]', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ:']) and line.strip().endswith(':')]
    np_lines = [line for line in text_lines if ('Ð½.Ð¿.' in line.lower() or 'Ð½. Ð¿.' in line.lower()) and 'Ð±Ð¿Ð»Ð°' in line.lower()]
    
    if len(region_lines) == 1 and len(np_lines) >= 1:  # Single region with Ð½.Ð¿. cities
        region_line = region_lines[0]
        region_name = region_line.replace(':', '').strip()
        
        add_debug_log(f"SINGLE-REGION NUMBERED: found {len(np_lines)} Ð½.Ð¿. cities in {region_name}", "single_region_numbered")
        
        numbered_tracks = []
        for i, line in enumerate(np_lines):
            # Extract city name from Ð½.Ð¿. pattern
            np_match = re.search(r'Ð½\.?\s*Ð¿\.?\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)', line.lower())
            if np_match:
                city_name_raw = np_match.group(1).strip()
                # Clean up - take only the city name before any separators
                city_name = city_name_raw.split(' - ')[0].split(' â€“')[0].split(' ')[0].strip()
                
                # Try to find coordinates for this city
                coords = CITY_COORDS.get(city_name)
                if not coords and city_name in UA_CITY_NORMALIZE:
                    coords = CITY_COORDS.get(UA_CITY_NORMALIZE[city_name])
                
                if coords:
                    lat, lng = coords
                    threat_type, icon = classify(line, city_name)
                    
                    numbered_tracks.append({
                        'id': f"{mid}_np_{i+1}",
                        'place': f"{city_name.title()} ({region_name})",
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': line[:500],
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': 'single_region_numbered_np',
                        'count': 1
                    })
                    add_debug_log(f"NUMBERED UAV SUCCESS: {city_name} -> {coords}", "single_region_numbered")
                else:
                    # Fallback to region coordinates if city not found
                    region_key = region_name.lower().replace('Ñ‰Ð¸Ð½Ð°', '').replace('Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', '').strip()
                    region_coords = CITY_COORDS.get(region_key)
                    if region_coords:
                        lat, lng = region_coords
                        threat_type, icon = classify(line)
                        
                        numbered_tracks.append({
                            'id': f"{mid}_np_fallback_{i+1}",
                            'place': f"{region_name} (Ð½.Ð¿. {city_name.title()})",
                            'lat': lat,
                            'lng': lng,
                            'threat_type': threat_type,
                            'text': line[:500],
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': icon,
                            'source_match': 'single_region_numbered_np_fallback',
                            'count': 1
                        })
                        add_debug_log(f"NUMBERED UAV FALLBACK: {city_name} -> {region_name} {region_coords}", "single_region_numbered")
        
        if numbered_tracks:
            add_debug_log(f"SINGLE-REGION NUMBERED SUCCESS: {len(numbered_tracks)} markers created", "single_region_numbered")
            return numbered_tracks

    # HIGHEST PRIORITY: Check for region-district patterns immediately
    import re as _re_priority
    region_district_pattern = _re_priority.compile(r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð°ÑƒÐ¸]?)\s*\(\s*([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)\s+Ñ€[-\s]*Ð½\)', _re_priority.IGNORECASE)
    region_district_match = region_district_pattern.search(original_text)
    
    if region_district_match:
        region_raw, district_raw = region_district_match.groups()
        target_city = district_raw.strip()
        
        add_debug_log(f"PRIORITY REGION-DISTRICT pattern FOUND: region='{region_raw}', district='{district_raw}'", "priority_region_district")
        
        # Normalize city name and try to find coordinates
        city_norm = target_city.lower()
        # Apply UA_CITY_NORMALIZE rules if available
        if 'UA_CITY_NORMALIZE' in globals():
            city_norm = UA_CITY_NORMALIZE.get(city_norm, city_norm)
        coords = CITY_COORDS.get(city_norm)
        
        add_debug_log(f"Priority district city lookup: '{target_city}' -> '{city_norm}' -> {coords}", "priority_region_district")
        
        if coords:
            lat, lng = coords
            threat_type, icon = classify(original_text)
            
            priority_result = [{
                'id': f"{mid}_priority_district",
                'place': target_city.title(),
                'lat': lat,
                'lng': lng,
                'threat_type': threat_type,
                'text': original_text[:500],
                'date': date_str,
                'channel': channel,
                'marker_icon': icon,
                'source_match': 'priority_region_district',
                'count': 1
            }]
            add_debug_log(f"Created PRIORITY region-district marker: {target_city.title()}", "priority_region_district")
            
            # Store priority result globally for combination with other results
            globals()['_current_priority_result'] = priority_result
            
            # Store priority result and continue with normal processing to catch other cities
            # This allows other parsers to find additional cities in the same message
        else:
            add_debug_log(f"No coordinates found for priority district city: '{target_city}' (normalized: '{city_norm}')", "priority_region_district")
            priority_result = None
    else:
        priority_result = None

    # Ð’Ð¡Ð•Ð“Ð”Ð Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÐºÐ°Ð¶Ð´Ð¾Ðµ Ð²Ñ…Ð¾Ð´ÑÑ‰ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸
    try:
        add_debug_log(f"process_message called - mid={mid}, channel={channel}, text_length={len(text or '')}", "message_processing")
        add_debug_log(f"message text preview: {(text or '')[:200]}...", "message_processing")
        # Check if this is our test message
        if 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°' in (text or '').lower() and 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°' in (text or '').lower():
            add_debug_log("MULTI-REGION MESSAGE DETECTED!", "multi_region")
            add_debug_log(f"Full text: {text}", "multi_region")
    except Exception:
        pass

    # PRIORITY: Handle emoji + city + oblast format BEFORE any other processing
    try:
        import re  # Import re module for pattern matching
        head = text.split('\n', 1)[0][:160] if text else ""
        
        # Handle general emoji + city + oblast format with any UAV threat (more flexible pattern)
        general_emoji_pattern = r'^[^\w\s]*\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)\s*\(([^)]*Ð¾Ð±Ð»[^)]*)\)'
        general_emoji_match = re.search(general_emoji_pattern, head, re.IGNORECASE)
        add_debug_log(f"PRIORITY: Testing general emoji pattern on head: {repr(head)}", "emoji_debug")
        add_debug_log(f"PRIORITY: General emoji match result: {general_emoji_match}", "emoji_debug")
        
        if general_emoji_match and any(uav_word in text.lower() for uav_word in ['Ð±Ð¿Ð»Ð°', 'Ð´Ñ€Ð¾Ð½', 'ÑˆÐ°Ñ…ÐµÐ´', 'Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ', 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°', 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°', 'Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»', 'Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»']):
            city_from_general = general_emoji_match.group(1).strip()
            oblast_from_general = general_emoji_match.group(2).strip()
            add_debug_log(f"PRIORITY: Found city: {repr(city_from_general)}, oblast: {repr(oblast_from_general)}", "emoji_debug")
            
            if city_from_general and 2 <= len(city_from_general) <= 40:
                base = city_from_general.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                base = re.sub(r'\s+',' ', base)
                norm = UA_CITY_NORMALIZE.get(base, base)
                
                # First try to find city+oblast specific coordinates
                oblast_key = oblast_from_general.lower()
                coords = None
                
                # Try different lookup strategies for city+oblast disambiguation
                if 'ÑÑƒÐ¼' in oblast_key and norm == 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°':
                    coords = (51.5667, 34.1333)  # ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°, Ð¡ÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
                    add_debug_log(f"PRIORITY: Using specific coordinates for ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° (Ð¡ÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð».): {coords}", "emoji_debug")
                elif 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐº' in oblast_key and norm == 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°':
                    coords = (47.0667, 31.8333)  # ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°, ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ
                    add_debug_log(f"PRIORITY: Using specific coordinates for ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ° (ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».): {coords}", "emoji_debug")
                # Handle districts by mapping to their administrative centers
                elif 'Ñ€Ð°Ð¹Ð¾Ð½' in norm:
                    if 'ÑÐ¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½' in norm:
                        coords = CITY_COORDS.get('ÑÐ¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²Ðµ')  # Ð¡Ð¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²Ðµ - Ñ†ÐµÐ½Ñ‚Ñ€ Ñ€Ð°Ð¹Ð¾Ð½Ñƒ
                        add_debug_log(f"PRIORITY: Mapping Ð¡Ð¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ -> Ð¡Ð¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²Ðµ: {coords}", "emoji_debug")
                    elif 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½' in norm and 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐº' in oblast_key:
                        coords = CITY_COORDS.get('Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²')  # ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð² - Ñ†ÐµÐ½Ñ‚Ñ€ Ñ€Ð°Ð¹Ð¾Ð½Ñƒ
                        add_debug_log(f"PRIORITY: Mapping ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ -> ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²: {coords}", "emoji_debug")
                    # For other districts, try to find coordinates in DISTRICT_CENTERS first
                    else:
                        # Extract district name without 'Ñ€Ð°Ð¹Ð¾Ð½' suffix
                        district_name = norm.replace('Ñ€Ð°Ð¹Ð¾Ð½', '').strip()
                        district_coords = DISTRICT_CENTERS.get(district_name)
                        if district_coords:
                            coords = district_coords
                            add_debug_log(f"PRIORITY: Found district coordinates for '{district_name}': {coords}", "emoji_debug")
                
                if not coords:
                    # Fallback to general lookup
                    coords = CITY_COORDS.get(norm)
                    add_debug_log(f"PRIORITY: General lookup: base={repr(base)}, norm={repr(norm)}, coords={coords}", "emoji_debug")
                
                if not coords and 'SETTLEMENTS_INDEX' in globals():
                    idx_map = globals().get('SETTLEMENTS_INDEX') or {}
                    coords = idx_map.get(norm)
                if coords:
                    lat, lon = coords[:2]
                    
                    # Check for threat cancellation BEFORE creating marker
                    text_lower = text.lower()
                    if ('Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸' in text_lower or 
                        'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸' in text_lower or
                        ('Ð²Ñ–Ð´Ð±Ñ–Ð¹' in text_lower and any(cancel_word in text_lower for cancel_word in ['Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸', 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸']))):
                        # This is a cancellation message - create list_only entry, no map marker
                        track = {
                            'id': f"{mid}_priority_emoji_cancel_{city_from_general.replace(' ','_')}",
                            'place': city_from_general.title(),
                            'threat_type': 'alarm_cancel',
                            'text': clean_text(text)[:500], 
                            'date': date_str, 
                            'channel': channel,
                            'list_only': True,  # NO map marker for cancellation
                            'source_match': 'priority_emoji_cancel'
                        }
                        add_debug_log(f'PRIORITY CANCELLATION: {city_from_general} -> list_only=True (no marker)', "emoji_debug")
                        return [track]  # Early return - cancellation handled
                    
                    # Regular threat - create map marker
                    threat_type, icon = classify(text, city_from_general)
                    track = {
                        'id': f"{mid}_priority_emoji_{city_from_general.replace(' ','_')}",
                        'place': city_from_general.title(),
                        'lat': lat, 'lng': lon,
                        'threat_type': threat_type,
                        'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'priority_emoji_threat'
                    }
                    add_debug_log(f'PRIORITY EARLY RETURN: {city_from_general} -> {coords} -> {icon}', "emoji_debug")
                    return [track]  # Early return - highest priority
    except Exception as e:
        add_debug_log(f"PRIORITY emoji processing error: {e}", "emoji_debug")

    # PRIORITY: Handle emoji + oblast format (when only oblast is specified, place marker in regional center)
    try:
        import re  # Import re module for pattern matching
        head = text.split('\n', 1)[0][:160] if text else ""
        
        # Handle emoji + oblast format (e.g. "ðŸ‘ï¸ ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».")
        oblast_emoji_pattern = r'^[^\w\s]*\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]*Ð¾Ð±Ð»\.?)\s*\*\*'
        oblast_emoji_match = re.search(oblast_emoji_pattern, head, re.IGNORECASE)
        add_debug_log(f"PRIORITY: Testing oblast emoji pattern on head: {repr(head)}", "emoji_debug")
        add_debug_log(f"PRIORITY: Oblast emoji match result: {oblast_emoji_match}", "emoji_debug")
        
        if oblast_emoji_match and any(uav_word in text.lower() for uav_word in ['Ð±Ð¿Ð»Ð°', 'Ð´Ñ€Ð¾Ð½', 'ÑˆÐ°Ñ…ÐµÐ´', 'Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ', 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°', 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°']):
            oblast_from_emoji = oblast_emoji_match.group(1).strip()
            add_debug_log(f"PRIORITY: Found oblast from emoji: {repr(oblast_from_emoji)}", "emoji_debug")
            
            # Map oblast to regional center
            regional_center = None
            coords = None
            
            oblast_key = oblast_from_emoji.lower()
            if 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐº' in oblast_key:
                regional_center = 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²'
                coords = CITY_COORDS.get('Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²')
            elif 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐº' in oblast_key:
                regional_center = 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾'
                coords = CITY_COORDS.get('Ð´Ð½Ñ–Ð¿Ñ€Ð¾')
            elif 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐº' in oblast_key:
                regional_center = 'Ð¥Ð°Ñ€ÐºÑ–Ð²'
                coords = CITY_COORDS.get('Ñ…Ð°Ñ€ÐºÑ–Ð²')
            elif 'ÑÑƒÐ¼ÑÑŒÐº' in oblast_key:
                regional_center = 'Ð¡ÑƒÐ¼Ð¸'
                coords = CITY_COORDS.get('ÑÑƒÐ¼Ð¸')
            elif 'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐº' in oblast_key:
                regional_center = 'Ð¥ÐµÑ€ÑÐ¾Ð½'
                coords = CITY_COORDS.get('Ñ…ÐµÑ€ÑÐ¾Ð½')
            elif 'Ð¾Ð´ÐµÑÑŒÐº' in oblast_key:
                regional_center = 'ÐžÐ´ÐµÑÐ°'
                coords = CITY_COORDS.get('Ð¾Ð´ÐµÑÐ°')
            elif 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐº' in oblast_key:
                regional_center = 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ'
                coords = CITY_COORDS.get('Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ')
            elif 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐº' in oblast_key:
                regional_center = 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°'
                coords = CITY_COORDS.get('Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°')
            
            add_debug_log(f"PRIORITY: Oblast {oblast_from_emoji} -> regional center {regional_center} -> coords {coords}", "emoji_debug")
            
            if coords and regional_center:
                lat, lon = coords[:2]
                threat_type, icon = classify(text)
                track = {
                    'id': f"{mid}_priority_oblast_{regional_center.replace(' ','_')}",
                    'place': regional_center,
                    'lat': lat, 'lng': lon,
                    'threat_type': threat_type,
                    'text': text[:160], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'priority_oblast_threat'
                }
                add_debug_log(f'PRIORITY OBLAST EARLY RETURN: {oblast_from_emoji} -> {regional_center} -> {coords} -> {icon}', "emoji_debug")
                return [track]  # Early return - highest priority
    except Exception as e:
        add_debug_log(f"PRIORITY oblast processing error: {e}", "emoji_debug")
    
    # Continue with existing logic...
    
    # Strip embedded links (Markdown [text](url) or raw URLs) while keeping core message text.
    # Requested: if message contains links, remove them but keep the rest.
    try:
        import re as _re_strip  # type: ignore
        if text:
            _orig_text = text
            # Remove markdown links [ ... ](http...). Keep the visible text (group 1) only.
            text = _re_strip.sub(r"\[([^\]]*)\]\((?:https?|tg|mailto)://[^)]+\)", lambda m: (m.group(1) or '').strip(), text)
            # Remove any residual raw URLs (http/https/t.me) leaving a single space
            text = _re_strip.sub(r"https?://\S+", " ", text)
            text = _re_strip.sub(r"t\.me/\S+", " ", text)
            # Remove lone decorative symbols (âœ™, â€¢, â˜…) that may have surrounded links
            text = _re_strip.sub(r"[âœ™â€¢â˜…]{1,}", " ", text)
            # Collapse multiple spaces and trim each line
            cleaned_lines = []
            for _ln in text.splitlines():
                _cl = ' '.join(_ln.split())
                if _cl:
                    cleaned_lines.append(_cl)
            if cleaned_lines:
                text = '\n'.join(cleaned_lines)
            else:
                # If stripping removed everything, fall back to original
                text = _orig_text
    except Exception:
        pass
    # Ensure original_text is defined early to avoid UnboundLocalError in early parsing branches
    original_text = text
    
    # Special handling for oblast+raion format: "Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½), ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½)"
    import re as _re_oblast
    oblast_raion_pattern = r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+ÑÑŒÐºÐ°\s+Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ)\s*\(([^)]*?Ñ€Ð°Ð¹Ð¾Ð½Ð¸?[^)]*?)\)'
    oblast_raion_matches = _re_oblast.findall(oblast_raion_pattern, text.lower(), _re_oblast.IGNORECASE)
    
    # Also check for pattern without requiring "Ñ€Ð°Ð¹Ð¾Ð½Ð¸" in parentheses - some messages might have just names
    if not oblast_raion_matches:
        oblast_raion_pattern_simple = r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+ÑÑŒÐºÐ°\s+Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ)\s*\(([^)]+)\)'
        oblast_raion_matches_simple = _re_oblast.findall(oblast_raion_pattern_simple, text.lower(), _re_oblast.IGNORECASE)
        # Filter to only those that contain district-like words
        oblast_raion_matches = [(oblast, raion) for oblast, raion in oblast_raion_matches_simple 
                               if any(word in raion for word in ['Ñ€Ð°Ð¹Ð¾Ð½', 'Ñ€-Ð½', 'ÑÑŒÐºÐ¸Ð¹', 'Ñ†ÑŒÐºÐ¸Ð¹'])]
    
    add_debug_log(f"Oblast+raion pattern check: found {len(oblast_raion_matches)} matches in text: {text[:200]}...", "oblast_raion")
    
    if oblast_raion_matches and any(word in text.lower() for word in ['Ð±Ð¿Ð»Ð°', 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°', 'ÑƒÐºÑ€Ð¸Ñ‚Ñ‚Ñ']):
        add_debug_log(f"Oblast+raion format detected: {oblast_raion_matches}", "oblast_raion")
        tracks = []
        
        for oblast_text, raion_text in oblast_raion_matches:
            add_debug_log(f"Processing oblast: '{oblast_text}', raion_text: '{raion_text}'", "oblast_raion")
            # Extract individual raions from the parentheses
            # Handle both single and multiple raions: "ÑÑƒÐ¼ÑÑŒÐºÐ¸Ð¹, ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½Ð¸"
            raion_parts = _re_oblast.split(r',\s*|\s+Ñ‚Ð°\s+', raion_text)
            add_debug_log(f"Split raion_parts: {raion_parts}", "oblast_raion")
            
            for raion_part in raion_parts:
                raion_part = raion_part.strip()
                if not raion_part:
                    continue
                    
                add_debug_log(f"Processing raion_part: '{raion_part}'", "oblast_raion")
                    
                # Extract raion name (remove "Ñ€Ð°Ð¹Ð¾Ð½"/"Ñ€Ð°Ð¹Ð¾Ð½Ð¸" suffix)
                raion_name = _re_oblast.sub(r'\s*(Ñ€Ð°Ð¹Ð¾Ð½Ð¸?|Ñ€-Ð½\.?).*$', '', raion_part).strip()
                add_debug_log(f"After removing suffix, raion_name: '{raion_name}'", "oblast_raion")
                
                # Normalize raion name
                raion_normalized = _re_oblast.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$', 'ÑÑŒÐºÐ¸Ð¹', raion_name)
                add_debug_log(f"Normalized raion: '{raion_normalized}', checking in RAION_FALLBACK", "oblast_raion")
                
                if raion_normalized in RAION_FALLBACK:
                    lat, lng = RAION_FALLBACK[raion_normalized]
                    add_debug_log(f"Creating oblast+raion marker: {raion_normalized} at {lat}, {lng}", "oblast_raion")
                    
                    # Use classify function to determine correct threat type and icon
                    threat_type, icon = classify(original_text, raion_normalized)
                    
                    tracks.append({
                        'id': f"{mid}_raion_{raion_normalized}",
                        'place': f"{raion_normalized.title()} Ñ€Ð°Ð¹Ð¾Ð½",
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': original_text[:500],
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': 'oblast_raion_format'
                    })
                else:
                    add_debug_log(f"Raion not found in RAION_FALLBACK: '{raion_normalized}'. Available keys: {list(RAION_FALLBACK.keys())[:10]}...", "oblast_raion")
        
        if tracks:
            add_debug_log(f"Returning {len(tracks)} oblast+raion markers", "oblast_raion")
            return tracks
    
    large_message_mode = False
    LARGE_THRESHOLD = 15000
    HARD_CUTOFF = 40000  # safety to avoid pathological regex backtracking
    parse_started_ts = time.perf_counter()
    if text and len(text) > LARGE_THRESHOLD:
        large_message_mode = True
        orig_len = len(text)
        if orig_len > HARD_CUTOFF:
            # Keep head + tail slices to retain some closing context
            head = text[:HARD_CUTOFF//2]
            tail = text[-2000:]
            text = head + "\n...TRUNCATED...\n" + tail
            log.debug(f"mid={mid} large_message_mode truncation {orig_len}->{len(text)} chars")
        else:
            log.debug(f"mid={mid} large_message_mode len={orig_len}")
        # Quick complexity metrics
        try:
            lc = text.lower()
            metrics = {
                'len': orig_len,
                'lines': text.count('\n') + 1,
                'bpla': lc.count('Ð±Ð¿Ð»Ð°'),
                'shahed': lc.count('ÑˆÐ°Ñ…ÐµÐ´'),
                'course': lc.count('ÐºÑƒÑ€Ñ'),
                'napr': lc.count('Ð½Ð°Ð¿Ñ€ÑÐ¼') + lc.count('Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ')
            }
            log.debug(f"mid={mid} large_msg_metrics {metrics}")
        except Exception:
            pass
        # Pre-scan chunk optimization: if many repeated 'Ð±Ð¿Ð»Ð° ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°', extract tokens fast before heavy regex blocks
        try:
            if text.lower().count('ÐºÑƒÑ€Ñ') > 8 and text.lower().count('Ð±Ð¿Ð»Ð°') > 8:
                fast_tokens = []
                for ln in text.split('\n'):
                    lnl = ln.lower()
                    if 'Ð±Ð¿Ð»Ð°' in lnl and 'ÐºÑƒÑ€Ñ' in lnl and ' Ð½Ð° ' in lnl:
                        # light-weight extraction (avoid complex backtracking)
                        # Regex: capture token after 'ÐºÑƒÑ€Ñ(Ð¾Ð¼) Ð½Ð°' up to 40 chars (letters, spaces, dashes)
                        m = re.search(r"ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-\s]{3,40})", lnl, re.IGNORECASE)
                        if m:
                            tok = m.group(1).strip()
                            if tok and tok not in fast_tokens:
                                fast_tokens.append(tok)
                if fast_tokens:
                    log.debug(f"mid={mid} pre-scan collected {len(fast_tokens)} fast_tokens (will still run main parser)")
        except Exception as _e_fast:
            log.debug(f"mid={mid} pre-scan error: {_e_fast}")
    # Early benign filter: city name + emojis / hearts without any threat keywords -> ignore
    try:
        lt = (text or '').lower().strip()
        if lt:
            # threat indicator tokens (broad stems)
            threat_tokens = (
                'ÑˆÐ°Ñ…ÐµÐ´','shahed','Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','Ñ€Ð°ÐºÐµÑ‚','ÐºÐ°Ð±','Ð²Ð¸Ð±ÑƒÑ…','Ð¿Ñ€Ð¸Ð»','ÑƒÐ´Ð°Ñ€','Ð·Ð°Ð³Ñ€Ð¾Ð·','Ñ‚Ñ€Ð¸Ð²Ð¾Ð³',
                'Ð¿ÑƒÑÐº','Ð·Ð»Ñ–Ñ‚','Ð·Ð»ÐµÑ‚','avia','Ð°Ð²Ñ–Ð°','Ð¿Ð²Ð¾','Ð¾Ð±ÑÑ‚Ñ€','mlrs','rszv','fpv','Ð°Ñ€Ñ‚Ð¸Ð»','Ð·ÐµÐ½Ñ–Ñ‚','Ð·ÐµÐ½Ð¸Ñ‚'
            )
            if not any(t in lt for t in threat_tokens):
                # strip emojis & symbols leaving letters, spaces and apostrophes
                import re as _re_benign
                core = _re_benign.sub(r"[^a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘'â€™Ê¼`\s-]","", lt)
                core = ' '.join(core.split())
                # If core matches exactly a known city (or its normalized form) and original text length small -> benign
                if 2 <= len(core) <= 30:
                    base = UA_CITY_NORMALIZE.get(core, core)
                    if base in CITY_COORDS or ('SETTLEMENTS_INDEX' in globals() and (globals().get('SETTLEMENTS_INDEX') or {}).get(base)):
                        # Ignore this message (no tracks)
                        add_debug_log(f"BENIGN FILTER blocked message mid={mid} - detected city name without threats: '{core}'", "filter")
                        return []
            # NEW suppression: reconnaissance-only notes ("Ð´Ð¾Ñ€Ð¾Ð·Ð²Ñ–Ð´ÐºÐ° Ð¿Ð¾ Ð‘Ð¿Ð›Ð") should not produce a marker
            # Pattern triggers if word 'Ð´Ð¾Ñ€Ð¾Ð·Ð²Ñ–Ð´Ðº' present together with UAV terms but no other threat verbs
            if 'Ð´Ð¾Ñ€Ð¾Ð·Ð²Ñ–Ð´Ðº' in lt and any(k in lt for k in ['Ð±Ð¿Ð»Ð°','shahed','ÑˆÐ°Ñ…ÐµÐ´','Ð´Ñ€Ð¾Ð½']):
                # Avoid suppressing if explosions or launches also present
                if not any(k in lt for k in ['Ð²Ð¸Ð±ÑƒÑ…','ÑƒÐ´Ð°Ñ€','Ð¿ÑƒÑÐº','Ð¿Ñ€Ð¸Ð»','Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»','Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»','Ð·Ð»Ñ–Ñ‚','Ð·Ð»ÐµÑ‚']):
                    add_debug_log(f"RECONNAISSANCE FILTER blocked message mid={mid} - reconnaissance only", "filter")
                    return []
    except Exception:
        pass
    
    # SPECIAL: Handle multiple threats in one message BEFORE other parsing
    def handle_multiple_threats():
        """Check for messages with multiple different threats and process each separately"""
        all_threats = []
        text_lower = text.lower()
        
        # 1. Check for northeast tactical aviation threat
        if ('Ñ‚Ð°ÐºÑ‚Ð¸Ñ‡Ð½' in text_lower or 'Ð°Ð²Ñ–Ð°Ñ†Ñ–' in text_lower or 'Ð°Ð²Ð¸Ð°Ñ†' in text_lower) and (
            'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in text_lower or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾ ÑÑ…Ñ–Ð´Ð½' in text_lower or 'ÑÐµÐ²ÐµÑ€Ð¾-Ð²Ð¾ÑÑ‚Ð¾Ñ‡' in text_lower or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ' in text_lower
        ):
            lat, lng = 50.9, 34.8  # Near Sumy city (in Ukrainian territory)
            all_threats.append({
                'id': f"{mid}_ne_multi", 'place': 'ÐŸÑ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹ Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº', 'lat': lat, 'lng': lng,
                'threat_type': 'avia', 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': 'avia.png', 'source_match': 'multiple_threats_northeast_aviation'
            })
        
        # 2. Check for reconnaissance UAV in Mykolaiv oblast (Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð¸/Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ñ–)
        if ('Ñ€Ð¾Ð·Ð²Ñ–Ð´' in text_lower or 'Ñ€Ð¾Ð·Ð²ÐµÐ´ÑƒÐ²Ð°Ð»ÑŒ' in text_lower) and ('Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð¸' in text_lower or 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ñ–' in text_lower or 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°' in text_lower):
            # Use Mykolaiv city coordinates
            lat, lng = 46.9750, 31.9946
            all_threats.append({
                'id': f"{mid}_mykolaiv_recon", 'place': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°', 'lat': lat, 'lng': lng,
                'threat_type': 'rozved', 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': 'rozved.png', 'source_match': 'multiple_threats_mykolaiv_recon'
            })
        
        # 3. Check for general Ð‘ÐŸÐ›Ð threats in oblast format (Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð¸/Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ñ–) without "Ñ€Ð¾Ð·Ð²Ñ–Ð´"
        elif ('Ð±Ð¿Ð»Ð°' in text_lower or 'Ð´Ñ€Ð¾Ð½' in text_lower) and ('Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð¸' in text_lower or 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ñ–' in text_lower or 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°' in text_lower):
            lat, lng = 46.9750, 31.9946
            all_threats.append({
                'id': f"{mid}_mykolaiv_uav", 'place': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°', 'lat': lat, 'lng': lng,
                'threat_type': 'shahed', 'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                'marker_icon': 'shahed.png', 'source_match': 'multiple_threats_mykolaiv_uav'
            })
        
        return all_threats

    # Check if this is a multi-threat message
    if 'ðŸ›¬' in text and 'ðŸ›¸' in text:
        multi_threats = handle_multiple_threats()
        if multi_threats:
            add_debug_log(f"MULTIPLE THREATS DETECTED: Found {len(multi_threats)} threats", "multi_threats")
            return multi_threats

    # EARLY CHECK: Multi-regional UAV messages (before other logic can interfere)
    text_lines = text.split('\n')
    region_count = sum(1 for line in text_lines if any(region in line.lower() for region in ['Ñ‰Ð¸Ð½Ð°:', 'Ñ‰Ð¸Ð½Ð°]', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ:', 'ÐºÑ€Ð°Ð¹:']) or (
        'Ñ‰Ð¸Ð½Ð°' in line.lower() and line.lower().strip().endswith(':')
    ))
    uav_count = sum(1 for line in text_lines if 'Ð±Ð¿Ð»Ð°' in line.lower() and ('ÐºÑƒÑ€Ñ' in line.lower() or 'Ð½Ð° ' in line.lower()))
    
    if region_count >= 2 and uav_count >= 3:
        add_debug_log(f"EARLY MULTI-REGIONAL UAV DETECTION: {region_count} regions, {uav_count} UAVs", "multi_regional")
        # We'll process this later when all functions are defined
        # Set a flag for now
        multi_regional_flag = True
    else:
        multi_regional_flag = False

    # ... existing parsing logic continues ...
    # At the very end of function (before return default) we'll log duration.
    # Air alarm region/raion tracking (start / cancel) before other parsing
    try:
        low_full = (text or '').lower()
        now_ep = time.time()
        lines = [l.strip() for l in (text or '').split('\n') if l.strip()][:3]
        if lines:
            header = lines[0].lower()
            header_norm = header.replace('Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¾Ð±Ð».').replace('Ð¾Ð±Ð»..','Ð¾Ð±Ð».')
            # Oblast alarm start: contains '<adj> Ð¾Ð±Ð».' and body has 'Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°'
            if ('Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€' in low_full or 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³' in low_full) and ' Ð¾Ð±Ð»' in header_norm:
                m_obl = re.search(r"([Ð°-ÑÑ–Ñ—Ñ”Ò‘\-']+?)\s+Ð¾Ð±Ð»\.?", header_norm)
                if m_obl:
                    stem = m_obl.group(1)
                    # Match against OBLAST_CENTERS keys
                    for k in OBLAST_CENTERS.keys():
                        if k.startswith(stem):
                            is_new = k not in ACTIVE_OBLAST_ALARMS
                            rec = ACTIVE_OBLAST_ALARMS.setdefault(k, {'since': now_ep, 'last': now_ep})
                            if rec['since'] > now_ep: rec['since'] = now_ep
                            rec['last'] = now_ep
                            persist_alarm('oblast', k, rec['since'], rec['last'])
                            if is_new:
                                log_alarm_event('oblast', k, 'start', now_ep)
                            break
            # Raion alarm start: '<name> Ñ€Ð°Ð¹Ð¾Ð½'
            if ('Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€' in low_full or 'Ñ‚Ñ€Ð¸Ð²Ð¾Ð³' in low_full) and ' Ñ€Ð°Ð¹Ð¾Ð½' in header:
                m_r = re.search(r"([Ð°-ÑÑ–Ñ—Ñ”Ò‘\-']+?)\s+Ñ€Ð°Ð¹Ð¾Ð½", header)
                if m_r:
                    rb = m_r.group(1).replace('â€™',"'").replace('Ê¼',"'")
                    if rb in RAION_FALLBACK:
                        is_new_r = rb not in ACTIVE_RAION_ALARMS
                        rec = ACTIVE_RAION_ALARMS.setdefault(rb, {'since': now_ep, 'last': now_ep})
                        if rec['since'] > now_ep: rec['since'] = now_ep
                        rec['last'] = now_ep
                        persist_alarm('raion', rb, rec['since'], rec['last'])
                        if is_new_r:
                            log_alarm_event('raion', rb, 'start', now_ep)
        # Cancellation lines contain 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸' or 'Ð¾Ñ‚Ð±Ð¾Ð¹ Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ð¸'
        if ('Ð²Ñ–Ð´Ð±Ñ–Ð¹' in low_full or 'Ð¾Ñ‚Ð±Ð¾Ð¹' in low_full) and ('Ñ‚Ñ€Ð¸Ð²Ð¾Ð³' in low_full or 'Ñ‚Ñ€ÐµÐ²Ð¾Ð³' in low_full):
            # Precise: look for explicit oblast adjectives endings '-ÑÑŒÐºÐ°', '-Ñ†ÑŒÐºÐ°', '-Ð½Ð¸Ñ†ÑŒÐºÐ°', etc.
            # Pattern: Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸ Ñƒ / Ð² <word...> Ð¾Ð±Ð»Ð°ÑÑ‚Ñ– OR '<adj> Ð¾Ð±Ð».'
            m_cancel_obl = re.findall(r"(\b[Ð°-ÑÑ–Ñ—Ñ”Ò‘\-']+?)(?:ÑÑŒÐºÐ°|Ñ†ÑŒÐºÐ°|Ð½Ð¸Ñ†ÑŒÐºÐ°|Ð·ÑŒÐºÐ°|Ð¶ÑÑŒÐºÐ°)\s+Ð¾Ð±Ð»(?:Ð°ÑÑ‚ÑŒ|\.|)", low_full)
            removed_any = False
            if m_cancel_obl:
                for stem in m_cancel_obl:
                    for k in list(ACTIVE_OBLAST_ALARMS.keys()):
                        if k.startswith(stem):
                            ACTIVE_OBLAST_ALARMS.pop(k, None); remove_alarm('oblast', k); log_alarm_event('oblast', k, 'cancel', now_ep); removed_any=True
            # Raion precise cancel: "Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð¸ Ñƒ <name> Ñ€Ð°Ð¹Ð¾Ð½Ñ–" (locative: -ÑÑŒÐºÐ¾Ð¼Ñƒ / -Ñ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ)
            m_cancel_r = re.findall(r"Ð²Ñ–Ð´Ð±Ñ–Ð¹[^\n]*?\b([Ð°-ÑÑ–Ñ—Ñ”Ò‘\-']+?)(?:ÑÑŒÐºÐ¾Ð¼Ñƒ|Ñ–Ð²ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÑŒÐºÐ¾Ð¼Ñƒ)\s+Ñ€Ð°Ð¹Ð¾Ð½Ñ–", low_full)
            if m_cancel_r:
                for stem in m_cancel_r:
                    for r in list(ACTIVE_RAION_ALARMS.keys()):
                        if r.startswith(stem):
                            ACTIVE_RAION_ALARMS.pop(r, None); remove_alarm('raion', r); log_alarm_event('raion', r, 'cancel', now_ep); removed_any=True
            # Fallback broad cancel if phrase generic and no explicit names matched
            if not removed_any and re.search(r"Ð²Ñ–Ð´Ð±Ñ–Ð¹\s+Ñ‚Ñ€Ð¸Ð²Ð¾Ð³|Ð¾Ñ‚Ð±Ð¾Ð¹\s+Ñ‚Ñ€ÐµÐ²Ð¾Ð³", low_full):
                # remove all (global Ð²Ñ–Ð´Ð±Ñ–Ð¹)
                for k in list(ACTIVE_OBLAST_ALARMS.keys()):
                    ACTIVE_OBLAST_ALARMS.pop(k, None); remove_alarm('oblast', k); log_alarm_event('oblast', k, 'cancel', now_ep)
                for r in list(ACTIVE_RAION_ALARMS.keys()):
                    ACTIVE_RAION_ALARMS.pop(r, None); remove_alarm('raion', r); log_alarm_event('raion', r, 'cancel', now_ep)
        # Expire stale
        ttl_cut = now_ep - APP_ALARM_TTL_MINUTES*60
        for dct in (ACTIVE_OBLAST_ALARMS, ACTIVE_RAION_ALARMS):
            for k in list(dct.keys()):
                if dct[k]['last'] < ttl_cut:
                    level = 'oblast' if dct is ACTIVE_OBLAST_ALARMS else 'raion'
                    dct.pop(k, None)
                    remove_alarm(level, k)
                    log_alarm_event(level, k, 'expire', now_ep)
    except Exception as _e_alarm:
        log.debug(f'alarm tracking block error: {_e_alarm}')
    # Early single-city (bold/emoji tolerant) parser
    try:
        orig = text
        head = orig.split('\n',1)[0][:160]
        
        # NEW: Handle emoji-prefixed threat messages like "ðŸ›¸ Ð—Ð²ÑÐ³ÐµÐ»ÑŒ (Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ° Ð¾Ð±Ð».) Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð‘ÐŸÐ›Ð"
        emoji_threat_pattern = r'^[^\w\s]*\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)\s*\([^)]*Ð¾Ð±Ð»[^)]*\)\s*Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°\s+Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ\s+Ð±Ð¿Ð»Ð°'
        emoji_match = re.search(emoji_threat_pattern, head, re.IGNORECASE)
        if emoji_match:
            city_from_emoji = emoji_match.group(1).strip()
            if city_from_emoji and 2 <= len(city_from_emoji) <= 40:
                base = city_from_emoji.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                base = re.sub(r'\s+',' ', base)
                norm = UA_CITY_NORMALIZE.get(base, base)
                coords = CITY_COORDS.get(norm)
                if not coords and 'SETTLEMENTS_INDEX' in globals():
                    idx_map = globals().get('SETTLEMENTS_INDEX') or {}
                    coords = idx_map.get(norm)
                if coords:
                    lat, lon = coords[:2]
                    threat_type, icon = classify(text)
                    track = {
                        'id': f"{mid}_emoji_threat_{city_from_emoji.replace(' ','_')}",
                        'place': city_from_emoji,
                        'lat': lat, 'lng': lon,
                        'threat_type': threat_type,
                        'text': clean_text(orig)[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'emoji_threat'
                    }
                    log.debug(f'Emoji threat parser: {city_from_emoji} -> {coords} -> {icon}')
                    return [track]  # Early return
        
        # NEW: Handle general emoji + city + oblast format with any UAV threat (more flexible pattern)
        general_emoji_pattern = r'^[^\w\s]*\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+)\s*\([^)]*Ð¾Ð±Ð»[^)]*\)'
        general_emoji_match = re.search(general_emoji_pattern, head, re.IGNORECASE)
        add_debug_log(f"Testing general emoji pattern on head: {repr(head)}", "emoji_debug")
        add_debug_log(f"General emoji match result: {general_emoji_match}", "emoji_debug")
        
        if general_emoji_match and any(uav_word in text.lower() for uav_word in ['Ð±Ð¿Ð»Ð°', 'Ð´Ñ€Ð¾Ð½', 'ÑˆÐ°Ñ…ÐµÐ´', 'Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ', 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°']):
            city_from_general = general_emoji_match.group(1).strip()
            add_debug_log(f"Found city from general emoji: {repr(city_from_general)}", "emoji_debug")
            
            if city_from_general and 2 <= len(city_from_general) <= 40:
                base = city_from_general.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                base = re.sub(r'\s+',' ', base)
                norm = UA_CITY_NORMALIZE.get(base, base)
                coords = CITY_COORDS.get(norm)
                add_debug_log(f"Looking up coordinates: base={repr(base)}, norm={repr(norm)}, coords={coords}", "emoji_debug")
                
                if not coords and 'SETTLEMENTS_INDEX' in globals():
                    idx_map = globals().get('SETTLEMENTS_INDEX') or {}
                    coords = idx_map.get(norm)
                if coords:
                    lat, lon = coords[:2]
                    threat_type, icon = classify(text)
                    track = {
                        'id': f"{mid}_general_emoji_{city_from_general.replace(' ','_')}",
                        'place': city_from_general.title(),
                        'lat': lat, 'lng': lon,
                        'threat_type': threat_type,
                        'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'general_emoji_threat'
                    }
                    add_debug_log(f'EARLY RETURN: General emoji threat parser: {city_from_general} -> {coords} -> {icon}', "emoji_debug")
                    return [track]  # Early return
        
        if '(' in head and ('Ð¾Ð±Ð»' in head.lower() or 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ' in head.lower()):
            import re as _re_early
            cleaned = head.replace('**','')
            for _zw in ('\u200b','\u200c','\u200d','\ufeff','\u2060','\u00a0'):
                cleaned = cleaned.replace(_zw,' ')
            cleaned = ' '.join(cleaned.split())
            cleaned = _re_early.sub(r'^[^A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘]+','', cleaned)
            # NEW: if pipe '|' separates multiple city headers in one line, attempt multi-city extraction here
            if '|' in cleaned:
                parts = [p.strip() for p in cleaned.split('|') if p.strip()]
                multi_tracks = []
                for idx, part in enumerate(parts, start=1):
                    if '(' not in part:
                        continue
                    par_pos = part.find('(')
                    if par_pos <= 1:
                        continue
                    city_candidate = part[:par_pos].strip()
                    if not (2 <= len(city_candidate) <= 40):
                        continue
                    try:
                        base = city_candidate.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace('â€™',"'").replace('`',"'")
                        base = _re_early.sub(r'\s+',' ', base)
                        norm = UA_CITY_NORMALIZE.get(base, base)
                        coords = CITY_COORDS.get(norm)
                        if not coords and 'SETTLEMENTS_INDEX' in globals():
                            idx_map = globals().get('SETTLEMENTS_INDEX') or {}
                            coords = idx_map.get(norm)
                        approx_flag = False
                        if not coords:
                            enriched = ensure_city_coords(norm)
                            if enriched:
                                if isinstance(enriched, tuple) and len(enriched) == 3:
                                    coords = (enriched[0], enriched[1])
                                    approx_flag = enriched[2]
                                else:
                                    coords = enriched
                        if not coords:
                            continue
                        # classification per segment
                        lseg = part.lower()
                        if any(ph in lseg for ph in ['Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÑÑŽÑ‚ÑŒ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð¾ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…Ð¸','Ñ„Ñ–ÐºÑÐ°Ñ†Ñ–Ñ Ð²Ð¸Ð±ÑƒÑ…','Ñ„Ñ–ÐºÑÑƒÑŽÑ‚ÑŒ Ð²Ð¸Ð±ÑƒÑ…',' Ð²Ð¸Ð±ÑƒÑ….',' Ð²Ð¸Ð±ÑƒÑ…Ð¸.']):
                            threat, icon = 'vibuh','vibuh.png'
                        elif 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð¾Ð±ÑÑ‚Ñ€' in lseg or 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ' in lseg or 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð±Ð¿Ð»Ð°' in lseg:
                            # treat as list-only cancellation fragment -> skip map marker for this part
                            multi_tracks.append({
                                'id': f"{mid}_p{idx}", 'text': part[:500], 'date': date_str, 'channel': channel,
                                'list_only': True, 'threat_type': 'alarm_cancel', 'place': city_candidate.title()
                            })
                            continue
                        elif 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð±Ð¿Ð»Ð°' in lseg or 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚' in lseg:
                            threat, icon = 'shahed','shahed.png'
                        elif 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ' in lseg or 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»Ð°' in lseg:
                            threat, icon = 'artillery','obstril.png'
                        else:
                            threat, icon = classify(part)
                        lat, lng = coords
                        track = {
                            'id': f"{mid}_p{idx}", 'place': city_candidate.title(), 'lat': lat, 'lng': lng,
                            'threat_type': threat, 'text': part[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'multi_city_pipe'
                        }
                        if approx_flag:
                            track['approx'] = True
                        multi_tracks.append(track)
                    except Exception:
                        continue
                # Return only if 2+ actual geo tracks (ignore if we only produced one, fall back to single-city logic)
                geo_count = sum(1 for t in multi_tracks if not t.get('list_only'))
                if geo_count >= 2:
                    return multi_tracks
            par = cleaned.find('(')
            if par > 1:
                city_candidate = cleaned[:par].strip()
                if 2 <= len(city_candidate) <= 40:
                    base = city_candidate.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace('â€™',"'").replace('`',"'")
                    base = _re_early.sub(r'\s+',' ', base)
                    norm = UA_CITY_NORMALIZE.get(base, base)
                    coords = CITY_COORDS.get(norm)
                    if not coords and 'SETTLEMENTS_INDEX' in globals():
                        idx = globals().get('SETTLEMENTS_INDEX') or {}
                        coords = idx.get(norm)
                    approx_flag = False
                    if not coords:
                        enriched = ensure_city_coords_with_message_context(norm, orig)
                        if enriched:
                            if isinstance(enriched, tuple) and len(enriched) == 3:
                                coords = (enriched[0], enriched[1])
                                approx_flag = enriched[2]
                            else:
                                coords = enriched
                    if coords:
                        l = orig.lower()
                        if any(ph in l for ph in ['Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÑÑŽÑ‚ÑŒ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð¾ Ð¿Ñ€Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…','Ð·Ð°Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ð¾ Ð²Ð¸Ð±ÑƒÑ…Ð¸','Ñ„Ñ–ÐºÑÐ°Ñ†Ñ–Ñ Ð²Ð¸Ð±ÑƒÑ…','Ñ„Ñ–ÐºÑÑƒÑŽÑ‚ÑŒ Ð²Ð¸Ð±ÑƒÑ…',' Ð²Ð¸Ð±ÑƒÑ….',' Ð²Ð¸Ð±ÑƒÑ…Ð¸.']):
                            threat, icon = 'vibuh','vibuh.png'
                        elif 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð¾Ð±ÑÑ‚Ñ€' in l or 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ' in l or 'Ð²Ñ–Ð´Ð±Ñ–Ð¹ Ð·Ð°Ð³Ñ€Ð¾Ð·Ð¸ Ð±Ð¿Ð»Ð°' in l:
                            # Treat city-level cancellation as list event, not a geo marker
                            return [{
                                'id': str(mid), 'text': orig[:500], 'date': date_str, 'channel': channel,
                                'list_only': True, 'threat_type': 'alarm_cancel', 'place': city_candidate.title()
                            }]
                        elif 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð±Ð¿Ð»Ð°' in l or 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚' in l:
                            threat, icon = 'shahed','shahed.png'
                        elif 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ' in l or 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»Ð°' in l:
                            threat, icon = 'artillery','obstril.png'
                        else:
                            threat, icon = classify(orig)
                        lat,lng = coords
                        track = {
                            'id': str(mid), 'place': city_candidate.title(), 'lat': lat, 'lng': lng,
                            'threat_type': threat, 'text': orig[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'single_city_simple_early'
                        }
                        if approx_flag:
                            track['approx'] = True
                        return [track]
    except Exception:
        pass

    
    # Directional multi-region (e.g. "Ð³Ñ€ÑƒÐ¿Ð° Ð‘Ð¿Ð›Ð Ð½Ð° Ð”Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ñ– ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñƒ") -> list-only, no fixed marker
    try:
        lorig = text.lower()
        # Skip if this message has route patterns (handled by route parser above)
        if 'Ñ‡ÐµÑ€ÐµÐ·' in lorig or 'Ð¿Ð¾Ð²Ð·' in lorig:
            pass
        elif (('ÐºÑƒÑ€Ñ' in lorig or 'âž¡' in lorig or 'â†’' in lorig or 'Ð½Ð°Ð¿Ñ€ÑÐ¼' in lorig) and ('Ð±Ð¿Ð»Ð°' in lorig or 'Ð´Ñ€Ð¾Ð½' in lorig or 'Ð³Ñ€ÑƒÐ¿' in lorig)) or ('Ð±Ð¿Ð»Ð°' in lorig and 'Ñ‡Ð°ÑÑ‚Ð¸Ð½' in lorig) or ('Ð´Ñ€Ð¾Ð½' in lorig and 'Ñ‡Ð°ÑÑ‚Ð¸Ð½' in lorig):
            # Special case: BPLA current location with directional info
            # e.g., "Ð‘Ð¿Ð›Ð Ð² Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ñ–Ð¹ Ñ‡Ð°ÑÑ‚Ð¸Ð½Ñ– ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð¸, ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° ÐšÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ñƒ"
            # or "Ð‘Ð¿Ð›Ð Ð² Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ñ–Ð¹ Ñ‡Ð°ÑÑ‚Ð¸Ð½Ñ– Ð¥Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð¸"
            import re as _re_loc
            
            # Look for current location patterns
            location_match = _re_loc.search(r'(?:Ð±Ð¿Ð»Ð°|Ð´Ñ€Ð¾Ð½[Ð¸Ð°]?)\s+(?:Ð²|Ð½Ð°|Ð½Ð°Ð´)\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\-\s]+(?:Ñ‡Ð°ÑÑ‚Ð¸Ð½[Ñ–Ð°]|Ñ€Ð°Ð¹Ð¾Ð½[Ñ–Ðµ]|Ð¾ÐºÑ€ÑƒÐ³[ÑƒÑ–])\s+[Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð¸Ð°ÑŽ])', lorig)
            if location_match:
                current_location = location_match.group(1).strip()
                print(f"DEBUG: Found current BPLA location: {current_location}")
                
                # Extract region from current location
                region_in_location = None
                for reg_key in OBLAST_CENTERS.keys():
                    if reg_key in current_location:
                        region_in_location = reg_key
                        break
                
                if region_in_location:
                    # Get region center and apply directional offset
                    region_coords = OBLAST_CENTERS.get(region_in_location, (50.0, 30.0))
                    
                    # Apply directional offset based on specified part of region
                    offset_lat, offset_lon = 0, 0
                    if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = 0.8, -0.8  # Northwest
                    elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = 0.8, 0.8   # Northeast
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = -0.8, -0.8 # Southwest
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = -0.8, 0.8  # Southeast
                    elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½' in current_location:
                        offset_lat, offset_lon = 0.8, 0     # North
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½' in current_location:
                        offset_lat, offset_lon = -0.8, 0    # South
                    elif 'Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = 0, -0.8    # West
                    elif 'ÑÑ…Ñ–Ð´Ð½' in current_location:
                        offset_lat, offset_lon = 0, 0.8     # East
                    elif 'Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½' in current_location:
                        offset_lat, offset_lon = 0, 0       # Center
                    
                    final_lat = region_coords[0] + offset_lat
                    final_lon = region_coords[1] + offset_lon
                    
                    # Clean up city name for display
                    region_name = region_in_location.replace('Ñ‰Ð¸Ð½Ð¸', 'Ñ‰Ð¸Ð½Ð°').replace('Ñ‰Ð¸Ð½Ñƒ', 'Ñ‰Ð¸Ð½Ð°')
                    direction_part = ""
                    if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "ÐŸÐ½-Ð—Ñ… "
                    elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "ÐŸÐ½-Ð¡Ñ… "
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "ÐŸÐ´-Ð—Ñ… "
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "ÐŸÐ´-Ð¡Ñ… "
                    elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½' in current_location:
                        direction_part = "ÐŸÐ½ "
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½' in current_location:
                        direction_part = "ÐŸÐ´ "
                    elif 'Ð·Ð°Ñ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "Ð—Ñ… "
                    elif 'ÑÑ…Ñ–Ð´Ð½' in current_location:
                        direction_part = "Ð¡Ñ… "
                    elif 'Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»ÑŒÐ½' in current_location:
                        direction_part = "Ð¦ÐµÐ½Ñ‚Ñ€ "
                    
                    display_name = f"{direction_part}{region_name.title()}"
                    
                    print(f"DEBUG: Location '{current_location}' in {region_in_location} -> ({final_lat}, {final_lon})")
                    
                    return [{
                        'id': str(mid), 'text': clean_text(text)[:600], 'date': date_str, 'channel': channel,
                        'lat': final_lat, 'lon': final_lon, 'city': display_name,
                        'source_match': 'trajectory_current_location'
                    }]
            
            # Quick reject if explicit single settlement in parentheses (handled elsewhere)
            if '(' not in lorig:
                present_regions = []
                for reg_key in OBLAST_CENTERS.keys():
                    if reg_key in lorig:
                        present_regions.append(reg_key)
                        if len(present_regions) >= 4:
                            break
                distinct = {r.split()[0] for r in present_regions}
                # Accept patterns like "Ð½Ð¾Ð²Ð° Ð³Ñ€ÑƒÐ¿Ð° ÑƒÐ´Ð°Ñ€Ð½Ð¸Ñ… Ð‘Ð¿Ð›Ð Ð½Ð° Ð”Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ñ– ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñƒ"
                # even if only 2 region stems present
                if len(distinct) >= 2:
                    # Check if message contains specific cities that should create markers instead
                    city_keywords = ['Ð½Ð° ÐºÑ€Ð¾Ð»ÐµÐ²ÐµÑ†', 'Ð½Ð° ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿', 'Ð½Ð° Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 'Ð½Ð° Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´', 'Ð½Ð° Ð¿ÐµÑ‚Ñ€Ñ–Ð²Ñ†Ñ–', 'Ð½Ð° Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°', 'Ð½Ð° Ð±Ñ–Ð»Ñƒ Ñ†ÐµÑ€ÐºÐ²Ñƒ', 'Ð½Ð° Ð±Ñ€Ð¾Ð²Ð°Ñ€Ð¸', 'Ð½Ð° ÑÑƒÐ¼Ð¸', 'Ð½Ð° Ñ…Ð°Ñ€ÐºÑ–Ð²', 'Ð½Ð° Ð´Ð½Ñ–Ð¿Ñ€Ð¾', 'Ð½Ð° ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'Ð½Ð° Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€', 'Ð½Ð° Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÑƒ', 'Ð½Ð° Ð»Ð¸Ð¿Ð¾Ð²ÐµÑ†ÑŒ', 'Ð½Ð° Ñ€Ñ–Ð¿ÐºÐ¸', 'Ð½Ð° Ñ‚ÐµÑ€Ð½Ð¸', 'Ð½Ð° Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´']
                    has_specific_cities = any(city_kw in lorig for city_kw in city_keywords)
                    
                    # Also check for pattern "Ð‘Ð¿Ð›Ð Ð½Ð° [city]" which should create markers
                    import re as _re_cities
                    bpla_na_pattern = _re_cities.findall(r'Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘Ê¼`\-\s]{3,20})', lorig)
                    if bpla_na_pattern:
                        has_specific_cities = True
                    
                    if has_specific_cities:
                        # Let multi-city parser handle this instead
                        pass
                    else:
                        # Extra guard: if a well-known large city (e.g. Ð´Ð½Ñ–Ð¿Ñ€Ð¾, Ñ…Ð°Ñ€ÐºÑ–Ð², ÐºÐ¸Ñ—Ð²) appears ONLY because it's substring of region
                        # we still treat as region directional, not city marker
                        # But if message contains multiple explicit directional part-of-region clauses ("Ð½Ð° ÑÑ…Ð¾Ð´Ñ– <Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–>" ... "Ð½Ð° ÑÑ…Ð¾Ð´Ñ– <Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–>")
                        # then we want to produce separate segment markers instead of a single list-only event.
                        import re as _re_dd
                        dir_clause_count = len(_re_dd.findall(r'Ð½Ð°\s+(?:Ð¿Ñ–Ð²Ð½Ñ–Ñ‡|Ð¿Ñ–Ð²Ð´ÐµÐ½|ÑÑ…Ñ–Ð´|Ð·Ð°Ñ…Ð¾Ð´|Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾|Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾)[^\.]{0,40}?(?:Ñ‰Ð¸Ð½Ð°|Ñ‰Ð¸Ð½Ð¸|Ñ‰Ð¸Ð½Ñƒ)', lorig))
                        if dir_clause_count < 2:
                            return [{
                                'id': str(mid), 'text': clean_text(text)[:600], 'date': date_str, 'channel': channel,
                                'list_only': True, 'source_match': 'region_direction_multi'
                            }]
    except Exception:
        pass
    # Comparative directional relative to a city ("Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ Ð“Ð¾Ñ€Ð¾Ð´Ð½Ñ–", "ÑÑ…Ñ–Ð´Ð½Ñ–ÑˆÐµ ÐšÑ€Ð¾Ð»ÐµÐ²Ñ†Ñ") -> use base city location
    try:
        import re as _re_rel
        low_txt = text.lower()
        # NEW: pattern "<city> - Ð´Ð¾ Ð²Ð°Ñ Ð‘Ð¿Ð›Ð" -> marker at city
        m_dash = _re_rel.search(r"([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘'Ê¼â€™`\-]{3,40})\s*[-â€“â€”]\s*Ð´Ð¾ Ð²Ð°Ñ\s+Ð±Ð¿Ð»Ð°", low_txt)
        if m_dash:
            raw_city = m_dash.group(1)
            raw_city = raw_city.replace('\u02bc',"'").replace('Ê¼',"'").replace('â€™',"'").replace('`',"'")
            base = UA_CITY_NORMALIZE.get(raw_city, raw_city)
            coords = CITY_COORDS.get(base)
            if not coords and 'SETTLEMENTS_INDEX' in globals():
                coords = (globals().get('SETTLEMENTS_INDEX') or {}).get(base)
            if coords:
                lat,lng = coords
                threat, icon = 'shahed','shahed.png'
                return [{
                    'id': str(mid), 'place': base.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat, 'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'city_dash_uav'
                }]
        # NEW: pattern "Ð‘Ð¿Ð›Ð Ð½Ð° <city>" or "Ð±Ð¿Ð»Ð° Ð½Ð° <city>" -> marker at city
        uav_cities = _re_rel.findall(r"Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼\`\s/]+?)(?=\s+(?:Ð·|Ð½Ð°|Ð´Ð¾|Ð²Ñ–Ð´|Ñ‡ÐµÑ€ÐµÐ·|Ð¿Ð¾Ð²Ð·|ÐºÑƒÑ€ÑÐ¾Ð¼|Ð½Ð°Ð¿Ñ€ÑÐ¼)\s|[,\.\!\?;:\n]|$)", low_txt)
        if uav_cities:
            threats = []
            for idx, rc in enumerate(uav_cities):
                rc = rc.replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                
                # Handle cities separated by slash (e.g., "Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´/Ð¿ÐµÑ‚Ñ€Ñ–Ð²Ñ†Ñ–")
                cities_to_process = []
                if '/' in rc:
                    cities_to_process.extend(rc.split('/'))
                else:
                    cities_to_process.append(rc)
                
                for city_idx, city in enumerate(cities_to_process):
                    city = city.strip()
                    if not city:
                        continue
                        
                    base = UA_CITY_NORMALIZE.get(city, city)
                    
                    # Special handling for Kyiv - show directional approach instead of center point
                    if base.lower() == 'ÐºÐ¸Ñ—Ð²':
                        kyiv_lat, kyiv_lng, kyiv_label, direction_info = get_kyiv_directional_coordinates(text, base)
                        threat_type, icon = classify(text)
                        
                        # Use specialized icon for directional Kyiv threats
                        if direction_info:
                            icon = 'shahed.png'  # Could create special directional icon later
                            
                        threats.append({
                            'id': f"{mid}_uav_{idx}_{city_idx}_kyiv_dir", 'place': kyiv_label, 'lat': kyiv_lat, 'lng': kyiv_lng,
                            'threat_type': threat_type, 'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'uav_on_city_kyiv_directional',
                            'direction_info': direction_info
                        })
                        continue
                    
                    coords = CITY_COORDS.get(base)
                    if not coords and 'SETTLEMENTS_INDEX' in globals():
                        coords = (globals().get('SETTLEMENTS_INDEX') or {}).get(base)
                    if coords:
                        lat,lng = coords
                        threats.append({
                            'id': f"{mid}_uav_{idx}_{city_idx}", 'place': base.title(), 'lat': lat, 'lng': lng,
                            'threat_type': 'shahed', 'text': clean_text(text)[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': 'shahed.png', 'source_match': 'uav_on_city'
                        })
            if threats:
                return threats
        # pattern captures direction word + city morph form
        m_rel = _re_rel.search(r'(Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ|Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ñ–ÑˆÐµ|ÑÑ…Ñ–Ð´Ð½Ñ–ÑˆÐµ|Ð·Ð°Ñ…Ñ–Ð´Ð½Ñ–ÑˆÐµ)\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\'Ê¼â€™`\-]{3,40})', low_txt)
        if m_rel:
            raw_city = m_rel.group(2)
            # normalize apostrophes
            raw_city = raw_city.replace('\u02bc',"'").replace('Ê¼',"'").replace('â€™',"'").replace('`',"'")
            base = UA_CITY_NORMALIZE.get(raw_city, raw_city)
            coords = CITY_COORDS.get(base)
            if not coords and 'SETTLEMENTS_INDEX' in globals():
                idx = globals().get('SETTLEMENTS_INDEX') or {}
                coords = idx.get(base)
            if not coords:
                enriched = ensure_city_coords(base)
                if enriched:
                    if isinstance(enriched, tuple) and len(enriched)==3:
                        coords = (enriched[0], enriched[1])
                    else:
                        coords = enriched
            if coords:
                lat,lng = coords
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': base.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'relative_direction_city'
                }]
    except Exception:
        pass
    # Multi-segment UAV messages with pipe separator (e.g., "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° ÐšÐ°Ð³Ð°Ñ€Ð»Ð¸Ðº | 2Ñ… Ð‘Ð¿Ð›Ð Ð‘Ñ–Ð»Ð¾Ñ†ÐµÑ€ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ | 3Ñ… Ð‘Ð¿Ð›Ð Ð’Ð¸ÑˆÐµÐ½ÑŒÐºÐ¸ / Ð£ÐºÑ€Ð°Ñ—Ð½ÐºÐ°")
    try:
        if '|' in text and 'Ð±Ð¿Ð»Ð°' in text.lower():
            segments = [seg.strip() for seg in text.split('|') if seg.strip()]
            if len(segments) >= 2:  # At least 2 segments
                threats = []
                import re as _re_multi
                
                for seg_idx, segment in enumerate(segments):
                    seg_lower = segment.lower()
                    if 'Ð±Ð¿Ð»Ð°' not in seg_lower:
                        continue
                    
                    # Pattern 1: "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [city]" (with optional Ð½.Ð¿. prefix)
                    course_match = _re_multi.search(r'Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s*$|\s*\|)', seg_lower)
                    if course_match:
                        city_name = course_match.group(1).strip()
                        city_norm = clean_text(city_name).lower()
                        
                        # Accusative case normalization (Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ð´ÐµÐ¶)
                        if city_norm == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':
                            city_norm = 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°'
                        elif city_norm == 'Ð¼ÐµÐ½Ñƒ':
                            city_norm = 'Ð¼ÐµÐ½Ð°'
                        elif city_norm == 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÑƒ':
                            city_norm = 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÐ°'
                        elif city_norm == 'ÐºÐ¸Ñ”Ð²Ð¾Ð¼':
                            city_norm = 'ÐºÐ¸Ñ—Ð²'
                        # General accusative case endings
                        elif city_norm.endswith('Ñƒ') and len(city_norm) > 3:
                            city_norm = city_norm[:-1] + 'Ð°'
                        elif city_norm.endswith('ÑŽ') and len(city_norm) > 3:
                            city_norm = city_norm[:-1] + 'Ñ'
                        elif city_norm.endswith('ÐºÑƒ') and len(city_norm) > 4:
                            city_norm = city_norm[:-2] + 'ÐºÐ°'
                        
                        if city_norm in UA_CITY_NORMALIZE:
                            city_norm = UA_CITY_NORMALIZE[city_norm]
                        
                        coords = ensure_city_coords_with_message_context(city_norm, text)
                        
                        if coords and isinstance(coords, tuple) and len(coords) >= 2:
                            lat, lng = coords[0], coords[1]
                            threat_type, icon = classify(text)
                            # Use normalized city name for display, not the original accusative form
                            display_name = city_norm.title()
                            threats.append({
                                'id': f"{mid}_multi_{seg_idx}",
                                'place': display_name,
                                'lat': lat,
                                'lng': lng,
                                'threat_type': threat_type,
                                'text': f"ÐšÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° {display_name}",
                                'date': date_str,
                                'channel': channel,
                                'marker_icon': icon,
                                'source_match': 'multi_segment_course',
                                'count': 1
                            })
                    
                    # Pattern 1.5: "Ð‘Ð¿Ð›Ð Ð¿Ð¾Ð²Ð· [city1] ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [city2]" - extract both cities  
                    povz_match = _re_multi.search(r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+Ð¿Ð¾Ð²Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+?)(?:\s*$|\s*\|)', seg_lower)
                    if povz_match and not course_match:  # Don't double-process if already handled by Pattern 1
                        count_str, city1_name, city2_name = povz_match.groups()
                        count = int(count_str) if count_str and count_str.isdigit() else 1
                        
                        for city_idx, city_raw in enumerate([city1_name, city2_name]):
                            if not city_raw:
                                continue
                                
                            city_name = city_raw.strip()
                            city_norm = clean_text(city_name).lower()
                            
                            # Accusative case normalization for both cities
                            if city_norm == 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ':
                                city_norm = 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°'
                            elif city_norm == 'Ð¼ÐµÐ½Ñƒ':
                                city_norm = 'Ð¼ÐµÐ½Ð°'
                            elif city_norm == 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÑƒ':
                                city_norm = 'Ð¿Ñ–ÑÐºÑ–Ð²ÐºÐ°'
                            elif city_norm == 'ÐºÐ¸Ñ”Ð²Ð¾Ð¼':
                                city_norm = 'ÐºÐ¸Ñ—Ð²'
                            # General accusative case endings
                            elif city_norm.endswith('Ñƒ') and len(city_norm) > 3:
                                city_norm = city_norm[:-1] + 'Ð°'
                            elif city_norm.endswith('ÑŽ') and len(city_norm) > 3:
                                city_norm = city_norm[:-1] + 'Ñ'
                            elif city_norm.endswith('ÐºÑƒ') and len(city_norm) > 4:
                                city_norm = city_norm[:-2] + 'ÐºÐ°'
                            
                            if city_norm in UA_CITY_NORMALIZE:
                                city_norm = UA_CITY_NORMALIZE[city_norm]
                            
                            coords = ensure_city_coords(city_norm)
                            
                            if coords and isinstance(coords, tuple) and len(coords) >= 2:
                                lat, lng = coords[0], coords[1]
                                threat_type, icon = classify(text)
                                action = "ÐŸÐ¾Ð²Ð·" if city_idx == 0 else "ÐšÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°"
                                # Use normalized city name for display
                                display_name = city_norm.title()
                                threats.append({
                                    'id': f"{mid}_multi_{seg_idx}_povz_{city_idx}",
                                    'place': display_name,
                                    'lat': lat,
                                    'lng': lng,
                                    'threat_type': threat_type,
                                    'text': f"{action} {display_name} ({count}x)",
                                    'date': date_str,
                                    'channel': channel,
                                    'marker_icon': icon,
                                    'source_match': 'multi_segment_povz',
                                    'count': count
                                })
                    
                    # Pattern 2: "[N]Ñ… Ð‘Ð¿Ð›Ð [location]" - extract cities
                    location_match = _re_multi.search(r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+(.+?)(?:\.|$)', seg_lower)
                    if location_match and not course_match:  # Don't double-process course segments
                        count_str = location_match.group(1) or "1"
                        location_text = location_match.group(2).strip()
                        count = int(count_str) if count_str.isdigit() else 1
                        
                        # Split by common separators to get individual cities
                        cities = []
                        for sep in [' / ', ' Ñ‚Ð° ', ' Ñ– ', ', ']:
                            if sep in location_text:
                                cities = [c.strip() for c in location_text.split(sep) if c.strip()]
                                break
                        if not cities:
                            cities = [location_text]
                        
                        for city_idx, city in enumerate(cities):
                            city = city.strip()
                            if not city:
                                continue
                            
                            # Handle district references (e.g., "Ð‘Ñ–Ð»Ð¾Ñ†ÐµÑ€ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½")
                            if 'Ñ€Ð°Ð¹Ð¾Ð½' in city:
                                # Extract district name and try to find main city
                                district_name = city.replace('Ñ€Ð°Ð¹Ð¾Ð½', '').replace('ÑÑŒÐºÐ¸Ð¹', '').replace('Ñ†ÑŒÐºÐ¸Ð¹', '').strip()
                                
                                # Special case mappings
                                if 'Ð±Ñ–Ð»Ð¾Ñ†ÐµÑ€ÐºÑ–Ð²' in district_name:
                                    district_name = 'Ð±Ñ–Ð»Ð° Ñ†ÐµÑ€ÐºÐ²Ð°'
                                
                                if district_name:
                                    city = district_name
                                else:
                                    continue
                                
                            city_norm = clean_text(city).lower()
                            if city_norm in UA_CITY_NORMALIZE:
                                city_norm = UA_CITY_NORMALIZE[city_norm]
                            
                            coords = ensure_city_coords(city_norm)
                            
                            if coords and isinstance(coords, tuple) and len(coords) >= 2:
                                lat, lng = coords[0], coords[1]
                                threat_type, icon = classify(text)
                                threats.append({
                                    'id': f"{mid}_multi_{seg_idx}_{city_idx}",
                                    'place': city.title(),
                                    'lat': lat,
                                    'lng': lng,
                                    'threat_type': threat_type,
                                    'text': f"{count}Ñ… Ð‘Ð¿Ð›Ð Ð½Ð° {city.title()}",
                                    'date': date_str,
                                    'channel': channel,
                                    'marker_icon': icon,
                                    'source_match': f'multi_segment_location_{count}x',
                                    'count': count
                                })
                
                if threats:
                    # ALSO: Extract cities from emoji structure in the same text 
                    # Pattern for "| ðŸ›¸ Ð“Ð¾Ñ€Ð¾Ð´ (ÐžÐ±Ð»Ð°ÑÑ‚ÑŒ)"
                    emoji_pattern = r'\|\s*ðŸ›¸\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)\s*\([^)]*Ð¾Ð±Ð»[^)]*\)'
                    emoji_matches = re.finditer(emoji_pattern, text, re.IGNORECASE)
                    
                    for match in emoji_matches:
                        city_raw = match.group(1).strip()
                        if not city_raw or len(city_raw) < 2:
                            continue
                            
                        city_norm = clean_text(city_raw).lower()
                        if city_norm in UA_CITY_NORMALIZE:
                            city_norm = UA_CITY_NORMALIZE[city_norm]
                        
                        coords = ensure_city_coords(city_norm)
                        
                        if coords:
                            lat, lng = coords[:2]
                            threat_type, icon = classify(text)
                            
                            threat_id = f"{mid}_emoji_struct_{len(threats)}"
                            threats.append({
                                'id': threat_id,
                                'place': city_raw.title(),
                                'lat': lat,
                                'lng': lng,
                                'threat_type': threat_type,
                                'text': f"Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° Ð² {city_raw}",
                                'date': date_str,
                                'channel': channel,
                                'marker_icon': icon,
                                'source_match': 'emoji_structure_multi',
                                'count': 1
                            })
                            
                            add_debug_log(f"Multi emoji structure: {city_raw} -> {coords}", "emoji_struct_multi")
                        else:
                            add_debug_log(f"Multi emoji structure: No coords for {city_raw}", "emoji_struct_multi")
                    
                    # Check for priority result to combine
                    if '_current_priority_result' in globals() and globals()['_current_priority_result']:
                        combined_result = globals()['_current_priority_result'] + threats
                        add_debug_log(f"MULTI-SEGMENT: Combined priority result ({len(globals()['_current_priority_result'])}) with threats ({len(threats)}) = {len(combined_result)} total", "priority_combine")
                        # Clear the global priority result after use
                        globals()['_current_priority_result'] = None
                        return combined_result
                    return threats
                    
    except Exception:
        pass
    
    # Course towards single city ("ÐºÑƒÑ€Ñ(Ð¾Ð¼) Ð½Ð° Ð‘Ð°Ñ‚ÑƒÑ€Ð¸Ð½") -> place marker at that city
    try:
        import re as _re_course
        low_txt2 = text.lower()
        m_course = _re_course.search(r"ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\'Ê¼'`\-\s]{3,60})(?=\s*(?:$|[,\.\!\?;]|\n))", low_txt2)
        if m_course:
            raw_city = m_course.group(1).strip()
            raw_city = raw_city.replace('\u02bc',"'").replace('Ê¼',"'").replace('â€™',"'").replace('`',"'")
            base = UA_CITY_NORMALIZE.get(raw_city, raw_city)
            
            # Use enhanced coordinate lookup with Nominatim fallback
            coords = get_coordinates_enhanced(base, context="Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°")
            
            if not coords:
                # Legacy fallback for backwards compatibility
                enriched = ensure_city_coords(base)
                if enriched:
                    if isinstance(enriched, tuple) and len(enriched)==3:
                        coords = (enriched[0], enriched[1])
                    else:
                        coords = enriched
                    if isinstance(enriched, tuple) and len(enriched)==3:
                        coords = (enriched[0], enriched[1])
                    else:
                        coords = enriched
            if coords:
                lat,lng = coords
                threat_type, icon = classify(text)
                
                # Extract course information for Shahed threats
                course_info = None
                if threat_type == 'shahed':
                    course_info = extract_shahed_course_info(text)
                
                # Extract count from text (look for pattern like "10Ñ… Ð‘Ð¿Ð›Ð")
                uav_count = 1
                import re as _re_count
                count_match = _re_count.search(r'(\d+)\s*[xÑ…Ã—]\s*Ð±Ð¿Ð»Ð°', low_txt2)
                if count_match:
                    uav_count = int(count_match.group(1))
                
                # Create multiple tracks for multiple drones
                tracks_to_create = max(1, uav_count)
                threat_tracks = []
                
                for i in range(tracks_to_create):
                    track_name = base.title()
                    if tracks_to_create > 1:
                        track_name += f" #{i+1}"
                    
                    # Add small coordinate offsets to prevent marker overlap
                    marker_lat = lat
                    marker_lng = lng
                    if tracks_to_create > 1:
                        # Create a circular pattern around the target
                        import math
                        angle = (2 * math.pi * i) / tracks_to_create
                        offset_distance = 0.01  # ~1km offset
                        marker_lat += offset_distance * math.cos(angle)
                        marker_lng += offset_distance * math.sin(angle)
                    
                    threat_data = {
                        'id': f"{mid}_{i+1}", 'place': track_name, 'lat': marker_lat, 'lng': marker_lng,
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'course_to_city', 'count': 1
                    }
                    
                    # Add course information if available
                    if course_info:
                        threat_data.update({
                            'course_source': course_info.get('source_city'),
                            'course_target': course_info.get('target_city'),
                            'course_direction': course_info.get('course_direction'),
                            'course_type': course_info.get('course_type')
                        })
                    
                    threat_tracks.append(threat_data)
                
                return threat_tracks
    except Exception:
        pass
    
    # --- PRIORITY: Early explicit pattern for districts - MOVED UP TO AVOID CONFLICTS ---
    # Check before region direction processing to prevent fallback to oblast centers
    try:
        import re as _re_raion
        # Pattern 1: "<RaionName> Ñ€Ð°Ð¹Ð¾Ð½ (<Oblast ...>)"
        m_raion_oblast = _re_raion.search(r'([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-]{4,})\s+Ñ€Ð°Ð¹Ð¾Ð½\s*\(([^)]*Ð¾Ð±Ð»[^)]*)\)', text)
        if m_raion_oblast:
            raion_token = m_raion_oblast.group(1).strip().lower()
            # Normalize morphological endings
            raion_base = _re_raion.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$', 'ÑÑŒÐºÐ¸Ð¹', raion_token)
            if raion_base in RAION_FALLBACK:
                lat, lng = RAION_FALLBACK[raion_base]
                threat_type, icon = classify(text)
                add_debug_log(f"PRIORITY: Early district processing - {raion_base} Ñ€Ð°Ð¹Ð¾Ð½ -> {lat}, {lng}", "district_early")
                return [{
                    'id': str(mid), 'place': f"{raion_base.title()} Ñ€Ð°Ð¹Ð¾Ð½", 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500],
                    'date': date_str, 'channel': channel, 'marker_icon': icon, 'source_match': 'raion_oblast_combo_early'
                }]
            else:
                add_debug_log(f"Early district processing - {raion_base} not found in RAION_FALLBACK", "district_early")

        # Pattern 2: "<RaionName> Ñ€Ð°Ð¹Ð¾Ð½ <OblastName>" (Ð±ÐµÐ· Ð´ÑƒÐ¶Ð¾Ðº)
        m_raion_oblast2 = _re_raion.search(r'([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-]{4,})\s+Ñ€Ð°Ð¹Ð¾Ð½\s+([\w\']+(?:Ñ‰Ð¸Ð½Ð¸|Ñ‰Ð¸Ð½Ñƒ|Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸|Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–))', text)
        if m_raion_oblast2:
            raion_token = m_raion_oblast2.group(1).strip().lower()
            # Normalize morphological endings
            raion_base = _re_raion.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$', 'ÑÑŒÐºÐ¸Ð¹', raion_token)
            if raion_base in RAION_FALLBACK:
                lat, lng = RAION_FALLBACK[raion_base]
                threat_type, icon = classify(text)
                add_debug_log(f"PRIORITY: Early district processing (format 2) - {raion_base} Ñ€Ð°Ð¹Ð¾Ð½ -> {lat}, {lng}", "district_early")
                return [{
                    'id': str(mid), 'place': f"{raion_base.title()} Ñ€Ð°Ð¹Ð¾Ð½", 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500],
                    'date': date_str, 'channel': channel, 'marker_icon': icon, 'source_match': 'raion_oblast_combo_early_v2'
                }]
            else:
                add_debug_log(f"Early district processing (format 2) - {raion_base} not found in RAION_FALLBACK", "district_early")
    except Exception as e:
        add_debug_log(f"Early district processing error: {e}", "district_early")
    
    # Region directional segments specifying part of oblast ("Ð½Ð° ÑÑ…Ð¾Ð´Ñ– Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸") possibly multiple in one line
    try:
        import re as _re_seg
        lower_full = text.lower()
        pattern = _re_seg.compile(r'Ð½Ð°\s+([\w\-\s/]+?)\s+(?:Ñ‡Ð°ÑÑ‚Ð¸Ð½Ñ–\s+)?([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ð°|[a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ð¸|[a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½Ñƒ)')
        seg_matches = list(pattern.finditer(lower_full))
        seg_tracks = []
        used_spans = []
        if seg_matches:
            # Map Ukrainian directional forms to codes
            dir_map_words = {
                'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡':'n','Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ':'s','ÑÑ…Ñ–Ð´':'e','Ð·Ð°Ñ…Ñ–Ð´':'w','ÑÑ…Ð¾Ð´Ñ–':'e','Ð·Ð°Ñ…Ð¾Ð´Ñ–':'w','Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–':'n','Ð¿Ñ–Ð²Ð´Ð½Ñ–':'s',
                'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´':'ne','Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ð¾Ð´':'ne','Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾ ÑÑ…Ñ–Ð´':'ne','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´':'se','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾ ÑÑ…Ñ–Ð´':'se',
                'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´':'nw','Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾ Ð·Ð°Ñ…Ñ–Ð´':'nw','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´':'sw','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾ Ð·Ð°Ñ…Ñ–Ð´':'sw'
            }
            def direction_codes(raw:str):
                parts = [p.strip() for p in raw.replace('â€“','-').split('/') if p.strip()]
                out = []
                for p in parts:
                    # compress multiple spaces
                    p2 = ' '.join(p.split())
                    # find best match in dir_map_words by prefix
                    code=None
                    for k,v in dir_map_words.items():
                        if k in p2:
                            code=v; break
                    if not code:
                        # try simple endings
                        if p2.startswith('ÑÑ…Ñ–Ð´'): code='e'
                        elif p2.startswith('Ð·Ð°Ñ…Ñ–Ð´'): code='w'
                    if code and code not in out:
                        out.append(code)
                return out or ['center']
            for m in seg_matches:
                dir_raw = m.group(1).strip()
                region_raw = m.group(2).strip()
                # Normalize region key to match OBLAST_CENTERS keys
                region_key = None
                for k in OBLAST_CENTERS.keys():
                    if region_raw in k:
                        region_key = k
                        break
                if not region_key:
                    continue
                base_lat, base_lng = OBLAST_CENTERS[region_key]
                codes = direction_codes(dir_raw)
                for idx, code in enumerate(codes,1):
                    # offset placement (reuse logic similar to later region_direction block)
                    def offset(lat,lng,code):
                        lat_step = 0.55
                        lng_step = 0.85 / max(0.2, abs(math.cos(math.radians(lat))))
                        if code=='n': return lat+lat_step, lng
                        if code=='s': return lat-lat_step, lng
                        if code=='e': return lat, lng+lng_step
                        if code=='w': return lat, lng-lng_step
                        lat_diag=lat_step*0.8; lng_diag=lng_step*0.8
                        if code=='ne': return lat+lat_diag, lng+lng_diag
                        if code=='nw': return lat+lat_diag, lng-lng_diag
                        if code=='se': return lat-lat_diag, lng+lng_diag
                        if code=='sw': return lat-lat_diag, lng-lng_diag
                        return lat, lng
                    lat_o, lng_o = offset(base_lat, base_lng, code)
                    label_region = region_key.split()[0].title()
                    dir_label_map = {
                        'n':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','s':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','e':'ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','w':'Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°',
                        'ne':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','nw':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','se':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','sw':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°','center':'Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°'
                    }
                    label = f"{label_region} ({dir_label_map.get(code,'Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°')})"
                    threat_type, icon = classify(text)
                    
                    # Skip if this segment contains "ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° [city]" after the region match
                    # to give priority to specific city course tracking
                    segment_after = text[m.end():]
                    if _re_seg.search(r'ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?[Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,}', segment_after, _re_seg.IGNORECASE):
                        continue
                    
                    seg_tracks.append({
                        'id': f"{mid}_rd{len(seg_tracks)+1}", 'place': label, 'lat': lat_o, 'lng': lng_o,
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'region_direction_segment'
                    })
            if seg_tracks:
                return seg_tracks
    except Exception as e:
        try: log.debug(f'region_dir_segments error: {e}')
        except: pass
    # --- Pre-split case: several bold oblast headers inside a single line (e.g. **ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°:** ... **Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°:** ... ) ---
    try:
        import re as _pre_hdr_re
        # Detect two or more bold oblast headers
        hdr_pat = re.compile(r'(\*\*[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘]+Ñ‰Ð¸Ð½Ð°\*\*:)')
        if text.count('**') >= 4:  # quick filter
            matches = list(hdr_pat.finditer(text))
            if len(matches) >= 2:
                # Insert newline before each header (except first) if not already line-start
                # Build new text chunk-wise
                new_parts = []
                last = 0
                for i, m in enumerate(matches):
                    start = m.start()
                    if i == 0 and start > 0 and text[start-1] != '\n':
                        # ensure header is at line start
                        new_parts.append(text[last:start])
                    elif i > 0:
                        # append text before header ensuring newline separation
                        segment = text[last:start]
                        if not segment.endswith('\n'):
                            segment += '\n'
                        new_parts.append(segment)
                    last = start
                # append remaining
                new_parts.append(text[last:])
                new_text_joined = ''.join(new_parts)
                if new_text_joined != text:
                    text = new_text_joined
    except Exception:
        pass
    # --- Ð¡Ð¿ÐµÑ†. Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸-Ð¾Ð±Ð»Ð°ÑÑ‚ÑÐ¼Ð¸ Ð¸ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼ Ð³Ð¾Ñ€Ð¾Ð´Ð¾Ð² ---
    import unicodedata
    def normalize_city_name(name):
        # ÐŸÑ€Ð¸Ð²ÐµÑÑ‚Ð¸ Ðº Ð½Ð¸Ð¶Ð½ÐµÐ¼Ñƒ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ñƒ, Ð·Ð°Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð°Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ„Ñ‹ Ð½Ð° ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹, ÑƒÐ±Ñ€Ð°Ñ‚ÑŒ Ð»Ð¸ÑˆÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹
        n = name.lower().strip()
        n = n.replace('Ê¼', "'").replace('â€™', "'").replace('`', "'")
        n = unicodedata.normalize('NFC', n)
        return n
    # Ð•ÑÐ»Ð¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ñ€Ð¾Ðº Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸-Ð¾Ð±Ð»Ð°ÑÑ‚ÑÐ¼Ð¸ Ð¸ Ð³Ð¾Ñ€Ð¾Ð´Ð°Ð¼Ð¸
    # ÐŸÑ€ÐµÐ´Ð²Ð°Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ±ÐµÑ€Ñ‘Ð¼ Ñ‡Ð¸ÑÑ‚Ð¾ Ð´Ð¾Ð½Ð°Ñ‚Ð½Ñ‹Ðµ/Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ¸ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¸Ð· Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð±Ð»Ð¾ÐºÐ°, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð½Ð¸ Ð½Ðµ Ð¼ÐµÑˆÐ°Ð»Ð¸
    raw_lines = text.splitlines()
    
    # NEW: Handle single-line messages with multiple regions like "Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°: 1 Ð‘Ð¿Ð›Ð Ð½Ð° ÐšÐ¾Ð·ÐµÐ»ÐµÑ†ÑŒ ... Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð°: 3 Ð‘Ð¿Ð›Ð..."
    # First try to split by region headers in single line
    single_line_regions = ['Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½', 'ÑÑƒÐ¼Ñ‰Ð¸Ð½', 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½', 'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½', 'Ð´Ð¾Ð½ÐµÑ†ÑŒÐº', 'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½']
    if len(raw_lines) == 1 and any(region in text.lower() for region in single_line_regions):
        add_debug_log(f"Single-line multi-region message detected, raw_lines count: {len(raw_lines)}", "multi_region")
        # Split by oblast headers that have colon after them
        import re as _re_split
        region_split = _re_split.split(r'([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+Ñ‰Ð¸Ð½Ð°):\s*', text)
        add_debug_log(f"Region split result: {region_split}", "multi_region")
        if len(region_split) > 2:  # We have actual splits
            new_lines = []
            for i in range(1, len(region_split), 2):  # Take every odd element (region name) and next even (content)
                if i+1 < len(region_split):
                    region_name = region_split[i]
                    content = region_split[i+1].strip()
                    new_lines.append(f"{region_name}:")
                    new_lines.append(content)
                    add_debug_log(f"Added region header: '{region_name}:' and content: '{content}'", "multi_region")
            if new_lines:
                raw_lines = new_lines
                add_debug_log(f"Split single line into {len(raw_lines)} lines for multi-region processing", "multi_region")
        else:
            add_debug_log("Region split failed, keeping original format", "multi_region")
    
    cleaned_for_multiline = []
    import re as _re_clean
    donation_keys = ['Ð¼Ð¾Ð½Ð¾Ð±Ð°Ð½Ðº','send.monobank','patreon','donat','Ð´Ð¾Ð½Ð°Ñ‚','Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ ÐºÐ°Ð½Ð°Ð»','Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸']
    for l in raw_lines:
        ls = l.strip()
        if not ls:
            continue
        # If line combines header and content ("Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°: Ð“Ñ€ÑƒÐ¿Ð° ÐšÐ  ..." possibly with formatting ** **)
        m_comb = _re_clean.match(r'^\**([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘]+Ñ‰Ð¸Ð½Ð°)\**:\s*(.+)$', ls)
        if m_comb:
            header_part = m_comb.group(1) + ':'
            rest_part = m_comb.group(2).strip()
            cleaned_for_multiline.append(header_part)
            ls = rest_part  # continue processing rest_part below (could still contain links)
        low_ls = ls.lower()
        # Strip markdown links / segments that are purely donation or service references, keep threat fragment
        def _strip_bad_links(s: str):
            # Remove any [text](url) where text or url contains donation_keys
            def _repl(m):
                inner_text = m.group(1).lower()
                url = m.group(2).lower()
                if any(k in inner_text or k in url for k in donation_keys):
                    return ''
                return m.group(0)
            s2 = _re_clean.sub(r'\[([^\]]{0,60})\]\(([^) ]+?)\)', _repl, s)
            return s2
        ls_no_links = _strip_bad_links(ls)
        low_no_links = ls_no_links.lower()
        if any(k in low_no_links for k in donation_keys):
            # If after stripping links still only donation noise and no threat keywords, skip.
            if not any(t in low_no_links for t in ['Ð±Ð¿Ð»Ð°','ÐºÑƒÑ€Ñ','Ñ€Ð°ÐºÐµÑ‚Ð°','Ñ€Ð°ÐºÐµÑ‚Ð¸','Ñ€ÑƒÐ¿Ð°','Ð³Ñ€ÑƒÐ¿Ð°','ÐºÑ€']):
                continue
            # Else remove the donation substrings explicitly.
            for k in donation_keys:
                low_no_links = low_no_links.replace(k,' ')
            ls_no_links = ' '.join(low_no_links.split())
        cleaned_for_multiline.append(ls_no_links.strip())
    lines = cleaned_for_multiline
    
    oblast_hdr = None
    multi_city_tracks = []
    add_debug_log(f"Processing {len(lines)} cleaned lines for multi-city tracks", "multi_region")
    for ln in lines:
        add_debug_log(f"Processing line: '{ln}'", "multi_region")
        
        # PRIORITY: Check for specific region-city patterns FIRST
        import re as _re_region_city
        ln_lower = ln.lower()
        
        # Pattern 1: "Ð½Ð° [region] [count] ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð½Ð° [city]"
        region_city_pattern1 = _re_region_city.compile(r'Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ñ–Ð°Ñƒ]?)\s+(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ñ–Ñ—Ð²]*\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)', _re_region_city.IGNORECASE)
        region_city_match1 = region_city_pattern1.search(ln_lower)
        
        # Pattern 2: "[region] - ÑˆÐ°Ñ…ÐµÐ´Ð¸ Ð½Ð° [city]"
        region_city_pattern2 = _re_region_city.compile(r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð°ÑƒÐ¸]?)\s*-\s*ÑˆÐ°Ñ…ÐµÐ´[Ñ–Ñ—Ð¸Ð²]*\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)', _re_region_city.IGNORECASE)
        region_city_match2 = region_city_pattern2.search(ln_lower)
        
        # Pattern 3: "[region] ([city] Ñ€-Ð½)" - for district headquarters
        region_district_pattern = _re_region_city.compile(r'([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð°ÑƒÐ¸]?)\s*\(\s*([Ð°-ÑÑ–Ñ—Ñ”Ò‘\'\-\s]+)\s+Ñ€[-\s]*Ð½\)', _re_region_city.IGNORECASE)
        region_district_match = region_district_pattern.search(ln_lower)
        
        add_debug_log(f"CHECKING region-city patterns for line: '{ln_lower}'", "region_city_debug")
        
        region_city_match = region_city_match1 or region_city_match2
        
        if region_district_match:
            # Handle "Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð° (Ð½Ð¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹ Ñ€-Ð½)" format
            region_raw, district_raw = region_district_match.groups()
            target_city = district_raw.strip()
            
            add_debug_log(f"REGION-DISTRICT pattern FOUND: region='{region_raw}', district='{district_raw}'", "region_district")
            
            # Normalize city name and try to find coordinates
            city_norm = target_city.lower()
            # Apply UA_CITY_NORMALIZE rules if available
            if 'UA_CITY_NORMALIZE' in globals():
                city_norm = UA_CITY_NORMALIZE.get(city_norm, city_norm)
            coords = CITY_COORDS.get(city_norm)
            
            add_debug_log(f"District city lookup: '{target_city}' -> '{city_norm}' -> {coords}", "region_district")
            
            if coords:
                lat, lng = coords
                threat_type, icon = classify(ln)
                
                multi_city_tracks.append({
                    'id': f"{mid}_region_district_{len(multi_city_tracks)+1}",
                    'place': target_city.title(),
                    'lat': lat,
                    'lng': lng,
                    'threat_type': threat_type,
                    'text': ln[:500],
                    'date': date_str,
                    'channel': channel,
                    'marker_icon': icon,
                    'source_match': 'region_district',
                    'count': 1
                })
                add_debug_log(f"Created region-district marker: {target_city.title()}", "region_district")
                continue  # Skip further processing of this line
            else:
                add_debug_log(f"No coordinates found for district city: '{target_city}' (normalized: '{city_norm}')", "region_district")
        
        elif region_city_match:
            if region_city_match1:
                region_raw, count_str, city_raw = region_city_match1.groups()
                count = int(count_str) if count_str.isdigit() else 1
            else:  # region_city_match2
                region_raw, city_raw = region_city_match2.groups()
                count = 1  # default count for pattern 2
                
            target_city = city_raw.strip()
            
            add_debug_log(f"REGION-CITY pattern FOUND: region='{region_raw}', count={count}, city='{target_city}'", "region_city")
            
            # Normalize city name and try to find coordinates
            city_norm = target_city.lower()
            # Apply UA_CITY_NORMALIZE rules if available
            if 'UA_CITY_NORMALIZE' in globals():
                city_norm = UA_CITY_NORMALIZE.get(city_norm, city_norm)
            coords = CITY_COORDS.get(city_norm)
            
            add_debug_log(f"City lookup: '{target_city}' -> '{city_norm}' -> {coords}", "region_city")
            
            if coords:
                lat, lng = coords
                threat_type, icon = classify(ln)
                
                multi_city_tracks.append({
                    'id': f"{mid}_region_city_{len(multi_city_tracks)+1}",
                    'place': target_city.title(),
                    'lat': lat,
                    'lng': lng,
                    'threat_type': threat_type,
                    'text': ln[:500],
                    'date': date_str,
                    'channel': channel,
                    'marker_icon': icon,
                    'source_match': 'region_city_shahed',
                    'count': count
                })
                add_debug_log(f"Created region-city marker: {target_city.title()} ({count} ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²)", "region_city")
                continue  # Skip further processing of this line
            else:
                add_debug_log(f"No coordinates found for city: '{target_city}' (normalized: '{city_norm}')", "region_city")
        else:
            add_debug_log(f"REGION-CITY pattern NOT FOUND for line: '{ln_lower}'", "region_city_debug")
        
        # Check if line contains Ð‘Ð¿Ð›Ð information without specific course
        ln_lower = ln.lower()
        if 'Ð±Ð¿Ð»Ð°' in ln_lower or 'Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚Ð½Ð¸Ðº' in ln_lower or 'Ð´Ñ€Ð¾Ð½' in ln_lower:
            add_debug_log(f"Line contains UAV keywords: {[k for k in ['Ð±Ð¿Ð»Ð°', 'Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚Ð½Ð¸Ðº', 'Ð´Ñ€Ð¾Ð½'] if k in ln_lower]}", "multi_region")
            if not any(keyword in ln_lower for keyword in ['ÐºÑƒÑ€Ñ', 'Ð½Ð° ', 'Ñ€Ð°Ð¹Ð¾Ð½Ñ–']):
                add_debug_log(f"UAV line lacks direction keywords (ÐºÑƒÑ€Ñ/Ð½Ð°/Ñ€Ð°Ð¹Ð¾Ð½Ñ–) - general activity message", "multi_region")
        else:
            add_debug_log(f"Line does not contain UAV keywords", "multi_region")
        # Ð•ÑÐ»Ð¸ ÑÑ‚Ñ€Ð¾ÐºÐ° â€” ÑÑ‚Ð¾ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð°:")
        # Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸: ÑÑ‚Ñ€Ð¾ÐºÐ°, Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÑŽÑ‰Ð°ÑÑÑ Ð½Ð° ':' (Ð²Ð¾Ð·Ð¼Ð¾Ð¶ÐµÐ½ Ð¿Ñ€Ð¾Ð±ÐµÐ» Ð¿ÐµÑ€ÐµÐ´ / Ð¿Ð¾ÑÐ»Ðµ) Ð¸Ð»Ð¸ Ñ„Ð¾Ñ€Ð¼Ð¾Ð¹ '<Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ>:' Ñ Ð»Ð¸ÑˆÐ½Ð¸Ð¼Ð¸ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð°Ð¼Ð¸
        # NEW: Also handle format like "**ðŸš¨ ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ (Ð¡ÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð».)**"
        import re
        oblast_hdr_match = None
        
        # Standard format: "Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð°:" or "Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°:"
        if re.match(r'^[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-Ê¼`\s]+:\s*$', ln):
            oblast_hdr = ln.split(':')[0].strip().lower()
            oblast_hdr_match = True
            add_debug_log(f"Standard region header format detected: '{oblast_hdr}'", "multi_region")
        
        # NEW format: "**ðŸš¨ ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ (Ð¡ÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð».)**" or similar with oblast in parentheses
        elif re.search(r'\(([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+ÑÑŒÐºÐ°\s+Ð¾Ð±Ð»\.?)\)', ln):
            oblast_match = re.search(r'\(([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+ÑÑŒÐºÐ°\s+Ð¾Ð±Ð»\.?)\)', ln)
            if oblast_match:
                oblast_full = oblast_match.group(1).lower().strip()
                # Convert "ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»." to "ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°"
                oblast_hdr = oblast_full.replace('ÑÑŒÐºÐ° Ð¾Ð±Ð».', 'Ñ‰Ð¸Ð½Ð°').replace('ÑÑŒÐºÐ° Ð¾Ð±Ð»', 'Ñ‰Ð¸Ð½Ð°')
                oblast_hdr_match = True
                add_debug_log(f"Parentheses region header format detected: '{oblast_full}' -> '{oblast_hdr}'", "multi_region")
        
        # NEW format: "Ð¥Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð° â€” Ð‘Ð¿Ð›Ð Ð½Ð° Ð“ÑƒÑ‚Ð¸" - region with dash followed by content
        elif re.search(r'^([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+Ñ‰Ð¸Ð½Ð°)\s*[-â€“â€”]\s*(.+)', ln):
            dash_match = re.search(r'^([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘]+Ñ‰Ð¸Ð½Ð°)\s*[-â€“â€”]\s*(.+)', ln)
            if dash_match:
                oblast_hdr = dash_match.group(1).lower().strip()
                remaining_content = dash_match.group(2).strip()
                oblast_hdr_match = True
                add_debug_log(f"Dash region header format detected: '{oblast_hdr}' with content: '{remaining_content}'", "multi_region")
                # Set the line content to just the remaining part after dash for further processing
                ln = remaining_content
        
        # NEW: Detect regional genitive forms like "Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð¸", "Ð¥Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð¸", etc.
        elif re.search(r'\b([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð¸Ñ–])\b', ln_lower):
            genitive_match = re.search(r'\b([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+Ñ‰Ð¸Ð½[Ð¸Ñ–])\b', ln_lower)
            if genitive_match:
                genitive_form = genitive_match.group(1)
                # Convert genitive to nominative: "ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð¸" -> "ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°"
                potential_oblast = genitive_form.replace('Ñ‰Ð¸Ð½Ð¸', 'Ñ‰Ð¸Ð½Ð°').replace('Ñ‰Ð¸Ð½Ñ–', 'Ñ‰Ð¸Ð½Ð°')
                
                # Validate that this is actually a known region, not just any word ending with Ñ‰Ð¸Ð½[Ð¸Ñ–]
                known_regions = ['ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°', 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°', 
                               'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°', 'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°', 'Ð·Ð°Ð¿Ð¾Ñ€Ð¾Ð¶Ð¶Ñ', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°', 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°',
                               'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð°', 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°',
                               'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°', 'Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½Ð°', 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°', 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°', 'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°',
                               'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ‰Ð¸Ð½Ð°', 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°', 'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°']
                
                if potential_oblast in known_regions:
                    oblast_hdr = potential_oblast
                    oblast_hdr_match = True
                    add_debug_log(f"Genitive region format detected: '{genitive_form}' -> '{oblast_hdr}' in line: '{ln}'", "multi_region")
                    add_debug_log(f"POTENTIAL ISSUE: Oblast set to '{oblast_hdr}' from genitive pattern in: '{ln}'", "oblast_detection")
                else:
                    add_debug_log(f"Ignored potential genitive form '{genitive_form}' -> '{potential_oblast}' (not in known regions) in line: '{ln}'", "multi_region")
        
        if oblast_hdr_match:
            add_debug_log(f"Region header detected: '{oblast_hdr}'", "multi_region")
            if oblast_hdr.startswith('Ð½Ð° '):  # handle 'Ð½Ð° Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°:' header variant
                oblast_hdr = oblast_hdr[3:].strip()
            if oblast_hdr and oblast_hdr[0] in ('Ðµ','Ñ”') and oblast_hdr.endswith('Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°'):
                # Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²Ñ‰Ð¸Ð½Ð° -> Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð° (fix dropped leading Ð§)
                oblast_hdr = 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°'
            # Ð”Ð¾Ð¿. Ð¿Ð¾Ñ‡Ð¸Ð½ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¿ÐµÑ€Ð²Ñ‹Ñ… Ð±ÑƒÐºÐ² Ð´Ð»Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹ (Ð¿Ð¾Ñ‚ÐµÑ€Ñ Ð¿ÐµÑ€Ð²Ð¾Ð¹ Ð±ÑƒÐºÐ²Ñ‹)
            if oblast_hdr and oblast_hdr.endswith('Ñ—Ð²Ñ‰Ð¸Ð½Ð°') and oblast_hdr != 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°':
                oblast_hdr = 'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°'
            if oblast_hdr and oblast_hdr.endswith('Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°') and oblast_hdr != 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°':
                oblast_hdr = 'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°'
            # header detected
            add_debug_log(f"Final region header: '{oblast_hdr}'", "multi_region")
            continue
        try:
            add_debug_log(f"MLINE_LINE oblast={oblast_hdr} raw='{ln}'", "multi_region")
        except Exception:
            pass
        
        # NEW: Check for specific direction patterns before falling back to general UAV activity
        import re
        ln_lower = ln.lower()
        # Check if line has Ð‘Ð¿Ð›Ð or starts with a number (implying drones)
        has_bpla = 'Ð±Ð¿Ð»Ð°' in ln_lower
        starts_with_number = re.match(r'^\d+', ln.strip())
        has_direction_pattern = any(pattern in ln_lower for pattern in ['Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', 'Ñ‡ÐµÑ€ÐµÐ·', 'Ð¿Ð¾Ð²Ð·'])
        
        if (has_bpla or starts_with_number) and has_direction_pattern:
            target_cities = []
            
            # Pattern 1: "Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ [city]"
            naprym_pattern = r'Ñƒ\s+Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*[\.\,\!\?;]|$)'
            naprym_matches = re.findall(naprym_pattern, ln, re.IGNORECASE)
            for city_raw in naprym_matches:
                target_cities.append(('Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', city_raw.strip()))
            
            # Pattern 2: "Ñ‡ÐµÑ€ÐµÐ· [city]"
            cherez_pattern = r'Ñ‡ÐµÑ€ÐµÐ·\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*[\.\,\!\?;]|$)'
            cherez_matches = re.findall(cherez_pattern, ln, re.IGNORECASE)
            for city_raw in cherez_matches:
                target_cities.append(('Ñ‡ÐµÑ€ÐµÐ·', city_raw.strip()))
            
            # Pattern 3: "Ð¿Ð¾Ð²Ð· [city]"
            povz_pattern = r'Ð¿Ð¾Ð²Ð·\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*[\.\,\!\?;]|$)'
            povz_matches = re.findall(povz_pattern, ln, re.IGNORECASE)
            for city_raw in povz_matches:
                target_cities.append(('Ð¿Ð¾Ð²Ð·', city_raw.strip()))
            
            # Process extracted target cities
            for direction_type, city_raw in target_cities:
                city_clean = city_raw.strip()
                city_norm = city_clean.lower()
                
                # Apply UA_CITY_NORMALIZE rules
                if city_norm in UA_CITY_NORMALIZE:
                    city_norm = UA_CITY_NORMALIZE[city_norm]
                
                # Try to get coordinates
                coords = CITY_COORDS.get(city_norm)
                if not coords and SETTLEMENTS_INDEX:
                    coords = SETTLEMENTS_INDEX.get(city_norm)
                if not coords:
                    coords = SETTLEMENT_FALLBACK.get(city_norm) if 'SETTLEMENT_FALLBACK' in globals() else None
                
                add_debug_log(f"Direction pattern '{direction_type}' found city: '{city_raw}' -> '{city_norm}' -> coords: {coords}", "direction_processing")
                
                if coords:
                    lat, lng = coords
                    threat_type, icon = classify(ln)
                    
                    # Create label showing direction
                    place_label = city_clean.title()
                    if direction_type == 'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ':
                        place_label += f" (Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº)"
                    elif direction_type == 'Ñ‡ÐµÑ€ÐµÐ·':
                        place_label += f" (Ñ‡ÐµÑ€ÐµÐ·)"
                    elif direction_type == 'Ð¿Ð¾Ð²Ð·':
                        place_label += f" (Ð¿Ð¾Ð²Ð·)"
                    
                    multi_city_tracks.append({
                        'id': f"{mid}_direction_{len(multi_city_tracks)+1}",
                        'place': place_label,
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': clean_text(ln)[:500],
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': f'direction_{direction_type.replace(" ", "_")}',
                        'count': 1
                    })
                    add_debug_log(f"Created direction marker: {place_label} ({direction_type})", "direction_processing")
                else:
                    add_debug_log(f"No coordinates found for direction target: '{city_raw}' (normalized: '{city_norm}')", "direction_processing")
            
            # If we found any target cities with valid coordinates, skip general UAV processing
            if any(coords for _, coords in [(city_norm, CITY_COORDS.get(UA_CITY_NORMALIZE.get(city_raw.strip().lower(), city_raw.strip().lower()))) for _, city_raw in target_cities]):
                add_debug_log(f"Direction processing complete, skipping general UAV activity for line: '{ln}'", "direction_processing")
                continue
        
        # NEW: Create markers for general UAV activity messages (without specific direction)
        if 'Ð±Ð¿Ð»Ð°' in ln_lower or 'Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚Ð½Ð¸Ðº' in ln_lower or 'Ð´Ñ€Ð¾Ð½' in ln_lower:
            add_debug_log(f"UAV activity detected in line: '{ln}', oblast_hdr: '{oblast_hdr}'", "uav_processing")
            # Check if we have a region and this is a UAV message
            if oblast_hdr:
                add_debug_log(f"Processing UAV with region context: '{oblast_hdr}'", "uav_processing")
                # Find the main city of the region to place the marker
                region_cities = {
                    'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': 'ÑÑƒÐ¼Ð¸',
                    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 
                    'Ñ…ÐµÑ€ÑÐ¾Ð½Ñ‰Ð¸Ð½Ð°': 'Ñ…ÐµÑ€ÑÐ¾Ð½',
                    'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ…Ð°Ñ€ÐºÑ–Ð²',
                    'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': 'ÐºÑ€Ð°Ð¼Ð°Ñ‚Ð¾Ñ€ÑÑŒÐº',  # safer than Ð´Ð¾Ð½ÐµÑ†ÑŒÐº
                    'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': 'ÑÑ”Ð²Ñ”Ñ€Ð¾Ð´Ð¾Ð½ÐµÑ†ÑŒÐº',
                    'Ð·Ð°Ð¿Ð¾Ñ€Ð¾Ð¶Ð¶Ñ': 'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ',
                    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾',
                    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°',
                    'ÐºÐ¸Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'ÐºÐ¸Ñ—Ð²',
                    'Ð»ÑŒÐ²Ñ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ð»ÑŒÐ²Ñ–Ð²',
                    'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': 'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº',
                    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð°': 'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ',
                    'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': 'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹',
                    'Ñ€Ñ–Ð²Ð½ÐµÐ½Ñ‰Ð¸Ð½Ð°': 'Ñ€Ñ–Ð²Ð½Ðµ',
                    'Ð²Ð¾Ð»Ð¸Ð½Ñ‰Ð¸Ð½Ð°': 'Ð»ÑƒÑ†ÑŒÐº',
                    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ñ‰Ð¸Ð½Ð°': 'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€',
                    'Ð²Ñ–Ð½Ð½Ð¸Ñ‡Ñ‡Ð¸Ð½Ð°': 'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ',
                    'Ñ‡ÐµÑ€ÐºÐ°Ñ‰Ð¸Ð½Ð°': 'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸',
                    'ÐºÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ñ‰Ð¸Ð½Ð°': 'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹',
                    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²',
                    'Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð°': 'Ð¾Ð´ÐµÑÐ°'
                }
                
                region_city = region_cities.get(oblast_hdr)
                if region_city:
                    # Check if message refers to entire region rather than specific city
                    # Skip marker creation for some regional threats, but create for KAB/aviation bombs
                    genitive_form = oblast_hdr.replace('Ñ‰Ð¸Ð½Ð°', 'Ñ‰Ð¸Ð½Ð¸')  # ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð° -> ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð¸
                    dative_form = oblast_hdr.replace('Ñ‰Ð¸Ð½Ð°', 'Ñ‰Ð¸Ð½Ñ–')    # ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð° -> ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñ–
                    accusative_form = oblast_hdr + 'Ñƒ'                  # ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð° -> ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñƒ
                    
                    is_regional_threat = any(regional_ref in ln_lower for regional_ref in [
                        f'Ð½Ð° {oblast_hdr}', f'{accusative_form}', f'{genitive_form}', f'{dative_form}',
                        f'Ð´Ð»Ñ {genitive_form}', f'Ð¿Ð¾ {dative_form}'
                    ])
                    
                    # For KAB/aviation bombs, always create marker even for regional threats
                    has_kab = any(kab_word in ln_lower for kab_word in ['ÐºÐ°Ð±', 'Ð°Ð²Ñ–Ð°Ð±Ð¾Ð¼Ð±', 'Ð°Ð²Ð¸Ð°Ð±Ð¾Ð¼Ð±'])
                    
                    if is_regional_threat and not has_kab:
                        add_debug_log(f"Skipping regional threat marker - affects entire region: {oblast_hdr} (found: {[ref for ref in [f'Ð½Ð° {oblast_hdr}', accusative_form, genitive_form, dative_form] if ref in ln_lower]})", "multi_region")
                        continue
                    
                    # Try to find coordinates for the region's main city
                    base_city = normalize_city_name(region_city)
                    base_city = UA_CITY_NORMALIZE.get(base_city, base_city)
                    coords = CITY_COORDS.get(base_city) or (SETTLEMENTS_INDEX.get(base_city) if SETTLEMENTS_INDEX else None)
                    
                    if coords:
                        lat, lng = coords
                        label = base_city.title()
                        label += f" [{oblast_hdr.title()}]"
                        
                        # Determine threat type based on message content using classify function
                        threat_type, icon = classify(ln)
                        # Keep shahed as default for UAV if classify doesn't return anything specific
                        if not threat_type:
                            threat_type = 'shahed'
                        
                        multi_city_tracks.append({
                            'id': f"{mid}_general_uav_{len(multi_city_tracks)+1}",
                            'place': label,
                            'lat': lat,
                            'lng': lng,
                            'threat_type': threat_type,
                            'text': clean_text(ln)[:500],
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': icon,
                            'source_match': 'general_uav_activity',
                            'count': 1
                        })
                        add_debug_log(f"Created general UAV marker: {label} ({threat_type})", "multi_region")
                        add_debug_log(f"MARKER CREATION: oblast_hdr='{oblast_hdr}', region_city='{region_city}', coords=({lat}, {lng})", "marker_creation")
                        continue  # move to next line
        
        # NEW: Handle UAV messages without region but with city name
        ln_lower = ln.lower()
        if (not oblast_hdr) and ('Ð±Ð¿Ð»Ð°' in ln_lower or 'Ð±ÐµÐ·Ð¿Ñ–Ð»Ð¾Ñ‚Ð½Ð¸Ðº' in ln_lower or 'Ð´Ñ€Ð¾Ð½' in ln_lower or 'Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»' in ln_lower or 'Ð²Ð¸Ð±ÑƒÑ…' in ln_lower):
            # Try to extract city name from the message
            import re
            # Pattern for messages like "â—ï¸ Ð¡Ð¸Ð½ÐµÐ»ÑŒÐ½Ð¸ÐºÐ¾Ð²Ðµ â€” 1Ñ… Ð‘Ð¿Ð›Ð Ð´Ð¾Ð²ÐºÐ¾Ð»Ð°" or "ðŸ’¥ Ð¥ÐµÑ€ÑÐ¾Ð½ â€” Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»"
            city_match = re.search(r'[â—ï¸âš ï¸ðŸ›¸ðŸ’¥]*\s*([Ð-Ð¯Ð†Ð‡Ð„Ð°-ÑÑ–Ñ—Ñ”Ò‘][Ð-Ð¯Ð°-ÑÐ†Ñ–Ð‡Ñ—Ð„Ñ”Ò‘\-\'Ê¼]{2,30}(?:ÑÑŒÐºÐµ|Ñ†ÑŒÐºÐµ|ÑÑŒÐºÐ¸Ð¹|ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½|Ð¾Ð²Ðµ|ÐµÐ²Ðµ|Ð¸Ð½Ðµ|Ð¸Ð½Ð¾|Ñ–Ð²ÐºÐ°|Ñ–Ð²ÑÑŒÐº|ÑÑŒÐº|Ð³Ñ€Ð°Ð´|Ð³Ð¾Ñ€Ð¾Ð´)?)', ln)
            if city_match:
                city_name = city_match.group(1).strip()
                
                # Normalize city name
                base_city = normalize_city_name(city_name)
                base_city = UA_CITY_NORMALIZE.get(base_city, base_city)
                coords = CITY_COORDS.get(base_city) or (SETTLEMENTS_INDEX.get(base_city) if SETTLEMENTS_INDEX else None)
                
                if coords:
                    lat, lng = coords
                    label = base_city.title()
                    
                    # Determine threat type based on message content using classify function
                    threat_type, icon = classify(ln)
                    # Keep shahed as default for UAV if classify doesn't return anything specific
                    if not threat_type:
                        threat_type = 'shahed'
                        icon = 'shahed.png'
                    
                    multi_city_tracks.append({
                        'id': f"{mid}_city_threat_{len(multi_city_tracks)+1}",
                        'place': label,
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': clean_text(ln)[:500],
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': 'city_threat_activity',
                        'count': 1
                    })
                    add_debug_log(f"Created city threat marker: {label} ({threat_type})", "multi_region")
                    continue  # move to next line
        
        # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ Ð³Ð¾Ñ€Ð¾Ð´ Ð¸ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, "2Ñ… Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð”ÐµÑÐ½Ñƒ")
        import re
        # --- NEW: Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€Ð°ÐºÐµÑ‚Ð½Ñ‹Ñ… ÑÑ‚Ñ€Ð¾Ðº Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð±Ð»Ð¾ÐºÐ° ---
        # ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹: "1 Ñ€Ð°ÐºÐµÑ‚Ð° Ð½Ð° Ð¥Ð¾Ð»Ð¼Ð¸", "2 Ñ€Ð°ÐºÐµÑ‚Ð¸ Ð½Ð° Ð›ÑƒÐ±Ð½Ð¸", "3 Ñ€Ð°ÐºÐµÑ‚Ñ‹ Ð½Ð° Ð›ÑƒÐ±Ð½Ð¸", "Ñ€Ð°ÐºÐµÑ‚Ð° Ð½Ð° <Ð¼Ñ–ÑÑ‚Ð¾>"
        rocket_city = None; rocket_count = 1
        mr = re.search(r'(?:^|\b)(?:([0-9]+)\s*)?(Ñ€Ð°ÐºÐµÑ‚Ð°|Ñ€Ð°ÐºÐµÑ‚Ð¸|Ñ€Ð°ÐºÐµÑ‚)\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{2,40})', ln, re.IGNORECASE)
        if mr:
            if mr.group(1):
                try: rocket_count = int(mr.group(1))
                except: rocket_count = 1
            rocket_city = mr.group(3)
        if rocket_city:
            base_r = normalize_city_name(rocket_city)
            base_r = UA_CITY_NORMALIZE.get(base_r, base_r)
            coords_r = CITY_COORDS.get(base_r) or (SETTLEMENTS_INDEX.get(base_r) if SETTLEMENTS_INDEX else None)
            if not coords_r and oblast_hdr:
                combo_r = f"{base_r} {oblast_hdr}"
                coords_r = CITY_COORDS.get(combo_r) or (SETTLEMENTS_INDEX.get(combo_r) if SETTLEMENTS_INDEX else None)
            if coords_r:
                lat, lng = coords_r
                label = UA_CITY_NORMALIZE.get(base_r, base_r).title()
                if rocket_count > 1:
                    label += f" ({rocket_count})"
                if oblast_hdr and oblast_hdr not in label.lower():
                    label += f" [{oblast_hdr.title()}]"
                multi_city_tracks.append({
                    'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                    'threat_type': 'rszv', 'text': clean_text(ln)[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': 'rszv.png', 'source_match': 'multiline_oblast_city_rocket', 'count': rocket_count
                })
                continue  # Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼ Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ ÑÑ‚Ñ€Ð¾ÐºÐµ (Ð½Ðµ Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ ÐºÐ°Ðº Ð‘Ð¿Ð›Ð)
        # --- NEW: Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹ ÐºÑ€Ñ‹Ð»Ð°Ñ‚Ñ‹Ñ… Ñ€Ð°ÐºÐµÑ‚ ("Ð“Ñ€ÑƒÐ¿Ð°/Ð“Ñ€ÑƒÐ¿Ð¸ ÐšÐ  ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° <Ð³Ð¾Ñ€Ð¾Ð´>") ---
        kr_city = None; kr_count = 1
        # Primary straightforward pattern for "Ð“Ñ€ÑƒÐ¿Ð°/Ð“Ñ€ÑƒÐ¿Ð¸ ÐšÐ  ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° <Ð¼Ñ–ÑÑ‚Ð¾>"
        mkr = re.search(r'(?:^|\b)(?:([0-9]+)[xÑ…]?\s*)?Ð³Ñ€ÑƒÐ¿[Ð°Ð¸]\s+ÐºÑ€\b.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
        if not mkr:
            # Tolerant pattern allowing missing leading "Ð³" or space glitches / lost letters
            mkr = re.search(r'(?:^|\b)(?:([0-9]+)[xÑ…]?\s*)?(?:Ð³)?Ñ€ÑƒÐ¿[Ð°Ð¸]\s*(?:Ðº)?Ñ€\b.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
        if not mkr and 'Ð³Ñ€ÑƒÐ¿' in ln.lower() and 'ÐºÑƒÑ€Ñ' in ln.lower() and ' Ð½Ð° ' in ln.lower():
            # Very loose fallback if 'ÐšÐ ' fragment dropped; capture after last 'Ð½Ð°'
            after = ln.rsplit('Ð½Ð°',1)[-1].strip()
            after = re.split(r'[,.!?:;]', after)[0].strip()
            if len(after) >= 3:
                class Dummy: pass
                mkr = Dummy(); mkr.group = lambda i: None if i==1 else after
        if mkr:
            try:
                log.info(f"KR_MATCH line='{ln}' groups={mkr.groups()}")
            except Exception:
                pass
            if mkr.group(1):
                try: kr_count = int(mkr.group(1))
                except: kr_count = 1
            kr_city = mkr.group(2)
        if kr_city:
            base_k = normalize_city_name(kr_city)
            base_k = UA_CITY_NORMALIZE.get(base_k, base_k)
            coords_k = CITY_COORDS.get(base_k) or (SETTLEMENTS_INDEX.get(base_k) if SETTLEMENTS_INDEX else None)
            if not coords_k and oblast_hdr:
                combo_k = f"{base_k} {oblast_hdr}"
                coords_k = CITY_COORDS.get(combo_k) or (SETTLEMENTS_INDEX.get(combo_k) if SETTLEMENTS_INDEX else None)
            if coords_k:
                lat, lng = coords_k
                label = UA_CITY_NORMALIZE.get(base_k, base_k).title()
                if kr_count > 1:
                    label += f" ({kr_count})"
                if oblast_hdr and oblast_hdr not in label.lower():
                    label += f" [{oblast_hdr.title()}]"
                multi_city_tracks.append({
                    'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                    'threat_type': 'raketa', 'text': ln[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': 'raketa.png', 'source_match': 'multiline_oblast_city_kr_group', 'count': kr_count
                })
                continue
        # Universal KR fallback (handles degraded OCR lines like '3Ñ… Ñ€ÑƒÐ¿Ð¸  ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ñ€Ð¸Ð»ÑƒÐºÐ¸')
        low_ln = ln.lower()
        if ('ÐºÑƒÑ€Ñ' in low_ln and ' Ð½Ð° ' in low_ln and ('Ð³Ñ€ÑƒÐ¿' in low_ln or ' ÐºÑ€' in low_ln)):
            # Extract count if present at start or before 'Ð³Ñ€ÑƒÐ¿'
            mcnt = re.search(r'^(\d+)[xÑ…]?\s*', low_ln)
            count_guess = 1
            if mcnt:
                try: count_guess = int(mcnt.group(1))
                except: pass
            # Try after last 'Ð½Ð° '
            parts = low_ln.rsplit(' Ð½Ð° ', 1)
            if len(parts) == 2:
                cand = parts[1]
                cand = re.split(r'[\n,.!?:;]', cand)[0].strip()
                # strip residual non-letter chars
                cand_clean = re.sub(r"[^A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]", '', cand).strip()
                if len(cand_clean) >= 3:
                    base_f = normalize_city_name(cand_clean)
                    base_f = UA_CITY_NORMALIZE.get(base_f, base_f)
                    coords_f = CITY_COORDS.get(base_f) or (SETTLEMENTS_INDEX.get(base_f) if SETTLEMENTS_INDEX else None)
                    if not coords_f and oblast_hdr:
                        combo_f = f"{base_f} {oblast_hdr}"
                        coords_f = CITY_COORDS.get(combo_f) or (SETTLEMENTS_INDEX.get(combo_f) if SETTLEMENTS_INDEX else None)
                    # Fuzzy repair: if still not found, try restoring a potentially lost first letter
                    if not coords_f:
                        for pref in ['Ð½','Ðº','Ñ‡','Ð¿','Ð³','Ñ','Ð²','Ð±','Ð´','Ð¼','Ñ‚','Ð»']:
                            test_base = pref + base_f
                            coords_try = CITY_COORDS.get(test_base) or (SETTLEMENTS_INDEX.get(test_base) if SETTLEMENTS_INDEX else None)
                            if not coords_try and oblast_hdr:
                                combo_try = f"{test_base} {oblast_hdr}"
                                coords_try = CITY_COORDS.get(combo_try) or (SETTLEMENTS_INDEX.get(combo_try) if SETTLEMENTS_INDEX else None)
                            if coords_try:
                                base_f = test_base
                                coords_f = coords_try
                                try: log.info(f"KR_FUZZ_REPAIR first_letter pref='{pref}' -> {base_f}")
                                except Exception: pass
                                break
                    if coords_f:
                        lat, lng = coords_f
                        label = UA_CITY_NORMALIZE.get(base_f, base_f).title()
                        if count_guess > 1:
                            label += f" ({count_guess})"
                        if oblast_hdr and oblast_hdr not in label.lower():
                            label += f" [{oblast_hdr.title()}]"
                        multi_city_tracks.append({
                            'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                            'threat_type': 'raketa', 'text': ln[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': 'raketa.png', 'source_match': 'multiline_oblast_city_kr_group_fallback2', 'count': count_guess
                        })
                        continue
        # Generic course fallback (any remaining 'ÐºÑƒÑ€Ñ' + ' Ð½Ð° ' line not yet matched)
        if 'ÐºÑƒÑ€Ñ' in low_ln and ' Ð½Ð° ' in low_ln and not any(tag in low_ln for tag in ['Ð±Ð¿Ð»Ð°','shahed']) and not any(mt['id'] == f"{mid}_mc{len(multi_city_tracks)+1}" for mt in multi_city_tracks):
            parts = low_ln.rsplit(' Ð½Ð° ',1)
            if len(parts)==2:
                cand = re.split(r'[\n,.!?:;]', parts[1])[0].strip()
                cand = re.sub(r"[^A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]", '', cand)
                if len(cand) >= 3:
                    base_g = normalize_city_name(cand)
                    base_g = UA_CITY_NORMALIZE.get(base_g, base_g)
                    coords_g = CITY_COORDS.get(base_g) or (SETTLEMENTS_INDEX.get(base_g) if SETTLEMENTS_INDEX else None)
                    if not coords_g and oblast_hdr:
                        combo_g = f"{base_g} {oblast_hdr}"
                        coords_g = CITY_COORDS.get(combo_g) or (SETTLEMENTS_INDEX.get(combo_g) if SETTLEMENTS_INDEX else None)
                    # NEW: allow oblast center lookup if destination is a region (e.g. Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð° / Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ñƒ)
                    if not coords_g and base_g in OBLAST_CENTERS:
                        coords_g = OBLAST_CENTERS[base_g]
                        try: log.info(f"GENERIC_COURSE_REGION dest='{base_g}' -> oblast center")
                        except Exception: pass
                    if not coords_g:
                        for pref in ['Ðº','Ñ','Ð¾','Ð»','Ð±','Ð²','Ð¶','Ñ‚','Ñ','Ñƒ','Ñ€','Ð½','Ð¿','Ð³','Ñ‡']:
                            test = pref + base_g
                            coords_try = CITY_COORDS.get(test) or (SETTLEMENTS_INDEX.get(test) if SETTLEMENTS_INDEX else None)
                            if not coords_try and oblast_hdr:
                                combo_try = f"{test} {oblast_hdr}"
                                coords_try = CITY_COORDS.get(combo_try) or (SETTLEMENTS_INDEX.get(combo_try) if SETTLEMENTS_INDEX else None)
                            if coords_try:
                                base_g = test
                                coords_g = coords_try
                                try: log.info(f"GENERIC_FUZZ_CITY pref='{pref}' -> {base_g}")
                                except Exception: pass
                                break
                    if not coords_g:
                        for pref in ['Ð½','Ðº','Ñ‡','Ð¿','Ð³','Ñ','Ð²','Ð±','Ð´','Ð¼','Ñ‚','Ð»']:
                            test_base = pref + base_g
                            coords_try = CITY_COORDS.get(test_base) or (SETTLEMENTS_INDEX.get(test_base) if SETTLEMENTS_INDEX else None)
                            if not coords_try and oblast_hdr:
                                combo_try = f"{test_base} {oblast_hdr}"
                                coords_try = CITY_COORDS.get(combo_try) or (SETTLEMENTS_INDEX.get(combo_try) if SETTLEMENTS_INDEX else None)
                            if coords_try:
                                base_g = test_base
                                coords_g = coords_try
                                try: log.info(f"GENERIC_COURSE_FUZZ pref='{pref}' -> {base_g}")
                                except Exception: pass
                                break
                    if coords_g:
                        lat, lng = coords_g
                        label = UA_CITY_NORMALIZE.get(base_g, base_g).title()
                        if oblast_hdr and oblast_hdr not in label.lower():
                            label += f" [{oblast_hdr.title()}]"
                        multi_city_tracks.append({
                            'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                            'threat_type': 'raketa', 'text': ln[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': 'raketa.png', 'source_match': 'multiline_oblast_city_course_generic', 'count': 1
                        })
                        continue
        # Fallback KR pattern if above failed but line mentions 'ÐšÐ ' and 'ÐºÑƒÑ€Ñ'
        if 'ÐºÑ€' in ln.lower() and 'ÐºÑƒÑ€Ñ' in ln.lower() and ' Ð½Ð° ' in f" {ln.lower()} ":
            mkr2 = re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
            if mkr2:
                base_k2 = normalize_city_name(mkr2.group(1))
                base_k2 = UA_CITY_NORMALIZE.get(base_k2, base_k2)
                coords_k2 = CITY_COORDS.get(base_k2) or (SETTLEMENTS_INDEX.get(base_k2) if SETTLEMENTS_INDEX else None)
                if not coords_k2 and oblast_hdr:
                    combo_k2 = f"{base_k2} {oblast_hdr}"
                    coords_k2 = CITY_COORDS.get(combo_k2) or (SETTLEMENTS_INDEX.get(combo_k2) if SETTLEMENTS_INDEX else None)
                if not coords_k2:
                    for pref in ['Ð½','Ðº','Ñ‡','Ð¿','Ð³','Ñ','Ð²','Ð±','Ð´','Ð¼','Ñ‚','Ð»']:
                        test_base = pref + base_k2
                        coords_try = CITY_COORDS.get(test_base) or (SETTLEMENTS_INDEX.get(test_base) if SETTLEMENTS_INDEX else None)
                        if not coords_try and oblast_hdr:
                            combo_try = f"{test_base} {oblast_hdr}"
                            coords_try = CITY_COORDS.get(combo_try) or (SETTLEMENTS_INDEX.get(combo_try) if SETTLEMENTS_INDEX else None)
                        if coords_try:
                            base_k2 = test_base
                            coords_k2 = coords_try
                            try: log.info(f"KR_FALLBACK_FUZZ pref='{pref}' -> {base_k2}")
                            except Exception: pass
                            break
                if coords_k2:
                    lat, lng = coords_k2
                    label = UA_CITY_NORMALIZE.get(base_k2, base_k2).title()
                    if oblast_hdr and oblast_hdr not in label.lower():
                        label += f" [{oblast_hdr.title()}]"
                    multi_city_tracks.append({
                        'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                        'threat_type': 'raketa', 'text': ln[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': 'raketa.png', 'source_match': 'multiline_oblast_city_kr_group_fallback', 'count': 1
                    })
                    continue
        # Ð Ð°Ð·Ñ€ÐµÑˆÐ°ÐµÐ¼ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ (Ð´Ð¾ 3 ÑÐ»Ð¾Ð²) Ð´Ð¾ ÐºÐ¾Ð½Ñ†Ð° ÑÑ‚Ñ€Ð¾ÐºÐ¸ / Ð·Ð½Ð°ÐºÐ° Ð¿Ñ€ÐµÐ¿Ð¸Ð½Ð°Ð½Ð¸Ñ
        m = re.search(r'(\d+)[xÑ…]?\s*Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
        if m:
            count = int(m.group(1))
            city = m.group(2)
        else:
            # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð²Ð¸Ð´Ð° "7Ñ… Ð‘Ð¿Ð›Ð Ð¿Ð¾Ð²Ð· <Ð¼Ñ–ÑÑ‚Ð¾> ..." Ð¸Ð»Ð¸ "Ð‘Ð¿Ð›Ð Ð¿Ð¾Ð²Ð· <Ð¼Ñ–ÑÑ‚Ð¾>"
            m2 = re.search(r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
            if m2:
                count = 1
                city = m2.group(1)
            else:
                m3 = re.search(r'(\d+)[xÑ…]?\s*Ð±Ð¿Ð»Ð°.*?Ð¿Ð¾Ð²Ð·\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
                if m3:
                    count = int(m3.group(1))
                    city = m3.group(2)
                else:
                    m4 = re.search(r'Ð±Ð¿Ð»Ð°.*?Ð¿Ð¾Ð²Ð·\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=[,\.\!\?;]|$)', ln, re.IGNORECASE)
                    count = 1
                    city = m4.group(1) if m4 else None
        # --- NEW: Shahed lines inside multi-line block (e.g. '2 ÑˆÐ°Ñ…ÐµÐ´Ð¸ Ð½Ð° Ð¡Ñ‚Ð°Ñ€Ð¸Ð¹ Ð¡Ð°Ð»Ñ‚Ñ–Ð²', '1 ÑˆÐ°Ñ…ÐµÐ´ Ð½Ð° ÐœÐµÑ€ÐµÑ„Ð° / Ð‘Ð¾Ñ€ÐºÐ¸') ---
        if not city:
            m_sha = re.search(r'^(?:([0-9]+)\s*[xÑ…]?\s*)?ÑˆÐ°Ñ…ÐµÐ´(?:Ð¸|Ñ–Ð²)?\s+Ð½Ð°\s+(.+)$', ln.strip(), re.IGNORECASE)
            if m_sha:
                try:
                    scount = int(m_sha.group(1) or '1')
                except Exception:
                    scount = 1
                cities_part = m_sha.group(2)
                raw_parts = re.split(r'\s*/\s*|\s*,\s*|\s*;\s*|\s+Ñ–\s+|\s+Ñ‚Ð°\s+', cities_part, flags=re.IGNORECASE)
                for ci in raw_parts:
                    c_raw = ci.strip().strip('.').strip()
                    if not c_raw or len(c_raw) < 2:
                        continue
                    cbase = normalize_city_name(c_raw)
                    cbase = UA_CITY_NORMALIZE.get(cbase, cbase)
                    coords_s = CITY_COORDS.get(cbase) or (SETTLEMENTS_INDEX.get(cbase) if SETTLEMENTS_INDEX else None)
                    if not coords_s and oblast_hdr:
                        combo_s = f"{cbase} {oblast_hdr}"
                        coords_s = CITY_COORDS.get(combo_s) or (SETTLEMENTS_INDEX.get(combo_s) if SETTLEMENTS_INDEX else None)
                    if not coords_s:
                        for pref in ['Ñ','Ð¼','Ðº','Ð±','Ð³','Ñ‡','Ð½','Ð¿','Ñ‚','Ð²','Ð»']:
                            test = pref + cbase
                            coords_try = CITY_COORDS.get(test) or (SETTLEMENTS_INDEX.get(test) if SETTLEMENTS_INDEX else None)
                            if not coords_try and oblast_hdr:
                                combo_try = f"{test} {oblast_hdr}"
                                coords_try = CITY_COORDS.get(combo_try) or (SETTLEMENTS_INDEX.get(combo_try) if SETTLEMENTS_INDEX else None)
                            if coords_try:
                                cbase = test; coords_s = coords_try; break
                    if not coords_s:
                        continue
                    lat, lng = coords_s
                    label = UA_CITY_NORMALIZE.get(cbase, cbase).title()
                    per_count = scount if len(raw_parts) == 1 else 1
                    if oblast_hdr and oblast_hdr not in label.lower():
                        label += f" [{oblast_hdr.title()}]"
                    
                    # Create multiple tracks for multiple shaheds
                    tracks_to_create = max(1, per_count)
                    for i in range(tracks_to_create):
                        track_label = label
                        if tracks_to_create > 1:
                            track_label += f" #{i+1}"
                        
                        # Add small coordinate offsets to prevent marker overlap
                        marker_lat = lat
                        marker_lng = lng
                        if tracks_to_create > 1:
                            # Create a circular pattern around the target
                            import math
                            angle = (2 * math.pi * i) / tracks_to_create
                            offset_distance = 0.01  # ~1km offset
                            marker_lat += offset_distance * math.cos(angle)
                            marker_lng += offset_distance * math.sin(angle)
                        
                        multi_city_tracks.append({
                            'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': track_label, 'lat': marker_lat, 'lng': marker_lng,
                            'threat_type': 'shahed', 'text': clean_text(ln)[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': 'shahed.png', 'source_match': 'multiline_oblast_city_shahed', 'count': 1
                        })
                continue
        
        # --- NEW: Simple "X Ð‘Ð¿Ð›Ð Ð½Ð° <city>" pattern (e.g. '1 Ð‘Ð¿Ð›Ð Ð½Ð° ÐšÐ¾Ð·ÐµÐ»ÐµÑ†ÑŒ', '2 Ð‘Ð¿Ð›Ð Ð½Ð° ÐšÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÑƒ') ---
        if not city:
            print(f"DEBUG: Checking simple Ð‘Ð¿Ð›Ð pattern for line: '{ln}'")
            m_simple = re.search(r'(\d+)\s+Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,40}?)(?=\s|$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_simple:
                try:
                    count = int(m_simple.group(1))
                except Exception:
                    count = 1
                city = m_simple.group(2).strip()
                print(f"DEBUG: Found simple Ð‘Ð¿Ð›Ð pattern - count: {count}, city: '{city}'")
            elif re.search(r'Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,}', ln, re.IGNORECASE):
                # Fallback for "Ð‘Ð¿Ð›Ð Ð½Ð° <city>" without count - handle cities with parentheses like "ÐšÑ€Ð¸Ð²Ð¸Ð¹ Ñ€Ñ–Ð³ (Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°)"
                m_simple_no_count = re.search(r'Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,}?)(?:\s*\([^)]*\))?(?=\s*$|[,\.\!\?;])', ln, re.IGNORECASE)
                if m_simple_no_count:
                    count = 1
                    city = m_simple_no_count.group(1).strip()
                    print(f"DEBUG: Found simple Ð‘Ð¿Ð›Ð pattern (no count) - city: '{city}'")
        
        # --- NEW: Handle "X Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ City1, City2" pattern (e.g. "4 Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ÐšÐ°Ñ€Ð»Ñ–Ð²ÐºÐ¸, ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð¸") ---
        if not city:
            print(f"DEBUG: Checking 'X Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ' pattern for line: '{ln}'")
            m_naprymku = re.search(r'(\d+)\s+Ñƒ\s+Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s,]{5,})(?=\s*$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_naprymku:
                try:
                    count = int(m_naprymku.group(1))
                except Exception:
                    count = 1
                cities_raw = m_naprymku.group(2).strip()
                print(f"DEBUG: Found 'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ' pattern - count: {count}, cities: '{cities_raw}'")
                
                # Split cities by comma
                cities_list = [c.strip() for c in cities_raw.split(',') if c.strip()]
                for city_name in cities_list:
                    base = normalize_city_name(city_name)
                    base = UA_CITY_NORMALIZE.get(base, base)
                    coords = CITY_COORDS.get(base)
                    
                    # If not found, try to handle declensions (ending with -Ð¸, -Ð¼Ð¸, -Ñƒ, etc)
                    if not coords and base:
                        if base.endswith('Ñ–') or base.endswith('Ð¸'):
                            base_nom = base[:-1] + 'Ð°'  # ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÐ¸ -> ÐºÐ°Ñ€Ð»Ñ–Ð²ÐºÐ°
                            coords = CITY_COORDS.get(base_nom)
                        elif base.endswith('Ñƒ'):
                            base_nom = base[:-1] + 'Ð°'  # Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñƒ -> Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°  
                            coords = CITY_COORDS.get(base_nom)
                        elif base.endswith('Ð¼Ð¸'):
                            base_nom = base[:-2] + 'Ð°'  # ÐºÐ¸Ñ—Ð²Ð¼Ð¸ -> ÐºÐ¸Ñ”Ð²Ð° -> doesn't work, try other variants
                            coords = CITY_COORDS.get(base_nom)
                    
                    if coords:
                        lat, lng = coords
                        multi_city_tracks.append({
                            'id': f"{mid}_naprymku{len(multi_city_tracks)+1}", 'place': city_name.title(), 'lat': lat, 'lng': lng,
                            'threat_type': 'shahed', 'text': clean_text(ln)[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': 'shahed.png', 'source_match': 'naprymku_pattern', 'count': count
                        })
                        print(f"DEBUG: Added marker for '{city_name}' at {lat}, {lng}")
                if multi_city_tracks:
                    continue
                
        # --- NEW: Handle "X Ð‘Ð¿Ð›Ð City1 / City2" pattern (e.g. "2Ñ… Ð‘Ð¿Ð›Ð Ð“Ð½Ñ–Ð´Ð¸Ð½ / Ð‘Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ") ---
        if not city:
            print(f"DEBUG: Checking Ð‘Ð¿Ð›Ð city/city pattern for line: '{ln}'")
            m_cities = re.search(r'(\d+)Ñ…?\s+Ð±Ð¿Ð»Ð°\s+(?:Ð½Ð°\s+)?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)\s*/\s*([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)(?=\s|$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_cities:
                try:
                    count = int(m_cities.group(1))
                except Exception:
                    count = 1
                city1 = m_cities.group(2).strip()
                city2 = m_cities.group(3).strip()
                print(f"DEBUG: Found Ð‘Ð¿Ð›Ð city/city pattern - count: {count}, cities: '{city1}' / '{city2}'")
                
                # Process both cities separately
                for city_name in [city1, city2]:
                    base = normalize_city_name(city_name)
                    base = UA_CITY_NORMALIZE.get(base, base)
                    coords = CITY_COORDS.get(base)
                    if coords:
                        print(f"DEBUG: Creating Ð‘Ð¿Ð›Ð track for {city_name} at {coords}")
                        multi_city_tracks.append({
                            'lat': coords[0],
                            'lon': coords[1],
                            'name': city_name,
                            'type': 'Ð‘Ð¿Ð›Ð',
                            'time': date_str,
                            'id': mid,
                            'message': text[:100] + ('...' if len(text) > 100 else ''),
                            'channel': channel
                        })
                    else:
                        print(f"DEBUG: No coordinates found for {city_name} (base: {base})")
                
                # Set city to processed to prevent further processing
                city = f"{city1} / {city2}"
        # --- NEW: Handle "Ð¼Ñ–Ð¶ X Ñ‚Ð° Y" pattern (e.g. "Ð¼Ñ–Ð¶ ÐšÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÐ¾ÑŽ Ñ‚Ð° ÐœÐµÐ½Ð¾ÑŽ") ---
        if not city:
            m_between = re.search(r'Ð¼Ñ–Ð¶\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)\s+Ñ‚Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)(?=\s|$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_between:
                city1 = m_between.group(1).strip()
                city2 = m_between.group(2).strip()
                # Try to geocode both cities and place marker at midpoint
                base1 = normalize_city_name(city1)
                base2 = normalize_city_name(city2)
                base1 = UA_CITY_NORMALIZE.get(base1, base1)
                base2 = UA_CITY_NORMALIZE.get(base2, base2)
                
                coords1 = CITY_COORDS.get(base1) or (SETTLEMENTS_INDEX.get(base1) if SETTLEMENTS_INDEX else None)
                coords2 = CITY_COORDS.get(base2) or (SETTLEMENTS_INDEX.get(base2) if SETTLEMENTS_INDEX else None)
                
                if not coords1 and oblast_hdr:
                    combo1 = f"{base1} {oblast_hdr}"
                    coords1 = CITY_COORDS.get(combo1) or (SETTLEMENTS_INDEX.get(combo1) if SETTLEMENTS_INDEX else None)
                if not coords2 and oblast_hdr:
                    combo2 = f"{base2} {oblast_hdr}"
                    coords2 = CITY_COORDS.get(combo2) or (SETTLEMENTS_INDEX.get(combo2) if SETTLEMENTS_INDEX else None)
                
                if coords1 and coords2:
                    # Place marker at midpoint
                    lat = (coords1[0] + coords2[0]) / 2
                    lng = (coords1[1] + coords2[1]) / 2
                    label = f"ÐœÑ–Ð¶ {base1.title()} Ñ‚Ð° {base2.title()}"
                    if oblast_hdr and oblast_hdr not in label.lower():
                        label += f" [{oblast_hdr.title()}]"
                    
                    # Extract count from beginning of line if present
                    count_match = re.search(r'^(\d+)\s*Ð±Ð¿Ð»Ð°', ln, re.IGNORECASE)
                    count = int(count_match.group(1)) if count_match else 1
                    
                    multi_city_tracks.append({
                        'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                        'threat_type': 'shahed', 'text': clean_text(ln)[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': 'shahed.png', 'source_match': 'multiline_oblast_city_between', 'count': count
                    })
                    continue
        
        # --- NEW: Handle "Ð½ÐµÐ¿Ð¾Ð´Ð°Ð»Ñ–Ðº X" pattern (e.g. "Ð½ÐµÐ¿Ð¾Ð´Ð°Ð»Ñ–Ðº Ð†Ñ‡Ð½Ñ–") ---
        if not city:
            m_near = re.search(r'Ð½ÐµÐ¿Ð¾Ð´Ð°Ð»Ñ–Ðº\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)(?=\s|$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_near:
                city = m_near.group(1).strip()
                # Extract count from beginning of line if present
                count_match = re.search(r'^(\d+)\s*Ð±Ð¿Ð»Ð°', ln, re.IGNORECASE)
                count = int(count_match.group(1)) if count_match else 1
        
        # --- NEW: Handle "Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ– X" pattern (e.g. "Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ– ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿Ñƒ") ---
        if not city:
            m_area = re.search(r'Ð²\s+Ñ€Ð°Ð¹Ð¾Ð½Ñ–\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\'Ê¼`\s]{3,30}?)(?=\s|$|[,\.\!\?;])', ln, re.IGNORECASE)
            if m_area:
                city = m_area.group(1).strip()
                # Extract count from beginning of line if present
                count_match = re.search(r'^(\d+)\s*Ð±Ð¿Ð»Ð°', ln, re.IGNORECASE)
                count = int(count_match.group(1)) if count_match else 1
        
        if city:
            print(f"DEBUG: Processing city '{city}' with oblast_hdr '{oblast_hdr}' and count {count}")
            base = normalize_city_name(city)
            print(f"DEBUG: Normalized city name: '{base}'")
            # ÐŸÑ€Ð¾ÑÑ‚ÐµÐ¹ÑˆÐ°Ñ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð°Ð´ÐµÐ¶Ð° -> Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ ("Ð²ÐµÐ»Ð¸ÐºÑƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ" -> "Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°")
            if base.endswith('Ñƒ Ð´Ð¸Ð¼ÐµÑ€ÐºÑƒ') and 'Ð²ÐµÐ»Ð¸Ðº' in base:
                base = 'Ð²ÐµÐ»Ð¸ÐºÐ° Ð´Ð¸Ð¼ÐµÑ€ÐºÐ°'
            # ÐžÐ±Ñ‰Ð°Ñ Ð¼Ð¾Ñ€Ñ„Ð¾Ð»Ð¾Ð³Ð¸Ñ: Ð·Ð°Ð¼ÐµÐ½ÑÐµÐ¼ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ "ÐºÑƒ"->"ÐºÐ°", "ÑŽ"->"Ñ" Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°
            if base.endswith('ÐºÑƒ '):
                base = base[:-3] + 'ÐºÐ° '
            elif base.endswith('ÐºÑƒ'):
                base = base[:-2] + 'ÐºÐ°'
            if base.endswith('ÑŽ '):
                base = base[:-3] + 'Ñ '
            elif base.endswith('ÑŽ'):
                base = base[:-1] + 'Ñ'
            # ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼Ñ‹ Ñ‡ÐµÑ€ÐµÐ· UA_CITY_NORMALIZE ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
            base = UA_CITY_NORMALIZE.get(base, base)
            if base == 'Ñ‚Ñ€Ð¾Ñ”Ñ‰Ð¸Ð½Ñƒ':
                base = 'Ñ‚Ñ€Ð¾Ñ”Ñ‰Ð¸Ð½Ð°'
                
            # Use enhanced coordinate lookup with Nominatim fallback and region context
            coords = get_coordinates_enhanced(base, region=oblast_hdr, context="Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð°")
            
            print(f"DEBUG: Enhanced lookup for '{base}'" + (f" in {oblast_hdr}" if oblast_hdr else "") + f": {coords}")
            
            if not coords and oblast_hdr:
                # Legacy combo lookup as fallback
                combo = f"{base} {oblast_hdr}"
                print(f"DEBUG: Trying legacy combo lookup for '{combo}'")
                coords = CITY_COORDS.get(combo)
                if not coords and SETTLEMENTS_INDEX:
                    coords = SETTLEMENTS_INDEX.get(combo)
                print(f"DEBUG: Combo lookup result: {coords}")
            if not coords:
                print(f"DEBUG: Calling ensure_city_coords_with_message_context for '{base}' with oblast context '{oblast_hdr}'")
                # Try with full message context first to get oblast-specific coordinates
                context_message = f"{oblast_hdr} {original_text if 'original_text' in locals() else text}"
                coords = ensure_city_coords_with_message_context(base, context_message)
                if not coords:
                    print(f"DEBUG: Context-based lookup failed, trying standard ensure_city_coords for '{base}'")
                    coords = ensure_city_coords(base)
                print(f"DEBUG: ensure_city_coords result: {coords}")
            if coords:
                print(f"DEBUG: Found coords {coords} for city '{base}', creating track")
                # Handle both 2-tuple (lat, lng) and 3-tuple (lat, lng, approx_flag) returns
                if len(coords) == 3:
                    lat, lng, approx_flag = coords
                else:
                    lat, lng = coords
                    approx_flag = False
                threat_type, icon = 'shahed', 'shahed.png'
                label = UA_CITY_NORMALIZE.get(base, base).title()
                if oblast_hdr and oblast_hdr not in label.lower():
                    label += f" [{oblast_hdr.title()}]"
                
                # Create multiple tracks for multiple drones instead of one track with count
                tracks_to_create = max(1, count)
                for i in range(tracks_to_create):
                    track_label = label
                    if tracks_to_create > 1:
                        track_label += f" #{i+1}"
                    
                    # Add small coordinate offsets to prevent marker overlap
                    marker_lat = lat
                    marker_lng = lng
                    if tracks_to_create > 1:
                        # Create a circular pattern around the target
                        import math
                        angle = (2 * math.pi * i) / tracks_to_create
                        offset_distance = 0.01  # ~1km offset
                        marker_lat += offset_distance * math.cos(angle)
                        marker_lng += offset_distance * math.sin(angle)
                    
                    print(f"DEBUG: Creating track {i+1}/{tracks_to_create} with label '{track_label}' at {marker_lat}, {marker_lng}")
                    multi_city_tracks.append({
                        'id': f"{mid}_mc{len(multi_city_tracks)+1}", 'place': track_label, 'lat': marker_lat, 'lng': marker_lng,
                        'threat_type': threat_type, 'text': clean_text(ln)[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'multiline_oblast_city', 'count': 1
                    })
            else:
                print(f"DEBUG: No coordinates found for city '{base}'")
    print(f"DEBUG: Multi-city tracks processing complete. Found {len(multi_city_tracks)} tracks")
    if multi_city_tracks:
        print(f"DEBUG: Returning {len(multi_city_tracks)} multi-city tracks")
        # Combine with priority result if available
        if 'priority_result' in locals() and priority_result:
            combined_result = priority_result + multi_city_tracks
            add_debug_log(f"Combined priority result ({len(priority_result)}) with multi-city tracks ({len(multi_city_tracks)}) = {len(combined_result)} total", "priority_combine")
            return combined_result
        return multi_city_tracks
    # --- Detect and split multiple city targets in one message ---
    import re
    multi_city_tracks = []
    # 1. Patterns: 'Ð½Ð° <Ð³Ð¾Ñ€Ð¾Ð´>', 'Ð¿Ð¾Ð²Ð· <Ð³Ð¾Ñ€Ð¾Ð´>'
    # Ð—Ð°Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ð´Ð½Ð¾- Ð¸Ð»Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ÑÐ»Ð¾Ð²Ð½Ñ‹Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾ÑÐ»Ðµ "Ð½Ð°" / "Ð¿Ð¾Ð²Ð·" Ð´Ð¾ Ð·Ð½Ð°ÐºÐ° Ð¿Ñ€ÐµÐ¿Ð¸Ð½Ð°Ð½Ð¸Ñ / ÐºÐ¾Ð½Ñ†Ð° ÑÑ‚Ñ€Ð¾ÐºÐ¸
    city_patterns = re.findall(r'(?:Ð½Ð°|Ð¿Ð¾Ð²Ð·)\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘Ê¼`â€™\-\s]{3,40}?)(?=[,\.\n;:!\?]|$)', text.lower())
    # 2. Patterns: Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ Ð¸Ð»Ð¸ ÑÐ»ÑÑˆ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: "ÑˆÐ¸ÑˆÐ°ÐºÐ¸, Ð³Ð»Ð¾Ð±Ð¸Ð½Ðµ, Ñ€Ð¾Ð¼Ð¾Ð´Ð°Ð½" Ð¸Ð»Ð¸ "Ð¼Ð°Ð»Ð¸Ð½/Ð³Ñ€Ð°Ð½Ñ–Ñ‚Ð½Ðµ")
    # Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð² ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¸ Ð½ÐµÑ‚ ÑÐ²Ð½Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ
    city_enumerations = []
    for part in re.split(r'[\n\|]', text.lower()):
        # Ð¸Ñ‰ÐµÐ¼ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ
        if ',' in part:
            city_enumerations += [c.strip() for c in part.split(',') if len(c.strip()) > 2]
        # Ð¸Ñ‰ÐµÐ¼ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· ÑÐ»ÑÑˆ
        if '/' in part:
            city_enumerations += [c.strip() for c in part.split('/') if len(c.strip()) > 2]
    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð³Ð¾Ñ€Ð¾Ð´Ð°
    all_cities = set(city_patterns + city_enumerations)
    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð½Ð°Ð»Ð¸Ñ‡Ð¸ÑŽ Ð² CITY_COORDS (Ð¸Ð»Ð¸ SETTLEMENTS_INDEX)
    found_cities = []
    def _resolve_city_candidate(raw: str):
        cand = raw.strip().lower()
        cand = re.sub(r'["â€œâ€Â«Â»\(\)\[\]]','', cand)
        cand = re.sub(r'\s+',' ', cand)
        # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð¾Ñ‚ Ð´Ð»Ð¸Ð½Ð½Ð¾Ð³Ð¾ Ðº ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾Ð¼Ñƒ (Ð´Ð¾ 3 ÑÐ»Ð¾Ð² Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð»Ñ Ð½Ð°ÑˆÐ¸Ñ… ÑÐ»ÑƒÑ‡Ð°ÐµÐ²)
        words = cand.split()
        if not words:
            return None
        for ln in range(min(3, len(words)), 0, -1):
            sub = ' '.join(words[:ln])
            base = UA_CITY_NORMALIZE.get(sub, sub)
            if base in CITY_COORDS or (SETTLEMENTS_INDEX and base in SETTLEMENTS_INDEX):
                return base
            # ÐœÐ¾Ñ€Ñ„Ð¾Ð»Ð¾Ð³Ð¸Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ Ð²Ð¸Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾/Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°
            sub_mod = re.sub(r'Ñƒ\b','Ð°', sub)
            sub_mod = re.sub(r'ÑŽ\b','Ñ', sub_mod)
            sub_mod = re.sub(r'Ð¾Ñ—\b','Ð°', sub_mod)
            base2 = UA_CITY_NORMALIZE.get(sub_mod, sub_mod)
            if base2 in CITY_COORDS or (SETTLEMENTS_INDEX and base2 in SETTLEMENTS_INDEX):
                return base2
        return UA_CITY_NORMALIZE.get(cand, cand)
    for city in all_cities:
        norm = _resolve_city_candidate(city)
        if not norm:
            continue
        coords = CITY_COORDS.get(norm)
        if not coords and SETTLEMENTS_INDEX:
            coords = SETTLEMENTS_INDEX.get(norm)
        if coords:
            found_cities.append((norm, coords))
    # Ð•ÑÐ»Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ 2 Ð¸ Ð±Ð¾Ð»ÐµÐµ Ð³Ð¾Ñ€Ð¾Ð´Ð° â€” ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¼Ð°Ñ€ÐºÐµÑ€ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾
    if len(found_cities) >= 2:
        threat_type, icon = 'shahed', 'shahed.png'  # Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ auto-classify
        
        # Extract course information for Shahed threats
        course_info = None
        if threat_type == 'shahed':
            course_info = extract_shahed_course_info(original_text)
        
        for idx, (city, (lat, lng)) in enumerate(found_cities, 1):
            track = {
                'id': f"{mid}_mc{idx}", 'place': city.title(), 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': clean_text(original_text)[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'source_match': 'multi_city_auto'
            }
            
            # Add course information if available
            if course_info:
                track.update({
                    'course_source': course_info.get('source_city'),
                    'course_target': course_info.get('target_city'),
                    'course_direction': course_info.get('course_direction'),
                    'course_type': course_info.get('course_type')
                })
            
            multi_city_tracks.append(track)
        if multi_city_tracks:
            return multi_city_tracks
    """Extract coordinates or try simple city geocoding (lightweight)."""
    original_text = text
    # ---------------- Global region (oblast) hint detection for universal settlement binding ----------------
    region_hint_global = None
    try:
        low_rt = original_text.lower()
        for obl_name in OBLAST_CENTERS.keys():
            if obl_name in low_rt:
                region_hint_global = obl_name  # first hit
                break
    except Exception:
        region_hint_global = None
    # Additional: detect section headers like "Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð°:" "ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°:" at line starts to set region hint
    if not region_hint_global:
        for line in original_text.split('\n'):
            l = line.strip().lower()
            if l.endswith(':'):
                base = l[:-1]
                if base in OBLAST_CENTERS:
                    region_hint_global = base
                    break

    def region_enhanced_coords(base_name: str, region_hint_override: str = None):
        """Resolve coordinates for a settlement name by weighted order (remote first):
        1) External geocode (region-qualified, then plain)
        2) Exact local datasets (CITY_COORDS, SETTLEMENTS_INDEX)
        3) Fuzzy approximate local match (Levenshtein-like via difflib)

        Rationale (user requirement): prefer freshest external resolution, fall back to local known list,
        and only then attempt approximate similarity mapping.
        """
        if not base_name:
            return None
        name_norm = UA_CITY_NORMALIZE.get(base_name, base_name).strip().lower()
        # --- 1. Remote geocode first ---
        region_for_query = region_hint_override or region_hint_global
        if region_for_query:
            canon = REGION_GEOCODE_CANON.get(region_for_query)
            if canon:
                region_for_query = canon
        # Region-qualified
        if OPENCAGE_API_KEY and region_for_query:
            try:
                combo = f"{name_norm} {region_for_query}".replace('  ', ' ').strip()
                c = geocode_opencage(combo)
                if c and 43.0 <= c[0] <= 53.8 and 20.0 <= c[1] <= 42.0:
                    return c
            except Exception:
                pass
        # Plain name remote
        if OPENCAGE_API_KEY:
            try:
                c = geocode_opencage(name_norm)
                if c and 43.0 <= c[0] <= 53.8 and 20.0 <= c[1] <= 42.0:
                    return c
            except Exception:
                pass
        # --- 2. Exact local datasets ---
        coord = CITY_COORDS.get(name_norm)
        if not coord and SETTLEMENTS_INDEX:
            coord = SETTLEMENTS_INDEX.get(name_norm)
        # Explicit settlement fallback (manual corrections for mis-geocoded small places)
        if not coord:
            coord = SETTLEMENT_FALLBACK.get(name_norm)
        if coord:
            return coord
        # --- 3. Fuzzy approximate search (only if not found) ---
        try:
            if SETTLEMENTS_INDEX:
                import difflib
                # Choose candidate list limited for performance
                names = list(SETTLEMENTS_INDEX.keys())
                # High cutoff to avoid bad matches
                best = difflib.get_close_matches(name_norm, names, n=1, cutoff=0.86)
                if best:
                    b = best[0]
                    return SETTLEMENTS_INDEX.get(b)
        except Exception:
            pass
        return None
    # ---- Fundraising / donation solicitation handling ----
    # Previous behavior: fully suppressed entire message if donation links found (blocked napramok multi-line threat posts with footer links)
    # New behavior: If donation lines present BUT the message also contains threat indicators, strip only the donation lines and continue parsing.
    low_full = original_text.lower()
    DONATION_KEYS = [
        'Ð¼Ð¾Ð½Ð¾Ð±Ð°Ð½Ðº','monobank','mono.bank','privat24','Ð¿Ñ€Ð¸Ð²Ð°Ñ‚24','Ñ€ÐµÐºÐ²Ñ–Ð·Ð¸Ñ‚','Ñ€ÐµÐºÐ²Ð¸Ð·Ð¸Ñ‚','Ð´Ð¾Ð½Ð°Ñ‚','donat','iban','paypal','patreon','send.monobank.ua','jar/','Ð±Ð°Ð½ÐºÐ°: http','ÐºÐ°Ñ€Ñ‚Ð°(','ÐºÐ°Ñ€Ñ‚Ð°(monobank)','ÐºÐ°Ñ€Ñ‚Ð°(privat24)','Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼Ð°Ñ‚Ð¸ ÐºÐ°Ð½Ð°Ð»'
    ]
    donation_present = any(k in low_full for k in DONATION_KEYS) or re.search(r'\b\d{16}\b', low_full)
    # Pure subscription / invite promo suppression (no threats, mostly t.me invite links + short call to action)
    if not any(w in low_full for w in ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','shahed','Ñ€Ð°ÐºÐµÑ‚Ð°','ÐºÐ°Ð±','Ð°Ñ€Ñ‚Ð¸Ð»','Ð³Ñ€Ð°Ð´Ð°','ÑÐ¼ÐµÑ€Ñ‡','ÑƒÑ€Ð°Ð³Ð°Ð½','mlrs','iskander','s-300','s300','border','trivoga','Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°','Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°']) and \
       low_full.count('t.me/') >= 1 and len(re.sub(r'\s+',' ', low_full)) < 260 and \
       len([ln for ln in low_full.splitlines() if ln.strip()]) <= 6:
        if all(tok not in low_full for tok in ['Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°','ÑƒÐºÑ€Ð¸Ñ‚Ñ‚','alert','launch','start','Ð²Ð¸Ð»ÑŒÐ¾Ñ‚','Ð²Ð¸Ñ…Ñ–Ð´','Ð¿ÑƒÑÐºÐ¸','air','strike']):
            return None
    if donation_present:
        # Threat keyword heuristic (lightweight; don't rely on later THREAT_KEYS definition yet)
        threat_tokens = ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','shahed','geran','Ñ€Ð°ÐºÐµÑ‚Ð°','Ñ€Ð°ÐºÐµÑ‚Ð¸','missile','iskander','s-300','s300','ÐºÐ°Ð±','Ð°Ñ€Ñ‚Ð¸Ð»','Ð³Ñ€Ð°Ð´Ð°','ÑÐ¼ÐµÑ€Ñ‡','ÑƒÑ€Ð°Ð³Ð°Ð½','mlrs']
        has_threat_word = any(tok in low_full for tok in threat_tokens)
        if has_threat_word:
            # ÐÐ• ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ Ð´Ð¾Ð½Ð°Ñ‚Ð°Ð¼Ð¸ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÑƒÐ³Ñ€Ð¾Ð·Ñ‹ - Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³
            log.debug(f"mid={mid} donation_present but has_threats - continuing without stripping")
            # text Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹
        else:
            return [{
                'id': str(mid), 'place': None, 'lat': None, 'lng': None,
                'threat_type': None, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                'list_only': True, 'suppress': True, 'suppress_reason': 'donation_only'
            }]
    # --- Universal link stripping (any clickable invite / http) ---
    def _strip_links(s: str) -> str:
        if not s:
            return s
        # markdown links [text](url)
        # handle bold inside brackets [**Text**](url) by stripping ** first
        s = re.sub(r'\*\*','', s)
        s = re.sub(r'\[([^\]]{0,80})\]\((https?://|t\.me/)[^\)]+\)', lambda m: (m.group(1) or '').strip(), s, flags=re.IGNORECASE)
        # bare urls
        s = re.sub(r'(https?://\S+|t\.me/\S+)', '', s, flags=re.IGNORECASE)
        # collapse whitespace and drop empty lines
        cleaned = []
        for ln in s.splitlines():
            ln2 = ln.strip()
            if not ln2:
                continue
            # pure decoration (arrows, bullets) or subscribe call to action lines
            if re.fullmatch(r'[>âž¡â†’\-\sÂ·â€¢]*', ln2):
                continue
            # remove any line that is just a subscribe CTA or starts with arrow+subscribe
            if re.search(r'(Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑŒ|Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ|Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ|Ð¿Ð¾Ð´Ð¿Ð¸ÑÐ°Ñ‚ÑŒÑÑ|Ð¿Ð¾Ð´Ð¿Ð¸ÑˆÐ¸ÑÑŒ|subscribe)', ln2, re.IGNORECASE):
                continue
            # remove arrow+subscribe pattern specifically
            if re.search(r'[âž¡â†’>]\s*Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ', ln2, re.IGNORECASE):
                continue
            cleaned.append(ln2)
        return '\n'.join(cleaned)
    new_text = _strip_links(text)
    if new_text != text:
        text = new_text
    new_orig = _strip_links(original_text)
    if new_orig != original_text:
        original_text = new_orig
    # --- Explicit launch site detection (multi-line). Create one marker per detected launch location.
    low_work = text.lower()
    if ('Ð¿ÑƒÑÐº' in low_work or 'Ð¿ÑƒÑÐºÐ¸' in low_work or '+ Ð¿ÑƒÑÐºÐ¸' in low_work):
        # find quoted or dash-separated site tokens: Â«NameÂ», "Name", or after 'Ð· ' preposition
        sites_found = set()
        # Quoted tokens
        for m in re.findall(r'Â«([^Â»]{2,40})Â»', text):
            sites_found.add(m.strip().lower())
        for m in re.findall(r'"([^"\n]{2,40})"', text):
            sites_found.add(m.strip().lower())
        # Phrases after 'Ð· ' (from) up to comma
        for m in re.findall(r'Ð·\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{2,40})', low_work):
            sites_found.add(m.strip().lower())
        # tokens after 'Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ñƒ' or 'Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°' inside quotes
        for m in re.findall(r'Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼[Ð°Ñƒ]\s+Â«([^Â»]{2,40})Â»', low_work):
            sites_found.add(m.strip().lower())
        for m in re.findall(r'Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼[Ð°Ñƒ]\s+Â«([^Â»]{2,40})Â»', low_work):
            sites_found.add(m.strip().lower())
        tracks = []
        threat_type = 'pusk'
        icon = 'pusk.png'
        idx = 0
        for raw_site in sites_found:
            norm_key = raw_site.replace(' â€” ','-').replace(' â€“ ','-').replace('â€”','-').replace('â€“','-')
            norm_key = norm_key.replace('  ',' ').strip()
            base_variants = [norm_key, norm_key.replace('Ð¿Ð¾Ð»Ñ–Ð³Ð¾Ð½ ','').replace('Ð¿Ð¾Ð»Ð¸Ð³Ð¾Ð½ ','')]
            coord = None
            chosen_name = raw_site
            for bv in base_variants:
                if bv in LAUNCH_SITES:
                    coord = LAUNCH_SITES[bv]
                    chosen_name = bv
                    break
            if not coord:
                continue
            idx += 1
            lat,lng = coord
            tracks.append({
                'id': f"{mid}_l{idx}", 'place': chosen_name.title(), 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'source_match': 'launch_site'
            })
        if tracks:
            return tracks
    # ---- Daily / periodic situation summary ("ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ñ–Ñ ÑÑ‚Ð°Ð½Ð¾Ð¼ Ð½Ð° HH:MM" + sectional bullets) ----
    # User request: do NOT create map markers for such aggregated status reports.
    # Heuristics: phrase "ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ñ–Ñ ÑÑ‚Ð°Ð½Ð¾Ð¼" (uk) or "ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ñ Ð½Ð°" (ru), OR presence of 2+ bullet headers like "â€¢ Ð°Ð²Ñ–Ð°Ñ†Ñ–Ñ", "â€¢ Ð±Ð¿Ð»Ð°", "â€¢ Ñ„Ð»Ð¾Ñ‚" in same message.
    bullet_headers = 0
    for hdr in ['â€¢ Ð°Ð²Ñ–Ð°Ñ†Ñ–Ñ', 'â€¢ Ð°Ð²Ð¸Ð°', 'â€¢ Ð±Ð¿Ð»Ð°', 'â€¢ Ð´Ñ€Ð¾', 'â€¢ Ñ„Ð»Ð¾Ñ‚', 'â€¢ ÐºÐ¾Ñ€Ð°Ð±', 'â€¢ Ñ€Ð°ÐºÐµÑ‚Ð¸', 'â€¢ Ñ€Ð°ÐºÐµÑ‚Ñ‹']:
        if hdr in low_full:
            bullet_headers += 1
    if re.search(r'ÑÐ¸Ñ‚ÑƒÐ°Ñ†[Ñ–Ñ][Ñi]\s+ÑÑ‚Ð°Ð½Ð¾Ð¼', low_full) or re.search(r'ÑÐ¸Ñ‚ÑƒÐ°Ñ†[Ð¸Ñ]\s+Ð½Ð°\s+\d{1,2}:\d{2}', low_full) or bullet_headers >= 2:
        # User clarified: completely skip (no site display at all)
        return [{
            'id': str(mid), 'place': None, 'lat': None, 'lng': None,
            'threat_type': None, 'text': original_text[:800], 'date': date_str, 'channel': channel,
            'list_only': True, 'summary': True, 'suppress': True
        }]
    # ---- Imprecise directional-only messages (no exact city location) suppression ----
    # User request: messages that only state relative / directional movement without a clear city position
    # Examples: "Ð³Ñ€ÑƒÐ¿Ð¸ ... Ñ€ÑƒÑ…Ð°ÑŽÑ‚ÑŒÑÑ Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ X Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ Y"; "... ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹ (Ð¼Ñ–ÑÑ‚Ð¾)"; region-only with direction
    # Allow cases with explicit target form "ÐºÑƒÑ€Ñ Ð½Ð° <city>" (precise intent) or patterns we already map like 'Ð¿Ð¾Ð²Ð· <city>' or multi-city slash/comma lists.
    def _has_threat_local(txt: str):
        l = txt.lower()
        return any(k in l for k in ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','shahed','geran','Ñ€Ð°ÐºÐµÑ‚Ð°','Ñ€Ð°ÐºÐµÑ‚Ð¸','missile'])
    lower_all = original_text.lower()
    if _has_threat_local(lower_all):
        directional_course = 'ÐºÑƒÑ€Ñ' in lower_all and any(w in lower_all for w in ['Ð¿Ñ–Ð²Ð½Ñ–Ñ‡','Ð¿Ñ–Ð²Ð´ÐµÐ½','ÑÑ…Ñ–Ð´','Ð·Ð°Ñ…Ñ–Ð´']) and not re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,}', lower_all)
        relative_dir_tokens = any(tok in lower_all for tok in ['Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ñ–ÑˆÐµ','ÑÑ…Ñ–Ð´Ð½Ñ–ÑˆÐµ','Ð·Ð°Ñ…Ñ–Ð´Ð½Ñ–ÑˆÐµ'])
        # Multi-city list heuristic (comma or slash separated multiple city tokens at start)
        multi_city_pattern = r"^[^\n]{0,120}?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'`â€™Ê¼\-]{3,}\s*,\s*){1,}[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'`â€™Ê¼\-]{3,}"
        multi_city_enumeration = bool(re.match(multi_city_pattern, lower_all)) or ('/' in lower_all)
        has_pass_near = 'Ð¿Ð¾Ð²Ð· ' in lower_all
        if (directional_course or relative_dir_tokens) and not has_pass_near and not multi_city_enumeration:
            return [{
                'id': str(mid), 'place': None, 'lat': None, 'lng': None,
                'threat_type': None, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                'list_only': True, 'suppress': True, 'suppress_reason': 'imprecise_direction_only'
            }]
    # ÐÐµ ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ "ÐŸÐ¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ñ‚Ñ€Ð¸Ð²Ð¾Ð³Ð°" Ñ‚ÐµÐ¿ÐµÑ€ÑŒ: Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð² ÑÐ¿Ð¸ÑÐºÐµ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹.
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚ÐµÐºÑÑ‚ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ Ð´Ð»Ñ event list.
    # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ markdown * _ ` Ð¸ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ ÑÐ¼Ð¾Ð´Ð·Ð¸-Ð¸ÐºÐ¾Ð½ÐºÐ¸ Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ ÑÑ‚Ñ€Ð¾Ðº
    text = re.sub(r'[\*`_]+', '', text)
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²ÐµÐ´ÑƒÑ‰Ð¸Ðµ ÑÐ¼Ð¾Ð´Ð·Ð¸/Ð¸ÐºÐ¾Ð½ÐºÐ¸ Ð¿ÐµÑ€ÐµÐ´ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸
    text = re.sub(r'^[\W_]+', '', text)
    # ÐžÐ±Ñ‰Ð¸Ð¹ Ð½Ð°Ð±Ð¾Ñ€ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð² ÑƒÐ³Ñ€Ð¾Ð·
    THREAT_KEYS = ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','shahed','geran','Ñ€Ð°ÐºÐµÑ‚Ð°','Ñ€Ð°ÐºÐµÑ‚Ð¸','missile','iskander','s-300','s300','ÐºÐ°Ð±','Ð°Ñ€Ñ‚Ð¸Ð»','Ð³Ñ€Ð°Ð´Ð°','ÑÐ¼ÐµÑ€Ñ‡','ÑƒÑ€Ð°Ð³Ð°Ð½','mlrs','avia','Ð°Ð²Ñ–Ð°','Ð°Ð²Ð¸Ð°','Ð±Ð¾Ð¼Ð±Ð°','Ð²Ð¸ÑÐ¾ÐºÐ¾ÑˆÐ²Ð¸Ð´ÐºÑ–ÑÐ½']
    def has_threat(txt: str):
        l = txt.lower()
        return any(k in l for k in THREAT_KEYS)
    
    # PRIORITY: Structured messages with regional headers (e.g., "ÐžÐ±Ð»Ð°ÑÑ‚ÑŒ:\n city details")
    if not _disable_multiline and has_threat(original_text):
        import re as _struct_re
        # Look for pattern: "RegionName:\n threats with cities"
        region_header_pattern = r'^([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘]+Ñ‰Ð¸Ð½Ð°):\s*$'
        text_lines = original_text.split('\n')
        
        structured_sections = []
        current_region = None
        current_threats = []
        
        for line in text_lines:
            line = line.strip()
            if not line or 'Ð¿Ñ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ' in line.lower():
                continue
                
            # Check if line is a region header
            region_match = _struct_re.match(region_header_pattern, line)
            if region_match:
                # Save previous section
                if current_region and current_threats:
                    structured_sections.append((current_region, current_threats))
                # Start new section
                current_region = region_match.group(1)
                current_threats = []
            elif current_region and ('ÑˆÐ°Ñ…ÐµÐ´' in line.lower() or 'Ð±Ð¿Ð»Ð°' in line.lower()):
                # This is a threat line under current region
                current_threats.append(line)
        
        # Don't forget last section
        if current_region and current_threats:
            structured_sections.append((current_region, current_threats))
        
        # Process structured sections if we found any
        if len(structured_sections) >= 2:
            add_debug_log(f"STRUCTURED REGIONS: Found {len(structured_sections)} regions with threats", "structured_regions")
            
            all_structured_tracks = []
            for region_name, threat_lines in structured_sections:
                add_debug_log(f"Processing region {region_name} with {len(threat_lines)} threats", "structured_region_detail")
                
                for threat_line in threat_lines:
                    # Process each threat line with region context
                    region_context_text = f"{region_name}:\n{threat_line}"
                    line_tracks = process_message(region_context_text, f"{mid}_{region_name}_{len(all_structured_tracks)}", 
                                                date_str, channel, _disable_multiline=True)
                    if line_tracks:
                        all_structured_tracks.extend(line_tracks)
                        add_debug_log(f"Region {region_name} threat '{threat_line[:50]}...' produced {len(line_tracks)} tracks", "structured_threat_result")
            
            if all_structured_tracks:
                add_debug_log(f"Structured processing complete: {len(all_structured_tracks)} total tracks", "structured_complete")
                return all_structured_tracks
    
    # NEW: Handle UAV messages with "Ñ‡ÐµÑ€ÐµÐ· [city]" and "Ð¿Ð¾Ð²Ð· [city]" patterns - BEFORE trajectory_phrase  
    try:
        lorig = text.lower()
        if 'Ð±Ð¿Ð»Ð°' in lorig and ('Ñ‡ÐµÑ€ÐµÐ·' in lorig or 'Ð¿Ð¾Ð²Ð·' in lorig):
            threats = []
            
            # Extract cities from "Ñ‡ÐµÑ€ÐµÐ· [city1], [city2]" pattern
            import re as _re_route
            route_pattern = r'Ñ‡ÐµÑ€ÐµÐ·\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\s\',\-]+?)(?:\s*\.\s+|$)'
            route_matches = _re_route.findall(route_pattern, text, re.IGNORECASE)
            
            for route_match in route_matches:
                # Split by comma to get individual cities
                cities_raw = [c.strip() for c in route_match.split(',') if c.strip()]
                
                for city_raw in cities_raw:
                    city_clean = city_raw.strip().strip('.,')
                    city_norm = clean_text(city_clean).lower()
                    
                    # Apply normalization rules
                    if city_norm in UA_CITY_NORMALIZE:
                        city_norm = UA_CITY_NORMALIZE[city_norm]
                    
                    # Try to get coordinates
                    coords = region_enhanced_coords(city_norm)
                    if not coords:
                        coords = ensure_city_coords(city_norm)
                    
                    if coords:
                        # Handle different coordinate formats
                        if isinstance(coords, tuple) and len(coords) >= 2:
                            lat, lng = coords[0], coords[1]
                        else:
                            continue
                        
                        threat_type, icon = classify(text)
                        
                        # Extract count from text context (look for patterns like "15Ñ… Ð‘Ð¿Ð›Ð Ñ‡ÐµÑ€ÐµÐ·")
                        count = 1
                        count_match = _re_route.search(rf'(\d+)[xÑ…]?\s*Ð±Ð¿Ð»Ð°.*?Ñ‡ÐµÑ€ÐµÐ·.*?{re.escape(city_clean)}', text, re.IGNORECASE)
                        if count_match:
                            count = int(count_match.group(1))
                        
                        threats.append({
                            'id': f"{mid}_route_{len(threats)}",
                            'place': city_clean.title(),
                            'lat': lat,
                            'lng': lng,
                            'threat_type': threat_type,
                            'text': f"Ð§ÐµÑ€ÐµÐ· {city_clean.title()} (Ð· Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ Ð¿Ñ€Ð¾ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚)",
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': icon,
                            'source_match': f'route_via_{count}x',
                            'count': count
                        })
                        
                        add_debug_log(f"Route via: {city_clean} ({count}x) -> {coords}", "route_via")
            
            # Extract cities from "Ð¿Ð¾Ð²Ð· [city]" pattern
            past_pattern = r'Ð¿Ð¾Ð²Ð·\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\s\',\-]+?)(?:\s*\.\s*|$)'
            past_matches = _re_route.findall(past_pattern, text, re.IGNORECASE)
            
            for past_match in past_matches:
                city_clean = past_match.strip().strip('.,')
                city_norm = clean_text(city_clean).lower()
                
                # Apply normalization rules
                if city_norm in UA_CITY_NORMALIZE:
                    city_norm = UA_CITY_NORMALIZE[city_norm]
                
                # Try to get coordinates
                coords = region_enhanced_coords(city_norm)
                if not coords:
                    coords = ensure_city_coords_with_message_context(city_norm, text)
                
                # Fallback: try accusative case normalization (e.g., "Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–ÑŽ" -> "Ð¾Ð»ÐµÐºÑÐ°Ð½Ð´Ñ€Ñ–Ñ")
                if not coords and city_norm.endswith('Ñ–ÑŽ'):
                    accusative_fallback = city_norm[:-2] + 'Ñ–Ñ'
                    coords = region_enhanced_coords(accusative_fallback)
                    if not coords:
                        coords = ensure_city_coords_with_message_context(accusative_fallback, text)
                    if coords:
                        city_norm = accusative_fallback
                        city_clean = accusative_fallback.title()  # Use normalized name for display
                
                if coords:
                    # Handle different coordinate formats
                        if isinstance(coords, tuple) and len(coords) >= 2:
                            lat, lng = coords[0], coords[1]
                        else:
                            continue
                        
                        threat_type, icon = classify(text)
                        
                        # Extract count from text context (look for patterns like "4Ñ… Ð‘Ð¿Ð›Ð Ð¿Ð¾Ð²Ð·")
                        count = 1
                        count_match = _re_route.search(rf'(\d+)[xÑ…]?\s*Ð±Ð¿Ð»Ð°.*?Ð¿Ð¾Ð²Ð·.*?{re.escape(city_clean)}', text, re.IGNORECASE)
                        if count_match:
                            count = int(count_match.group(1))
                    
                        threats.append({
                            'id': f"{mid}_past_{len(threats)}",
                            'place': city_clean.title(),
                            'lat': lat,
                            'lng': lng,
                            'threat_type': threat_type,
                            'text': f"ÐŸÐ¾Ð²Ð· {city_clean.title()} (Ð· Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ Ð¿Ñ€Ð¾ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚)",
                            'date': date_str,
                            'channel': channel,
                            'marker_icon': icon,
                            'source_match': f'route_past_{count}x',
                            'count': count
                        })
                        
                        add_debug_log(f"Route past: {city_clean} ({count}x) -> {coords}", "route_past")
            
            if threats:
                return threats
            else:
                pass
                
    except Exception:
        pass
    
    # --- Trajectory phrase pattern: "Ð· Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸ Ñ‡ÐµÑ€ÐµÐ· Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ñƒ Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð¸" ---
    # We map region stems to canonical OBLAST_CENTERS keys (simplistic stem matching).
    lower_full = text.lower()
    if has_threat(lower_full) and ' Ñ‡ÐµÑ€ÐµÐ· ' in lower_full and (' Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ' in lower_full or ' Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ' in lower_full or ' Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ' in lower_full):
        # Extract sequence tokens after prepositions Ð·/Ñ–Ð·/Ð²Ñ–Ð´ -> start, Ñ‡ÐµÑ€ÐµÐ· -> middle(s), Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ -> target
        # Very heuristic; splits by key words.
        try:
            norm = re.sub(r'\s+', ' ', lower_full)
            norm = norm.replace('Ñ–Ð· ', 'Ð· ').replace('Ð²Ñ–Ð´ ', 'Ð· ')
            if ' Ñ‡ÐµÑ€ÐµÐ· ' in norm:
                front, after = norm.split(' Ñ‡ÐµÑ€ÐµÐ· ', 1)
                start_token = front.split(' Ð· ')[-1].strip()
                target_part = None; mid_part = ''
                for marker in [' Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ', ' Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ', ' Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ']:
                    if marker in after:
                        mid_part, target_part = after.split(marker, 1)
                        break
                if target_part:
                    mid_token = mid_part.strip().split('.')[0]
                    target_token = target_part.strip().split('.')[0]
                    def region_center(token: str):
                        token = token.strip()
                        for k,(lat,lng) in OBLAST_CENTERS.items():
                            if token.startswith(k.split()[0][:6]) or token in k:
                                return (k,(lat,lng))
                        return None
                    seq = []
                    for tk in [start_token, mid_token, target_token]:
                        rc = region_center(tk)
                        if rc and (not seq or seq[-1][0] != rc[0]):
                            seq.append(rc)
                    if len(seq) >= 2:
                        threat_type, icon = classify(text)
                        tracks = []
                        for idx,(name,(lat,lng)) in enumerate(seq,1):
                            base = name.split()[0].title()
                            tracks.append({
                                'id': f"{mid}_t{idx}", 'place': base, 'lat': lat, 'lng': lng,
                                'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                                'marker_icon': icon, 'source_match': 'trajectory_phrase'
                            })
                        return tracks
        except Exception:
            pass
    # direct coordinates pattern
    # --- Direction with parenthetical specific settlement e.g. "Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾ Ñ€Ð°Ð¹Ð¾Ð½Ñƒ Ð¾Ð´ÐµÑ‰Ð¸Ð½Ð¸ (Ð·Ð°Ñ‚Ð¾ÐºÐ°)" ---
    if has_threat(lower_full) and 'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ' in lower_full and '(' in lower_full and ')' in lower_full:
        # capture last parenthetical token (short) that is a known settlement
        try:
            paren_tokens = re.findall(r'\(([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-\s]{3,})\)', lower_full)
            if paren_tokens:
                candidate = paren_tokens[-1].strip().lower()
                # trim descriptors like 'ÑÐ¼Ñ‚ ' , 'Ñ.' etc
                candidate = re.sub(r'^(ÑÐ¼Ñ‚|Ñ\.|Ð¼\.|Ð¼Ñ–ÑÑ‚Ð¾|ÑÐµÐ»Ð¸Ñ‰Ðµ)\s+','', candidate)
                norm = UA_CITY_NORMALIZE.get(candidate, candidate)
                coords = CITY_COORDS.get(norm)
                if not coords and SETTLEMENTS_INDEX:
                    coords = SETTLEMENTS_INDEX.get(norm)
                if not coords:
                    coords = SETTLEMENT_FALLBACK.get(norm)
                log.debug(f"parenthetical_dir detect mid={mid} candidate={candidate} norm={norm} found={bool(coords)}")
                if coords:
                    lat,lng = coords
                    threat_type, icon = classify(text)
                    return [{
                        'id': f"{mid}_dirp", 'place': norm.title(), 'lat': lat, 'lng': lng,
                        'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'direction_parenthetical'
                    }]
        except Exception:
            pass
    # --- Region-level shelling threat (e.g. "Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð». Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ Ð¿Ñ€Ð¸ÐºÐ¾Ñ€Ð´Ð¾Ð½Ð½Ð¸Ñ… Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ð¹") ---
    try:
        if re.search(r'(Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ|ÑƒÐ³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»Ð°)', lower_full):
            # attempt to match any oblast token present
            region_hit = None
            for reg_key in OBLAST_CENTERS.keys():
                if reg_key in lower_full:
                    region_hit = reg_key
                    log.debug(f"region_shelling candidate mid={mid} match={reg_key}")
                    break
            if region_hit:
                # Only emit if we haven't already returned a more specific structure earlier (heuristic: continue)
                lat, lng = OBLAST_CENTERS[region_hit]
                threat_type, icon = classify(text)
                # Enforce obstril icon for shelling phrasing even if classify changed in future
                if re.search(r'(Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»Ñƒ|ÑƒÐ³Ñ€Ð¾Ð·Ð° Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»Ð°|Ð¾Ð±ÑÑ‚Ñ€Ñ–Ð»|Ð¾Ð±ÑÑ‚Ñ€ÐµÐ»)', lower_full):
                    threat_type = 'artillery'; icon = 'obstril.png'
                border_shell = bool(re.search(r'Ð¿Ñ€Ð¸ÐºÐ¾Ñ€Ð´Ð¾Ð½|Ð¿Ñ€Ð¸Ð³Ñ€Ð°Ð½Ð¸Ñ‡', lower_full))
                place_label = region_hit
                if border_shell:
                    place_label += ' (Ð¿Ñ€Ð¸ÐºÐ¾Ñ€Ð´Ð¾Ð½Ð½Ñ)'
                log.debug(f"region_shelling emit mid={mid} region={region_hit} border={border_shell}")
                return [{
                    'id': f"{mid}_region_shell", 'place': place_label, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'region_shelling', 'border_shelling': border_shell
                }]
    except Exception:
        pass
    # Special handling for KAB threats with regional mentions (e.g., "Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° ÐšÐÐ‘ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ñ„Ñ€Ð¾Ð½Ñ‚Ð¾Ð²Ð¸Ñ… Ð³Ñ€Ð¾Ð¼Ð°Ð´ Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð¸")
    kab_region_match = re.search(r'(ÐºÐ°Ð±|Ð°Ð²Ñ–Ð°Ð±Ð¾Ð¼Ð±|Ð°Ð²Ð¸Ð°Ð±Ð¾Ð¼Ð±|Ð°Ð²Ñ–Ð°Ñ†Ñ–Ð¹Ð½Ð¸Ñ….*Ð±Ð¾Ð¼Ð±|ÐºÐµÑ€Ð¾Ð²Ð°Ð½.*Ð±Ð¾Ð¼Ð±)[^\.]*?(ÑÑƒÐ¼Ñ‰Ð¸Ð½[Ð¸Ñ–Ð°]|Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½[Ð¸Ñ–Ð°]|Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½[Ð¸Ñ–Ð°]|Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½[Ð¸Ñ–Ð°])', text.lower())
    if kab_region_match:
        region_mention = kab_region_match.group(2)
        # Convert genitive/dative to nominative
        if 'ÑÑƒÐ¼Ñ‰Ð¸Ð½' in region_mention:
            region_key = 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°'
        elif 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½' in region_mention:
            region_key = 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°'
        elif 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½' in region_mention:
            region_key = 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°'
        elif 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½' in region_mention:
            region_key = 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ð°'
        else:
            region_key = None
            
        if region_key and region_key in OBLAST_CENTERS:
            lat, lng = OBLAST_CENTERS[region_key]
            # For KAB threats, offset coordinates slightly from city center to avoid implying direct city impact
            if region_key == 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°':
                lat += 0.1  # Move north of Sumy city
                lng -= 0.1  # Move west of Sumy city
            elif region_key == 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°':
                lat += 0.1  # Move north of Kharkiv city
                lng -= 0.1  # Move west of Kharkiv city
            add_debug_log(f"Creating KAB regional threat marker for {region_key}: lat={lat}, lng={lng}", "kab_regional")
            return [{
                'id': f"{mid}_kab_regional", 'place': region_key.title(), 'lat': lat, 'lng': lng,
                'threat_type': 'raketa', 'text': original_text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': 'raketa.png', 'source_match': 'kab_regional_threat'
            }]
    
    # SPECIAL: Handle multi-regional UAV messages (like the user's example)
    def handle_multi_regional_uav():
        """Handle messages with multiple regional UAV threats listed separately"""
        threats = []
        text_lines = text.split('\n')
        
        # Check if this looks like a multi-regional UAV message
        region_count = 0
        uav_count = 0
        for line in text_lines:
            line_lower = line.lower().strip()
            if not line_lower:
                continue
                
            # Count regions mentioned
            if any(region in line_lower for region in ['Ñ‰Ð¸Ð½Ð°:', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ:', 'ÐºÑ€Ð°Ð¹:']):
                region_count += 1
            
            # Count UAV mentions
            if 'Ð±Ð¿Ð»Ð°' in line_lower and ('ÐºÑƒÑ€Ñ' in line_lower or 'Ð½Ð° ' in line_lower):
                uav_count += 1
        
        # If we have multiple regions and multiple UAV mentions, process each line
        if region_count >= 2 and uav_count >= 3:
            add_debug_log(f"MULTI-REGIONAL UAV MESSAGE: {region_count} regions, {uav_count} UAVs", "multi_regional")
            
            for line in text_lines:
                line_stripped = line.strip()
                if not line_stripped or ':' in line_stripped[:20]:  # Skip region headers
                    continue
                
                line_lower = line_stripped.lower()
                
                # Look for UAV course patterns
                if 'Ð±Ð¿Ð»Ð°' in line_lower and ('ÐºÑƒÑ€Ñ' in line_lower or ' Ð½Ð° ' in line_lower):
                    # Extract city name from patterns like "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿" or "2Ñ… Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð’ÐµÐ»Ð¸ÐºÑƒ Ð”Ð¸Ð¼ÐµÑ€ÐºÑƒ"
                    patterns = [
                        r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|])',
                        r'Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|])',
                        r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|])'
                    ]
                    
                    for pattern in patterns:
                        matches = re.finditer(pattern, line_stripped, re.IGNORECASE)
                        for match in matches:
                            if len(match.groups()) == 2:
                                count_str, city_raw = match.groups()
                            else:
                                count_str = None
                                city_raw = match.group(1)
                            
                            if not city_raw:
                                continue
                                
                            # Clean and normalize city name
                            city_clean = city_raw.strip()
                            city_norm = clean_text(city_clean).lower()
                            
                            # Apply normalization rules
                            if city_norm in UA_CITY_NORMALIZE:
                                city_norm = UA_CITY_NORMALIZE[city_norm]
                            
                            # Try to get coordinates
                            coords = region_enhanced_coords(city_norm)
                            if not coords:
                                coords = ensure_city_coords(city_norm)
                            
                            if coords:
                                lat, lng = coords
                                threat_type, icon = classify(text)
                                
                                # Extract count if present
                                uav_count_num = 1
                                if count_str and count_str.isdigit():
                                    uav_count_num = int(count_str)
                                
                                threat_id = f"{mid}_multi_{len(threats)}"
                                threats.append({
                                    'id': threat_id,
                                    'place': city_clean.title(),
                                    'lat': lat,
                                    'lng': lng,
                                    'threat_type': threat_type,
                                    'text': f"{line_stripped} (Ð· Ð±Ð°Ð³Ð°Ñ‚Ð¾Ñ€ÐµÐ³Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ)",
                                    'date': date_str,
                                    'channel': channel,
                                    'marker_icon': icon,
                                    'source_match': f'multi_regional_uav_{uav_count_num}x',
                                    'count': uav_count_num
                                })
                                
                                add_debug_log(f"Multi-regional UAV: {city_clean} ({uav_count_num}x) -> {coords}", "multi_regional")
                            else:
                                add_debug_log(f"Multi-regional UAV: No coords for {city_clean}", "multi_regional")
        
        return threats

    # SPECIAL: Handle single UAV course mentions in regular messages
    def handle_single_uav_courses():
        """Handle UAV course mentions like '4Ñ… Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð”Ð¾Ð±Ñ€Ð¾Ñ‚Ð²Ñ–Ñ€' in regular alert messages"""
        threats = []
        
        # Look for UAV course patterns in the entire message
        patterns = [
            r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|\(])',
            r'Ð±Ð¿Ð»Ð°\s+ÐºÑƒÑ€ÑÐ¾Ð¼?\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|\(])',
            r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+Ð½Ð°\s+([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)(?:\s*$|\s*[,\.\!\?\|\(])'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match.groups()) == 2:
                    count_str, city_raw = match.groups()
                else:
                    count_str = None
                    city_raw = match.group(1)
                
                if not city_raw:
                    continue
                    
                # Clean and normalize city name
                city_clean = city_raw.strip()
                city_norm = clean_text(city_clean).lower()
                
                # Apply normalization rules
                if city_norm in UA_CITY_NORMALIZE:
                    city_norm = UA_CITY_NORMALIZE[city_norm]
                
                # Try to get coordinates
                coords = region_enhanced_coords(city_norm)
                if not coords:
                    coords = ensure_city_coords(city_norm)
                
                if coords:
                    lat, lng = coords[:2]
                    threat_type, icon = classify(text)
                    
                    # Extract count if present
                    uav_count_num = 1
                    if count_str and count_str.isdigit():
                        uav_count_num = int(count_str)
                    
                    threat_id = f"{mid}_uav_course_{len(threats)}"
                    threats.append({
                        'id': threat_id,
                        'place': city_clean.title(),
                        'lat': lat,
                        'lng': lng,
                        'threat_type': threat_type,
                        'text': f"Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° {city_clean} ({uav_count_num}x)",
                        'date': date_str,
                        'channel': channel,
                        'marker_icon': icon,
                        'source_match': f'single_uav_course_{uav_count_num}x',
                        'count': uav_count_num
                    })
                    
                    add_debug_log(f"Single UAV course: {city_clean} ({uav_count_num}x) -> {coords}", "single_uav")
                else:
                    add_debug_log(f"Single UAV course: No coords for {city_clean}", "single_uav")
        
        # ALSO: Extract cities from emoji structure in the same text 
        # Pattern for "| ðŸ›¸ Ð“Ð¾Ñ€Ð¾Ð´ (ÐžÐ±Ð»Ð°ÑÑ‚ÑŒ)"
        emoji_pattern = r'\|\s*ðŸ›¸\s*([Ð-Ð¯Ð†Ð‡Ð„ÐÐ°-ÑÑ–Ñ—Ñ”Ñ‘\'\-\s]+?)\s*\([^)]*Ð¾Ð±Ð»[^)]*\)'
        emoji_matches = re.finditer(emoji_pattern, text, re.IGNORECASE)
        
        for match in emoji_matches:
            city_raw = match.group(1).strip()
            if not city_raw or len(city_raw) < 2:
                continue
                
            city_norm = clean_text(city_raw).lower()
            if city_norm in UA_CITY_NORMALIZE:
                city_norm = UA_CITY_NORMALIZE[city_norm]
            
            coords = region_enhanced_coords(city_norm)
            if not coords:
                coords = ensure_city_coords(city_norm)
            
            if coords:
                lat, lng = coords[:2]
                threat_type, icon = classify(text)
                
                threat_id = f"{mid}_emoji_struct_{len(threats)}"
                threats.append({
                    'id': threat_id,
                    'place': city_raw.title(),
                    'lat': lat,
                    'lng': lng,
                    'threat_type': threat_type,
                    'text': f"Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° Ð² {city_raw}",
                    'date': date_str,
                    'channel': channel,
                    'marker_icon': icon,
                    'source_match': 'emoji_structure',
                    'count': 1
                })
                
                add_debug_log(f"Emoji structure: {city_raw} -> {coords}", "emoji_struct")
            else:
                add_debug_log(f"Emoji structure: No coords for {city_raw}", "emoji_struct")
        
        return threats

    # Check for single UAV course mentions first (before multi-regional check)
    single_uav_threats = handle_single_uav_courses()
    if single_uav_threats:
        add_debug_log(f"SINGLE UAV COURSES: Found {len(single_uav_threats)} threats", "single_uav")
        # Continue processing to also get regular location markers
        # Don't return early - we want both UAV course markers AND location markers

    # Check for multi-regional UAV messages
    if multi_regional_flag:
        multi_regional_threats = handle_multi_regional_uav()
        if multi_regional_threats:
            add_debug_log(f"MULTI-REGIONAL UAV: Found {len(multi_regional_threats)} threats", "multi_regional")
            return multi_regional_threats

    # Southeast-wide tactical aviation activity (no specific settlement): place a synthetic marker off SE border.
    se_phrase = lower if 'lower' in locals() else original_text.lower()
    if ('Ñ‚Ð°ÐºÑ‚Ð¸Ñ‡Ð½' in se_phrase or 'Ð°Ð²Ñ–Ð°Ñ†Ñ–' in se_phrase or 'Ð°Ð²Ð¸Ð°Ñ†' in se_phrase) and ('Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in se_phrase or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾ ÑÑ…Ñ–Ð´Ð½' in se_phrase or 'ÑŽÐ³Ð¾-Ð²Ð¾ÑÑ‚Ð¾Ñ‡' in se_phrase or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ' in se_phrase):
        # Approx point in Azov Sea off SE (between Mariupol & Berdyansk) to avoid implying exact impact
        lat, lng = 46.5, 37.5
        return [{
            'id': f"{mid}_se", 'place': 'ÐŸÑ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹ Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº', 'lat': lat, 'lng': lng,
            'threat_type': 'avia', 'text': original_text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': 'avia.png', 'source_match': 'southeast_aviation'
        }]
    # North-east tactical aviation activity - coordinates moved to Ukrainian territory
    # Original coordinates (50.4, 36.8) were too close to Russian border
    # SKIP if this is a multi-threat message (handled separately above)
    if ('Ñ‚Ð°ÐºÑ‚Ð¸Ñ‡Ð½' in se_phrase or 'Ð°Ð²Ñ–Ð°Ñ†Ñ–' in se_phrase or 'Ð°Ð²Ð¸Ð°Ñ†' in se_phrase) and (
        'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in se_phrase or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾ ÑÑ…Ñ–Ð´Ð½' in se_phrase or 'ÑÐµÐ²ÐµÑ€Ð¾-Ð²Ð¾ÑÑ‚Ð¾Ñ‡' in se_phrase or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾-ÑÑ…Ð¾Ð´Ñƒ' in se_phrase
    ) and not ('ðŸ›¬' in original_text and 'ðŸ›¸' in original_text):
        # Moved coordinates to Sumy area (clearly in Ukrainian territory)
        lat, lng = 50.9, 34.8  # Near Sumy city
        return [{
            'id': f"{mid}_ne", 'place': 'ÐŸÑ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹ Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº', 'lat': lat, 'lng': lng,
            'threat_type': 'avia', 'text': original_text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': 'avia.png', 'source_match': 'northeast_aviation'
        }]
    m = re.search(r'(\d{1,2}\.\d+),(\d{1,3}\.\d+)', text)
    if m:
        lat = float(m.group(1)); lng = float(m.group(2))
        threat_type, icon = classify(text)
        return [{
            'id': str(mid), 'place': 'Unknown', 'lat': lat, 'lng': lng,
            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': icon
        }]
    # Alarm cancellation always list-only
    if re.search(r'Ð²Ñ–Ð´Ð±Ñ–Ð¹\s+Ñ‚Ñ€Ð¸Ð²Ð¾Ð³|Ð¾Ñ‚Ð±Ð¾Ð¹\s+Ñ‚Ñ€ÐµÐ²Ð¾Ð³', original_text.lower()):
        return [{
            'id': str(mid), 'place': None, 'lat': None, 'lng': None,
            'threat_type': 'alarm_cancel', 'text': original_text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': 'vidboi.png', 'list_only': True
        }]
    lower = text.lower()
    # Specialized single-line pattern: direction from one oblast toward another (e.g. 'Ð±Ð¿Ð»Ð° ... ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ñ‰Ð¸Ð½Ñƒ')
    import re as _re_one
    m_dir_oblast = _re_one.search(r'Ð±Ð¿Ð»Ð°[^\n]*ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([a-zÐ°-ÑÑ—Ñ–Ñ”Ò‘\-]+Ñ‰Ð¸Ð½[Ð°ÑƒÑ–])', lower)
    if m_dir_oblast:
        dest = m_dir_oblast.group(1)
        # normalize accusative -> nominative
        dest_norm = dest.replace('Ñ‰Ð¸Ð½Ñƒ','Ñ‰Ð¸Ð½Ð°').replace('Ñ‰Ð¸Ð½Ð¸','Ñ‰Ð¸Ð½Ð°')
        if dest_norm in OBLAST_CENTERS:
            lat, lng = OBLAST_CENTERS[dest_norm]
            return [{
                'id': f"{mid}_dir_oblast", 'place': dest_norm.title(), 'lat': lat, 'lng': lng,
                'threat_type': 'uav', 'text': original_text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': 'shahed.png', 'source_match': 'singleline_oblast_course'
            }]
    # Extract drone / shahed count pattern (e.g. "7Ñ… Ð±Ð¿Ð»Ð°", "6x Ð´Ñ€Ð¾Ð½Ñ–Ð²", "10 Ñ… Ð±Ð¿Ð»Ð°") early so later branches can reuse
    drone_count = None
    m_count = re.search(r'(\b\d{1,3})\s*[xÑ…]\s*(?:Ð±Ð¿Ð»Ð°|Ð´Ñ€Ð¾Ð½|Ð´Ñ€Ð¾Ð½Ð¸|ÑˆÐ°Ñ…ÐµÐ´|ÑˆÐ°Ñ…ÐµÐ´Ð¸|ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð²)', lower)
    if m_count:
        try:
            drone_count = int(m_count.group(1))
        except ValueError:
            drone_count = None
    # Normalize some genitive forms ("Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ñ—" -> base) to capture multiple oblasts in one message
    GENITIVE_NORMALIZE = {
        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ñ—': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
        'Ð´Ð½ÐµÐ¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÐºÐ¾Ð¹': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ¾Ñ—': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».',
        'Ñ‡ÐµÑ€Ð½Ð¸Ð³Ð¾Ð²ÑÐºÐ¾Ð¹': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».',
        'ÑÑƒÐ¼ÑÑŒÐºÐ¾Ñ—': 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
        'ÑÑƒÐ¼ÑÐºÐ¾Ð¹': 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
        'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ¾Ñ—': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».',
        'Ñ…Ð°Ñ€ÑŒÐºÐ¾Ð²ÑÐºÐ¾Ð¹': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».'
    }
    for gform, base_form in GENITIVE_NORMALIZE.items():
        if gform in lower:
            lower = lower.replace(gform, base_form)
    # Locative / prepositional oblast & region endings -> base ("Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñ–" -> "Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°")
    LOCATIVE_NORMALIZE = {
        'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñ–': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°',
        'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ñ–': 'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°',
        'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñ–': 'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°',
        'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ñ–': 'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°',
        'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ñ–': 'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°',
        'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ñ–': 'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ñ‰Ð¸Ð½Ð°'
    }
    for lform, base_form in LOCATIVE_NORMALIZE.items():
        if lform in lower:
            lower = lower.replace(lform, base_form)
    # City genitive -> nominative (subset) for settlement detection
    CITY_GENITIVE = [
        ('Ñ…Ð°Ñ€ÐºÐ¾Ð²Ð°','Ñ…Ð°Ñ€ÐºÑ–Ð²'), ('ÐºÐ¸Ñ”Ð²Ð°','ÐºÐ¸Ñ—Ð²'), ('Ð»ÑŒÐ²Ð¾Ð²Ð°','Ð»ÑŒÐ²Ñ–Ð²'), ('Ð¾Ð´ÐµÑÐ¸','Ð¾Ð´ÐµÑÐ°'), ('Ð´Ð½Ñ–Ð¿Ñ€Ð°','Ð´Ð½Ñ–Ð¿Ñ€Ð¾')
    ]
    for gform, base in CITY_GENITIVE:
        if gform in lower:
            lower = lower.replace(gform, base)
    # Normalize some accusative oblast forms to nominative for matching
    lower = lower.replace('Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ñƒ','Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°').replace('ÑÑƒÐ¼Ñ‰Ð¸Ð½Ñƒ','ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°')
    text = lower  # downstream logic mostly uses lower-case comparisons
    # Ð¡Ð°Ð½Ñ–Ñ‚Ð¸Ð·Ð°Ñ†Ñ–Ñ Ð´ÑƒÐ±Ð»ÑŽÐ²Ð°Ð½ÑŒ Ñ‚Ð¸Ð¿Ñƒ "Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–" -> Ð·Ð°Ð»Ð¸ÑˆÐ°Ñ”Ð¼Ð¾ Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
    text = re.sub(r'(Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ|Ð¾Ð±Ð»\.)\s+Ð¾Ð±Ð»Ð°ÑÑ‚[Ñ–Ð¸]', r'\1', text)

    # --- Simple sanitization of formatting noise (bold asterisks, stray stars) ---
    # Keeps Ukrainian characters while removing leading/trailing markup like ** or * around segments
    if '**' in text or '*' in text:
        # remove isolated asterisks not part of words
        text = re.sub(r'\*+', '', text)

    # --- Early explicit pattern: "<RaionName> Ñ€Ð°Ð¹Ð¾Ð½ (<Oblast ...>)" (e.g. "Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½ (Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð».)") ---
    # Sometimes such messages were slipping through as raw because the pre-parenthesis token ended with 'Ñ€Ð°Ð¹Ð¾Ð½'.
    m_raion_oblast = re.search(r'([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{4,})\s+Ñ€Ð°Ð¹Ð¾Ð½\s*\(([^)]*Ð¾Ð±Ð»[^)]*)\)', text)
    if m_raion_oblast:
        raion_token = m_raion_oblast.group(1).strip().lower()
        # Normalize morphological endings similar to later norm_raion logic
        raion_base = re.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$', 'ÑÑŒÐºÐ¸Ð¹', raion_token)
        if raion_base in RAION_FALLBACK:
            lat, lng = RAION_FALLBACK[raion_base]
            threat_type, icon = classify(original_text if 'original_text' in locals() else text)
            # Maintain active raion alarm state
            if threat_type == 'alarm':
                RAION_ALARMS[raion_base] = {'place': f"{raion_base.title()} Ñ€Ð°Ð¹Ð¾Ð½", 'lat': lat, 'lng': lng, 'since': time.time()}
            elif threat_type == 'alarm_cancel':
                RAION_ALARMS.pop(raion_base, None)
            return [{
                'id': str(mid), 'place': f"{raion_base.title()} Ñ€Ð°Ð¹Ð¾Ð½", 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': (original_text if 'original_text' in locals() else text)[:500],
                'date': date_str, 'channel': channel, 'marker_icon': icon, 'source_match': 'raion_oblast_combo'
            }]
        else:
            log.debug(f"raion_oblast primary matched token={raion_token} base={raion_base} no coords")
    else:
        # Secondary heuristic fallback if formatting (emoji / markup) broke regex
        if 'Ñ€Ð°Ð¹Ð¾Ð½ (' in text and ' Ð¾Ð±Ð»' in text and has_threat(text):
            try:
                prefix = text.split('Ñ€Ð°Ð¹Ð¾Ð½ (',1)[0]
                cand = prefix.strip().split()[-1].lower()
                cand_base = re.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$', 'ÑÑŒÐºÐ¸Ð¹', cand)
                if cand_base in RAION_FALLBACK:
                    lat,lng = RAION_FALLBACK[cand_base]
                    threat_type, icon = classify(original_text if 'original_text' in locals() else text)
                    log.debug(f"raion_oblast secondary emit cand={cand} base={cand_base}")
                    return [{
                        'id': str(mid), 'place': f"{cand_base.title()} Ñ€Ð°Ð¹Ð¾Ð½", 'lat': lat, 'lng': lng,
                        'threat_type': threat_type, 'text': (original_text if 'original_text' in locals() else text)[:500],
                        'date': date_str, 'channel': channel, 'marker_icon': icon, 'source_match': 'raion_oblast_secondary'
                    }]
                else:
                    log.debug(f"raion_oblast secondary no coords cand={cand} base={cand_base}")
            except Exception as _e:
                log.debug(f"raion_oblast secondary error={_e}")

    # --- Russian strategic aviation suppression ---
    def _is_russian_strategic_aviation(t: str) -> bool:
        """Suppress messages about Russian strategic aviation (Tu-95, etc.) from Russian airbases"""
        t_lower = t.lower()
        
        # Check for Russian strategic bombers
        russian_bombers = ['Ñ‚Ñƒ-95', 'tu-95', 'Ñ‚Ñƒ-160', 'tu-160', 'Ñ‚Ñƒ-22', 'tu-22']
        has_bomber = any(bomber in t_lower for bomber in russian_bombers)
        
        # Check for Russian airbases and regions
        russian_airbases = ['ÐµÐ½Ð³ÐµÐ»ÑŒÑ', 'engels', 'ÑÐ½Ð³ÐµÐ»ÑŒÑ', 'ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²', 'Ñ€ÑÐ·Ð°Ð½ÑŒ', 'Ð¼ÑƒÑ€Ð¾Ð¼', 'ÑƒÐºÑ€Ð°Ñ–Ð½ÐºÐ°', 'ÑƒÐºÑ€Ð°Ñ—Ð½ÐºÐ°']
        has_russian_airbase = any(airbase in t_lower for airbase in russian_airbases)
        
        # Check for Russian regions/areas
        russian_regions = ['ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²ÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–', 'ÑÐ°Ñ€Ð°Ñ‚Ð¾Ð²ÑÐºÐ¾Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸', 'Ñ‚ÑƒÐ»ÑŒÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–', 'Ñ€ÑÐ·Ð°Ð½ÑÑŒÐºÑ–Ð¹ Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–']
        has_russian_region = any(region in t_lower for region in russian_regions)
        
        # Check for terms indicating Russian territory/airbases
        russian_territory_terms = ['Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð· Ð°ÐµÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ñƒ', 'Ñ Ð°ÑÑ€Ð¾Ð´Ñ€Ð¾Ð¼Ð°', 'Ð¼ÐµÑ‚Ð° Ð²Ð¸Ð»ÑŒÐ¾Ñ‚Ñƒ Ð½ÐµÐ²Ñ–Ð´Ð¾Ð¼Ð°', 'Ñ†ÐµÐ»ÑŒ Ð²Ñ‹Ð»ÐµÑ‚Ð° Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°']
        has_russian_territory = any(term in t_lower for term in russian_territory_terms)
        
        # Check for generic relocation/transfer terms without specific threats
        relocation_terms = ['Ð¿ÐµÑ€ÐµÐ´Ð¸ÑÐ»Ð¾ÐºÐ°Ñ†Ñ–Ñ', 'Ð¿ÐµÑ€ÐµÐ´Ð¸ÑÐ»Ð¾ÐºÐ°Ñ†Ð¸Ñ', 'Ð¿ÐµÑ€ÐµÐ»Ñ–Ñ‚', 'Ð¿ÐµÑ€ÐµÐ»ÐµÑ‚', 'Ð²Ñ–Ð´Ð¼Ñ–Ñ‡ÐµÐ½Ð¾', 'Ð¾Ñ‚Ð¼ÐµÑ‡ÐµÐ½Ð¾']
        has_relocation = any(term in t_lower for term in relocation_terms)
        
        # Suppress if it's about Russian bombers from Russian territory
        if has_bomber and (has_russian_airbase or has_russian_territory or has_russian_region):
            return True
            
        # Suppress relocation/transfer messages between Russian airbases
        if has_relocation and has_bomber and (has_russian_airbase or has_russian_region):
            return True
            
        # Also suppress general strategic aviation reports without specific Ukrainian targets
        if ('Ð±Ð¾Ñ€Ñ‚' in t_lower or 'Ð±Ð¾Ñ€Ñ‚Ð¸' in t_lower) and ('Ð¼ÐµÑ‚Ð° Ð²Ð¸Ð»ÑŒÐ¾Ñ‚Ñƒ Ð½ÐµÐ²Ñ–Ð´Ð¾Ð¼Ð°' in t_lower or 'Ñ†ÐµÐ»ÑŒ Ð²Ñ‹Ð»ÐµÑ‚Ð° Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°' in t_lower):
            return True
            
        return False

    # --- General warning suppression ---

    if _is_russian_strategic_aviation(text):
        return None
        
    if _is_general_warning_without_location(text):
        return None

    # --- Western border drone reconnaissance suppression ---
    def _is_western_border_reconnaissance(t: str) -> bool:
        """Suppress messages about drones crossing western borders (Hungary, etc.) - not related to Russian threats"""
        t_lower = t.lower()
        
        # Check for western border crossing indicators
        border_crossing_terms = [
            'Ð¿ÐµÑ€ÐµÑ‚Ð½ÑƒÐ² Ð´ÐµÑ€Ð¶Ð°Ð²Ð½Ð¸Ð¹ ÐºÐ¾Ñ€Ð´Ð¾Ð½', 'Ð¿ÐµÑ€ÐµÑÐµÐº Ð³Ð¾ÑÑƒÐ´Ð°Ñ€ÑÑ‚Ð²ÐµÐ½Ð½ÑƒÑŽ Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñƒ',
            'Ð¿ÐµÑ€ÐµÑ‚Ð½ÑƒÐ² ÐºÐ¾Ñ€Ð´Ð¾Ð½', 'Ð¿ÐµÑ€ÐµÑÐµÐº Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñƒ',
            'Ð· Ð±Ð¾ÐºÑƒ ÑƒÐ³Ð¾Ñ€Ñ‰Ð¸Ð½Ð¸', 'ÑÐ¾ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹ Ð²ÐµÐ½Ð³Ñ€Ð¸Ð¸',
            'Ð· Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ñ— ÑƒÐ³Ð¾Ñ€Ñ‰Ð¸Ð½Ð¸', 'Ñ Ñ‚ÐµÑ€Ñ€Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¸ Ð²ÐµÐ½Ð³Ñ€Ð¸Ð¸'
        ]
        has_border_crossing = any(term in t_lower for term in border_crossing_terms)
        
        # Check for western regions (primarily Zakarpattya)
        western_regions = ['Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚', 'Ð·Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑŒ', 'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´', 'Ð¼ÑƒÐºÐ°Ñ‡ÐµÐ²']
        has_western_region = any(region in t_lower for region in western_regions)
        
        # Check for reconnaissance/monitoring context (not combat threats)
        recon_terms = ['Ñ€Ð°Ð´Ð°Ñ€Ð¸ Ð·ÑÑƒ', 'Ñ€Ð°Ð´Ð°Ñ€Ñ‹ Ð²ÑÑƒ', 'Ð·Ð°Ñ„Ñ–ÐºÑÑƒÐ²Ð°Ð»Ð¸ Ð¿Ñ€Ð¾Ð»Ñ–Ñ‚', 'Ð·Ð°Ñ„Ð¸ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ Ð¿Ñ€Ð¾Ð»ÐµÑ‚', 'ÑÑ‚ÐµÐ¶Ð¸Ñ‚Ð¸ Ð·Ð° Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¾ÑŽ', 'ÑÐ»ÐµÐ´Ð¸Ñ‚ÑŒ Ð·Ð° Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¾Ð¹']
        has_recon_context = any(term in t_lower for term in recon_terms)
        
        # Suppress if it's about western border reconnaissance
        if has_border_crossing and has_western_region:
            return True
            
        # Also suppress general monitoring messages about western regions
        if has_western_region and has_recon_context and ('Ð´Ñ€Ð¾Ð½' in t_lower or 'Ð±Ð¿Ð»Ð°' in t_lower):
            return True
            
        return False

    if _is_western_border_reconnaissance(text):
        return None

    # --- Aggregate / statistical summary suppression ---
    def _is_aggregate_summary(t: str) -> bool:
        # Situation report override: if starts with 'Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°' we evaluate full logic first (word 'Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°' inside shouldn't unblock)
        starts_obst = t.startswith('Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°')
        # Do not suppress if explicit real-time warning words present (unless it's a structured situation report)
        if not starts_obst and any(w in t for w in ['Ð·Ð°Ð³Ñ€Ð¾Ð·Ð°','Ð¿ÐµÑ€ÐµÐ¹Ð´Ñ–Ñ‚ÑŒ Ð² ÑƒÐºÑ€Ð¸Ñ‚Ñ‚Ñ','ÑƒÐºÑ€Ð¸Ñ‚Ñ‚Ñ!']):
            return False
        verbs = ['Ð·Ð±Ð¸Ñ‚Ð¾/Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¾','Ð·Ð±Ð¸Ñ‚Ð¾ / Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¾','Ð·Ð±Ð¸Ñ‚Ð¾-Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¾','Ð·Ð±Ð¸Ñ‚Ð¾','Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¾','Ð·Ð½Ð¸Ñ‰ÐµÐ½Ð¾']
        context = ['ÑÑ‚Ð°Ð½Ð¾Ð¼ Ð½Ð°','Ð·Ð° Ð¿Ð¾Ð¿ÐµÑ€ÐµÐ´Ð½Ñ–Ð¼Ð¸ Ð´Ð°Ð½Ð¸Ð¼Ð¸','Ñƒ Ð½Ñ–Ñ‡ Ð½Ð°','Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð¸Ð¹ Ð½Ð°Ð¿Ð°Ð´','Ð¿Ñ€Ð¾Ñ‚Ð¸Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð¾ÑŽ Ð¾Ð±Ð¾Ñ€Ð¾Ð½Ð¾ÑŽ','Ð¿Ñ€Ð¾Ñ‚Ð¸Ð¿Ð¾Ð²Ñ–Ñ‚Ñ€ÑÐ½Ð° Ð¾Ð±Ð¾Ñ€Ð¾Ð½Ð°','Ð¿Ñ–Ð´Ñ€Ð¾Ð·Ð´Ñ–Ð»Ð¸ Ñ€ÐµÐ±','Ð¼Ð¾Ð±Ñ–Ð»ÑŒÐ½Ñ– Ð²Ð¾Ð³Ð½ÐµÐ²Ñ– Ð³Ñ€ÑƒÐ¿Ð¸','Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°']
        objects_re = re.compile(r'\b\d{1,3}[\-â€“]?(Ð¼Ð°|)?\s*(Ð²Ð¾Ñ€Ð¾Ð¶Ð¸Ñ…|)\s*(Ð±Ð¿Ð»Ð°|shahed|Ð´Ñ€Ð¾Ð½(?:Ñ–Ð²|Ð¸)?|Ñ€Ð°ÐºÐµÑ‚|Ñ€Ð°ÐºÐµÑ‚Ð¸)')
        verb_hit = any(v in t for v in verbs)
        ctx_hits = sum(1 for c in context if c in t)
        obj_hit = bool(objects_re.search(t))
        # Strong aggregate if all three categories present OR multiple context + objects
        if (verb_hit and obj_hit and ctx_hits >= 1) or (ctx_hits >= 2 and obj_hit):
            return True
        # Long multiline with origins list and many commas plus 'Ñ‚Ð¸Ð¿Ñƒ shahed'
        if 'Ñ‚Ð¸Ð¿Ñƒ shahed' in t and t.count('\n') >= 2 and obj_hit:
            return True
        # Situation report structure: starts with 'Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÑ‚Ð°Ð½Ð¾Ð¼ Ð½Ð°' or begins with 'Ð¾Ð±ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°' and multiple category lines (â€” ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ñ–Ñ‡Ð½Ð° Ð°Ð²Ñ–Ð°Ñ†Ñ–Ñ, â€” Ð±Ð¿Ð»Ð°, â€” Ñ„Ð»Ð¾Ñ‚)
        if starts_obst:
            dash_lines = sum(1 for line in t.split('\n') if line.strip().startswith('â€”'))
            if dash_lines >= 2:
                return True
        return False
    if _is_aggregate_summary(text):
        return None

    # --- Pattern: City (Oblast ...) e.g. "ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ (Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».)" ---
    bracket_city = re.search(r'([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})\s*\(([^)]+)\)', text)
    if bracket_city:
        raw_city = bracket_city.group(1).strip().lower()
        raw_inside = bracket_city.group(2).lower()
        # ÐžÑÐ¾Ð±Ñ‹Ð¹ ÑÐ»ÑƒÑ‡Ð°Ð¹: "Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ¸Ð¹ Ñ€-Ð½)" -> ÑÑ‚Ð°Ð²Ð¸Ð¼ ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´
        if ('Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ' in raw_city or 'Ð¾Ð±Ð»' in raw_city) and ('Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´' in raw_inside):
            pav_key = 'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ¸Ð¹'
            if pav_key in RAION_FALLBACK:
                lat,lng = RAION_FALLBACK[pav_key]
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': 'ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½', 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'oblast_raion_combo'
                }]
        # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¸ Ð²Ð¸Ð´Ð° "<Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ> (<Ñ€Ð°Ð¹Ð¾Ð½ ...>)" Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ñ‚Ñ€Ð°ÐºÑ‚Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ»Ð¾Ð²Ð¾ 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ' ÐºÐ°Ðº Ð³Ð¾Ñ€Ð¾Ð´
        if raw_city in {'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ','Ð¾Ð±Ð»','Ð¾Ð±Ð».'} or raw_city.endswith('Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ'):
            bracket_city = None
    if bracket_city and raw_city != 'Ñ€Ð°Ð¹Ð¾Ð½':
            norm_city = UA_CITY_NORMALIZE.get(raw_city, raw_city)
            # Initial local attempt (static minimal list)
            coords = CITY_COORDS.get(norm_city)
            # Region hint extraction
            region_hint = None
            if any(tok in raw_inside for tok in ['Ð¾Ð±Ð»', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ']):
                region_hint = raw_inside.strip()
            # 1) Explicit override for (city, region) if provided
            if region_hint:
                override_key = (norm_city, region_hint)
                if override_key in OBLAST_CITY_OVERRIDES:
                    coords = OBLAST_CITY_OVERRIDES[override_key]
            # 2) If we have region hint and NO coords yet, attempt region-qualified geocode first (priority to enforce oblast binding)
            region_combo_tried = False
            if not coords and region_hint and OPENCAGE_API_KEY:
                combo_query = f"{norm_city} {region_hint}".replace('  ',' ').strip()
                try:
                    refined = geocode_opencage(combo_query)
                    if refined:
                        coords = refined
                    region_combo_tried = True
                except Exception:
                    pass
            # 3) Attempt city alone geocode only if still no coords
            if not coords and OPENCAGE_API_KEY:
                try:
                    coords = geocode_opencage(norm_city)
                except Exception:
                    pass
            # 4) If region hint exists and we got coords from plain city geocode but city is potentially duplicated across oblasts,
            # try region-qualified geocode as refinement (unless already tried).
            if region_hint and OPENCAGE_API_KEY and not region_combo_tried and coords and norm_city in ['Ð±Ð¾Ñ€Ð¾Ð²Ð°','Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÐºÐ°','Ð½Ð¸ÐºÐ¾Ð»Ð°ÐµÐ²ÐºÐ°']:
                try:
                    combo_query = f"{norm_city} {region_hint}".replace('  ',' ').strip()
                    refined2 = geocode_opencage(combo_query)
                    if refined2:
                        coords = refined2
                except Exception:
                    pass
            # Ambiguous manual mapping fallback (if still no coords or mismatch with region)
            if region_hint:
                # derive stem like 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐº', 'Ð»ÑŒÐ²Ñ–Ð²ÑÑŒÐº'
                rh_low = region_hint.lower()
                # choose first word containing 'Ñ…Ð°Ñ€ÐºÑ–Ð²' etc
                region_key = None
                for stem in ['Ñ…Ð°Ñ€ÐºÑ–Ð²','Ð»ÑŒÐ²Ñ–Ð²','ÐºÐ¸Ñ—Ð²','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²','Ð¿Ð¾Ð»Ñ‚Ð°Ð²','ÑÑƒÐ¼','Ñ‡ÐµÑ€Ð½Ñ–Ð³','Ð²Ð¾Ð»Ð¸Ð½','Ð·Ð°Ð¿Ð¾Ñ€','Ð¾Ð´ÐµÑ','Ð¼Ð¸ÐºÐ¾Ð»Ð°','Ñ‡ÐµÑ€ÐºÐ°','Ð¶Ð¸Ñ‚Ð¾Ð¼','Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒ','Ñ€Ñ–Ð²Ð½','Ñ–Ð²Ð°Ð½Ð¾','Ñ‚ÐµÑ€Ð½Ð¾Ð¿','ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´','ÐºÑ€Ð¾Ð¿Ð¸Ð²','Ð»ÑƒÐ³Ð°Ð½','Ð´Ð¾Ð½ÐµÑ†ÑŒ','Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†']:
                    if stem in rh_low:
                        region_key = stem
                        break
                AMBIGUOUS_CITY_REGION = {
                    ('Ð·Ð¾Ð»Ð¾Ñ‡Ñ–Ð²','Ñ…Ð°Ñ€ÐºÑ–Ð²'): (50.2788, 36.3644),  # Zolochiv Kharkiv oblast
                    ('Ð·Ð¾Ð»Ð¾Ñ‡Ñ–Ð²','Ð»ÑŒÐ²Ñ–Ð²'): (49.8078, 24.9002),   # Zolochiv Lviv oblast
                }
                if region_key:
                    key = (norm_city, region_key)
                    mapped = AMBIGUOUS_CITY_REGION.get(key)
                    if mapped:
                        coords = mapped
            if coords:
                lat,lng = coords
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': norm_city.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'bracket_city'
                }]

    # --- Multi-segment / enumerated lines (1. 2. 3.) region extraction ---
    # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ°Ð¼, ÑÐ¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹; ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹
    
    # PRIORITY: Detect trajectory patterns BEFORE multi-region processing
    # Pattern: "Ð· [source_region] Ð½Ð° [target_region(s)]" - trajectory, not multi-target
    trajectory_pattern = r'(\d+)?\s*ÑˆÐ°Ñ…ÐµÐ´[Ñ–Ñ—Ð²Ñ‹Ð¸Ñ”]*\s+Ð·\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘]+(Ñ‰Ð¸Ð½|Ñ‡Ñ‡Ð¸Ð½)[Ð°ÑƒÐ¸Ñ–])\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”Ò‘/]+(Ñ‰Ð¸Ð½|Ñ‡Ñ‡Ð¸Ð½)[Ð°ÑƒÐ¸Ñ–Ñƒ])'
    trajectory_match = re.search(trajectory_pattern, text.lower(), re.IGNORECASE)
    
    if trajectory_match:
        count_str = trajectory_match.group(1)
        source_region = trajectory_match.group(2)
        target_regions = trajectory_match.group(4)
        
        print(f"DEBUG: Trajectory detected - {count_str or ''}ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð· {source_region} Ð½Ð° {target_regions}")
        
        # For trajectory messages, we should NOT create markers in region centers
        # This represents movement through airspace, not attacks on specific locations
        # Options:
        # 1. Don't create any markers (trajectory only)
        # 2. Create trajectory line visualization 
        # 3. Create border crossing markers
        
        # For now, suppress markers for pure trajectory messages
        print(f"DEBUG: Suppressing region markers for trajectory message")
        return None
    
    region_hits = []  # list of (display_name, (lat,lng), snippet)
    # Treat semicolons as separators like newlines for multi-segment parsing
    seg_text = text.replace(';', '\n')
    lines = [ln.strip() for ln in seg_text.split('\n') if ln.strip()]
    # Pre-flag launch site style multi-line posts to avoid RAW fallback â€“ treat each line with a launch phrase as separate pseudo-track (no coords yet)
    launch_mode = any(ln.lower().startswith('Ð²Ñ–Ð´Ð¼Ñ–Ñ‡ÐµÐ½Ñ– Ð¿ÑƒÑÐºÐ¸') or ln.lower().startswith('+ Ð¿ÑƒÑÐºÐ¸') for ln in lines)
    for ln in lines:
        ln_low = ln.lower()
        local_regions = []
        for name, coords in OBLAST_CENTERS.items():
            if name in ln_low:
                local_regions.append((name, coords))
        # ÐµÑÐ»Ð¸ Ð² ÑÑ‚Ñ€Ð¾ÐºÐµ Ð±Ð¾Ð»ÐµÐµ 1â€” ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ, Ð¸Ð½Ð°Ñ‡Ðµ Ð¾Ð´Ð½Ñƒ
        for (rn, rc) in local_regions:
            region_hits.append((rn.title(), rc, ln[:180]))
    # Ð¯ÐºÑ‰Ð¾ Ð·Ð½Ð°Ð¹ÑˆÐ»Ð¸ >=2 Ñ€ÐµÐ³Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¸Ñ… Ð¼Ð°Ñ€ÐºÐµÑ€Ð¸ Ð² Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ð¿ÑƒÐ½ÐºÑ‚Ð°Ñ… ÑÐ¿Ð¸ÑÐºÑƒ â€” Ñ„Ð¾Ñ€Ð¼ÑƒÑ”Ð¼Ð¾ Ð¼Ð½Ð¾Ð¶Ð¸Ð½Ð½Ñ– Ñ‚Ñ€ÐµÐºÐ¸
    if len(region_hits) >= 2 and not launch_mode:
        # Ð’Ð˜Ð”ÐÐ›Ð•ÐÐž Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÑƒ course_line_present - Ñ‚ÐµÐ¿ÐµÑ€ Ð·Ð°Ð²Ð¶Ð´Ð¸ Ð´Ð¾Ð·Ð²Ð¾Ð»ÑÑ”Ð¼Ð¾ region markers + course parsing
        if True:  # Ð·Ð°Ð²Ð¶Ð´Ð¸ Ð²Ð¸ÐºÐ¾Ð½ÑƒÑ”Ð¼Ð¾ Ð±Ð»Ð¾Ðº Ñ€ÐµÐ³Ñ–Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¸Ñ… Ð¼Ð°Ñ€ÐºÐµÑ€Ñ–Ð²
            # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ñ ÑƒÐ³Ñ€Ð¾Ð·Ñ‹ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ
            if not has_threat(text):
                return None
            threat_type, icon = classify(text)
            tracks = []
            # deduplicate by name
            seen_names = set()
            # Directional offset helper
            def directional_offset(rlabel: str, lat: float, lng: float):
                base = rlabel.lower().split()[0]
                full = text  # already lower
                # detect "Ð½Ð° ÑÑ…Ñ–Ð´ <base>", "ÑÑ…Ñ–Ð´ <base>", etc., but ignore origins "Ð· Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´Ñƒ" for that base
                # We only tag if phrase contains base key AFTER direction (targeting side), not originating "Ð· <dir> ..." alone.
                directions = [
                    ('ÑÑ…Ñ–Ð´', 'east', (0.0, 0.9)),
                    ('Ð·Ð°Ñ…Ñ–Ð´', 'west', (0.0, -0.9)),
                    ('Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', 'north', (0.7, 0.0)),
                    ('Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ', 'south', (-0.7, 0.0))
                ]
                applied = None
                for word, code, (dlat, dlng) in directions:
                    patterns = [f"Ð½Ð° {word} {base}", f" {word} {base}"]
                    if any(pat in full for pat in patterns) and f"Ð· {word}" not in full:
                        applied = (code, dlat, dlng)
                        break
                if not applied:
                    return lat, lng, rlabel
                _, dlat, dlng = applied
                nlat = max(43.0, min(53.5, lat + dlat))
                nlng = max(21.0, min(41.0, lng + dlng))
                human = {'east':'ÑÑ…Ñ–Ð´','west':'Ð·Ð°Ñ…Ñ–Ð´','north':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡','south':'Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ'}[applied[0]]
                return nlat, nlng, f"{rlabel} ({human})"
            for idx, (rname, (lat,lng), snippet) in enumerate(region_hits, 1):
                if rname in seen_names: continue
                seen_names.add(rname)
                adj_lat, adj_lng, adj_label = directional_offset(rname, lat, lng)
                tracks.append({
                    'id': f"{mid}_{idx}", 'place': adj_label, 'lat': adj_lat, 'lng': adj_lng,
                    'threat_type': threat_type, 'text': snippet[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'region_multi'
                })
            if tracks:
                return tracks

    # --- Single border oblast KAB launch: place marker at predefined border point ---
    if len(region_hits) == 1 and 'ÐºÐ°Ð±' in lower and ('Ð¿ÑƒÑÐº' in lower or 'Ð¿ÑƒÑÐºÐ¸' in lower):
        rname, (olat, olng), snippet = region_hits[0]
        # Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸ Ð´Ð»Ñ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ
        key = rname.lower()
        BORDER_POINTS = {
            'Ð´Ð¾Ð½ÐµÑ‡Ñ‡Ð¸Ð½Ð°': (48.20, 37.90),
            'Ð´Ð¾Ð½ÐµÑ†ÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.20, 37.90),
            'ÑÑƒÐ¼Ñ‰Ð¸Ð½Ð°': (51.30, 34.40),
            'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (51.30, 34.40),
            'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°': (51.75, 31.60),
            'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (51.75, 31.60),
            'Ñ…Ð°Ñ€ÐºÑ–Ð²Ñ‰Ð¸Ð½Ð°': (50.25, 36.85),
            'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð».': (50.25, 36.85),
            'Ð»ÑƒÐ³Ð°Ð½Ñ‰Ð¸Ð½Ð°': (48.90, 39.40),
            'Ð»ÑƒÐ³Ð°Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ': (48.90, 39.40),
            'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ° Ð¾Ð±Ð».': (47.55, 35.60),
            'Ñ…ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ° Ð¾Ð±Ð».': (46.65, 32.60)
        }
        # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»ÑŽÑ‡Ð° (ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€ / Ð»Ð¸ÑˆÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹)
        k_simple = key.replace('â€™','').replace("'",'').strip()
        # Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
        coord = None
        for bk, bcoord in BORDER_POINTS.items():
            if bk in k_simple:
                coord = bcoord
                break
        if coord:
            threat_type, icon = classify(text)
            return [{
                'id': str(mid), 'place': rname + ' (ÐºÐ¾Ñ€Ð´Ð¾Ð½)', 'lat': coord[0], 'lng': coord[1],
                'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'source_match': 'border_kab'
            }]

    # --- Pattern: multiple shaheds with counts / directions / near-pass ("Ð¿Ð¾Ð²Ð·") ---
    # Handles composite direction phrases (Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´Ñƒ -> Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹ ÑÑ…Ñ–Ð´, Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´ Ñ‚Ð¾Ñ‰Ð¾)
    # Examples: "14 ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² ... 3 Ð½Ð° ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐµ Ð· Ð¿Ñ–Ð²Ð´Ð½Ñ, 9 Ð½Ð° ÐŸÐµÑ‚Ñ€Ð¾Ð¿Ð°Ð²Ð»Ñ–Ð²ÐºÑƒ Ð· Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´Ñƒ, 2 Ð½Ð° Ð¨Ð°Ñ…Ñ‚Ð°Ñ€ÑÑŒÐºÐµ Ð· Ð¿Ñ–Ð²Ð´Ð½Ñ"
    #           "16 ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² ... 2 Ð¿Ð¾Ð²Ð· Ð¢ÐµÑ€ÐµÐ½Ñ‚Ñ–Ð²ÐºÑƒ Ð½Ð° Ð¿Ñ–Ð²Ð½Ñ–Ñ‡, 6 Ð½Ð° Ð®Ñ€Ñ–Ñ—Ð²ÐºÑƒ Ð· Ð¿Ñ–Ð²Ð´Ð½Ñ, 7 Ð¿Ð¾Ð²Ð· ÐœÐµÐ¶Ð¾Ð²Ñƒ Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´" etc.
    if 'ÑˆÐ°Ñ…ÐµÐ´' in lower and ((' Ð½Ð° ' in lower) or (' Ð¿Ð¾Ð²Ð· ' in lower)):
        segs = re.split(r'[\n,âš ;]+', lower)
        found = []
        # Direction phrases may appear after 'Ð·', 'Ð·Ñ–', 'Ñ–Ð·', 'Ð½Ð°'. Capture full tail then normalize.
        pat_on = re.compile(r'(\d{1,2})\s+Ð½Ð°\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-Ê¼\']{3,})(?:Ñƒ|Ð°|Ðµ)?(?:\s+((?:Ð·|Ð·Ñ–|Ñ–Ð·|Ð½Ð°)\s+[a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-\s]+))?')
        pat_povz = re.compile(r'(\d{1,2})\s+Ð¿Ð¾Ð²Ð·\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-Ê¼\']{3,})(?:Ñƒ|Ð°|Ðµ)?(?:\s+(?:Ð½Ð°\s+)?([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-\s]+))?')
        def normalize_direction(raw_dir: str) -> str:
            if not raw_dir:
                return ''
            d = raw_dir.lower().strip()
            # remove leading prepositions
            d = re.sub(r'^(Ð·|Ð·Ñ–|Ñ–Ð·|Ð½Ð°|Ð²Ñ–Ð´)\s+', '', d)
            d = d.replace('â€“','-')
            # unify hyphen variants to space-separated tokens
            d = d.replace('-', ' ')
            d = re.sub(r'\s+', ' ', d).strip()
            # morphological endings -> base cardinal forms
            repl = [
                (r'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´Ñƒ', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹ ÑÑ…Ñ–Ð´'),
                (r'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾ ÑÑ…Ð¾Ð´Ñƒ', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹ ÑÑ…Ñ–Ð´'),
                (r'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð·Ð°Ñ…Ð¾Ð´Ñƒ', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹ Ð·Ð°Ñ…Ñ–Ð´'),
                (r'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾Ð³Ð¾ Ð·Ð°Ñ…Ð¾Ð´Ñƒ', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹ Ð·Ð°Ñ…Ñ–Ð´'),
                (r'Ð¿Ñ–Ð²Ð´Ð½Ñ', 'Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ'),
                (r'Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–', 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡'),
                (r'ÑÑ…Ð¾Ð´Ñƒ', 'ÑÑ…Ñ–Ð´'),
                (r'Ð·Ð°Ñ…Ð¾Ð´Ñƒ', 'Ð·Ð°Ñ…Ñ–Ð´')
            ]
            for pat, rep in repl:
                d = re.sub(pat, rep, d)
            # collapse duplicate words
            parts = []
            seen = set()
            for tok in d.split():
                if tok in seen:
                    continue
                seen.add(tok)
                parts.append(tok)
            return ' '.join(parts)
        for seg in segs:
            # Strip common trailing separators (colon, semicolon, space, slash, backslash)
            s = seg.strip(':; /\\')
            if not s or s.isdigit():
                continue
            matches = []
            matches.extend(list(pat_on.finditer(s)))
            matches.extend(list(pat_povz.finditer(s)))
            for m in matches:
                cnt = int(m.group(1))
                place_token = (m.group(2) or '').strip("-'Ê¼")
                raw_dir = ''
                # pat_on group(3); pat_povz group(3)
                if len(m.groups()) >= 3:
                    raw_dir = (m.group(3) or '').strip()
                direction = normalize_direction(raw_dir)
                place_token = place_token.replace('Ê¼',"'")
                variants = {place_token}
                # heuristic nominative recovery
                if place_token.endswith('ÐºÑƒ'): variants.add(place_token[:-2]+'ÐºÐ°')
                if place_token.endswith('Ð²Ñƒ'): variants.add(place_token[:-2]+'Ð²Ð°')
                if place_token.endswith('Ð¾Ð²Ñƒ'): variants.add(place_token[:-3]+'Ð¾Ð²Ð°')
                if place_token.endswith('ÑŽ'):
                    variants.add(place_token[:-1]+'Ñ'); variants.add(place_token[:-1]+'Ð°')
                if place_token.endswith('Ñƒ'): variants.add(place_token[:-1]+'Ð°')
                if place_token.endswith('Ð¾Ð¼Ñƒ'):
                    variants.add(place_token[:-3]+'Ðµ'); variants.add(place_token[:-3])
                matched_coord = None; matched_name = None
                for var in variants:
                    if var in CITY_COORDS:
                        matched_coord = CITY_COORDS[var]; matched_name = var; break
                if matched_coord:
                    plat, plng = matched_coord
                    found.append((matched_name, plat, plng, cnt, direction, s[:160]))
        if found:
            threat_type, icon = classify(text)
            tracks = []
            for idx,(p, plat, plng, cnt, direction, snippet) in enumerate(found,1):
                base_label = f"{p.title()} ({cnt})"
                if direction:
                    base_label += f" â†{direction}"
                tracks.append({
                    'id': f"{mid}_s{idx}", 'place': base_label, 'lat': plat, 'lng': plng,
                    'threat_type': threat_type, 'text': snippet[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'multi_shah_ed', 'count': cnt
                })
            if found and not tracks:
                log.debug(f"multi_shah_ed matched segments but no tracks mid={mid} raw={found}")
            if tracks:
                log.debug(f"multi_shah_ed tracks mid={mid} -> {[t['place'] for t in tracks]}")
                return tracks

    # --- Per-line UAV course / area city targeting ("Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° <Ð¼Ñ–ÑÑ‚Ð¾>", "8Ñ… Ð‘Ð¿Ð›Ð Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ– <Ð¼Ñ–ÑÑ‚Ð°>") ---
    # Triggered when region multi list suppressed earlier due to presence of course lines.
    if 'Ð±Ð¿Ð»Ð°' in lower and ('ÐºÑƒÑ€Ñ' in lower or 'Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ–' in lower):
        add_debug_log(f"UAV course parser triggered for message length: {len(text)} chars", "uav_course")
        original_text_norm = re.sub(r'(?i)(\b[Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,}(?:Ñ‰Ð¸Ð½Ð°|Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ|Ð¾Ð±Ð»\.)):(?!\s*\n)', r'\1:\n', original_text)
        lines_with_region = []
        current_region_hdr = None
        for raw_ln in original_text_norm.splitlines():
            ln_stripped = raw_ln.strip()
            if not ln_stripped:
                continue
            low_ln = ln_stripped.lower()
            # Allow region header if line ends with ':' even if preceded by emoji or bullets
            if low_ln.endswith(':'):
                # remove leading emojis/symbols
                cleaned_hdr = re.sub(r'^[^a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘]+','', low_ln[:-1])
                base_hdr = cleaned_hdr.strip()
                log.debug(f"mid={mid} region_header_check: '{low_ln}' -> cleaned: '{base_hdr}' -> found: {base_hdr in OBLAST_CENTERS}")
                if base_hdr in OBLAST_CENTERS:
                    current_region_hdr = base_hdr
                    log.debug(f"mid={mid} region_header_set: '{base_hdr}'")
                continue
            # split by semicolons; also break on pattern like " 2Ñ… Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼" inside the same segment later
            subparts = [p.strip() for p in re.split(r'[;]+', ln_stripped) if p.strip()]
            for part in subparts:
                lines_with_region.append((part, current_region_hdr))
        # Further split segments that contain multiple "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€Ñ" phrases glued together
        multi_start_re = re.compile(r'(?:\d+\s*[xÑ…]?\s*)?Ð±Ð¿Ð»Ð°\s*ÐºÑƒÑ€Ñ', re.IGNORECASE)
        expanded = []
        for part, region_hdr in lines_with_region:
            low_part = part.lower()
            starts = [m.start() for m in multi_start_re.finditer(low_part)]
            if len(starts) <= 1:
                expanded.append((part, region_hdr))
                continue
            for idx, s in enumerate(starts):
                seg_start = s
                seg_end = starts[idx+1] if idx+1 < len(starts) else len(low_part)
                segment = part[seg_start:seg_end].strip()
                if segment:
                    expanded.append((segment, region_hdr))
        if expanded:
            lines_with_region = expanded
        course_tracks = []
        pat_count_course = re.compile(r'^(\d+)\s*[xÑ…]?\s*Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-â€™Ê¼`\s]{3,40}?)(?=[,\.\n;:!\?]|$)', re.IGNORECASE)
        pat_course = re.compile(r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+(?:Ð½\.Ð¿\.?\s*)?([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-â€™Ê¼`\s]{3,40}?)(?=[,\.\n;:!\?]|$)', re.IGNORECASE)
        pat_area = re.compile(r'(\d+)?[xÑ…]?\s*Ð±Ð¿Ð»Ð°\s+Ð²\s+Ñ€Ð°Ð¹Ð¾Ð½Ñ–\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-â€™Ê¼`\s]{3,40}?)(?=[,\.\n;:!\?]|$)', re.IGNORECASE)
        if re.search(r'Ð±Ð¿Ð»Ð°.*?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+ÐºÑ–Ð¿Ñ‚[Ñ–Ñ—]', lower):
            coords = SETTLEMENT_FALLBACK.get('ÐºÑ–Ð¿Ñ‚Ñ–')
            if coords:
                lat, lng = coords
                threat_type, icon = classify(original_text)
                return [{
                    'id': f"{mid}_kipti_course", 'place': 'ÐšÑ–Ð¿Ñ‚Ñ–', 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'course_kipti'
                }]
        def norm_city_token(tok: str) -> str:
            t = tok.lower().strip(" .,'â€™Ê¼`-:")
            t = t.replace("'", "'")  # Normalize curly quotes
            if t.endswith('ÐºÑƒ'): t = t[:-2] + 'ÐºÐ°'
            elif t.endswith('Ð²Ñƒ'): t = t[:-2] + 'Ð²Ð°'
            elif t.endswith('Ð¾Ð²Ñƒ'): t = t[:-3] + 'Ð¾Ð²Ð°'
            elif t.endswith('ÑŽ'): t = t[:-1] + 'Ñ'
            elif t.endswith('Ñƒ'): t = t[:-1] + 'Ð°'
            if t.startswith('Ð½Ð¾Ð²Ñƒ '):
                t = 'Ð½Ð¾Ð²Ð° ' + t[5:]
            t = t.replace('Ð²Ð¾Ð´Ð¾Ð»Ð°Ð³Ñƒ','Ð²Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð°')
            return t
        for ln, region_hdr in lines_with_region:
            ln_low = ln.lower()
            if 'Ð±Ð¿Ð»Ð°' not in ln_low:
                continue
            add_debug_log(f"Processing UAV line: '{ln[:100]}...' (region: {region_hdr})", "uav_course")
            count = None; city = None; approx_flag = False
            m1 = pat_count_course.search(ln_low)
            if m1:
                count = int(m1.group(1)); city = m1.group(2)
            else:
                m2 = pat_area.search(ln_low)
                if m2:
                    if m2.group(1):
                        count = int(m2.group(1))
                    city = m2.group(2)
                else:
                    m3 = pat_course.search(ln_low)
                    if m3:
                        city = m3.group(1)
            if not city:
                add_debug_log("No city found in UAV line", "uav_course")
                continue
            add_debug_log(f"Found city '{city}' in UAV line", "uav_course")
            multi_norm = _resolve_city_candidate(city)
            base = norm_city_token(multi_norm)
            add_debug_log(f"City normalized to '{base}'", "uav_course")
            coords = CITY_COORDS.get(base) or (SETTLEMENTS_INDEX.get(base) if SETTLEMENTS_INDEX else None)
            add_debug_log(f"Coordinates lookup for '{base}': {coords}", "uav_course")
            if not coords:
                try:
                    coords = region_enhanced_coords(base, region_hint_override=region_hdr)
                except Exception:
                    coords = None
            if not coords:
                # Fallback: if we have a region header, place placeholder near its oblast center with slight jitter
                if region_hdr and region_hdr in OBLAST_CENTERS:
                    rlat, rlng = OBLAST_CENTERS[region_hdr]
                    # deterministic jitter based on hash of city token
                    h = abs(hash(base)) % 1000 / 1000.0
                    lat = max(43.0, min(53.5, rlat + (h - 0.5) * 0.4))
                    lng = max(21.0, min(41.0, rlng + (h - 0.5) * 0.6))
                    coords = (lat, lng)
                    approx_flag = True
                else:
                    # skip THIS city but continue processing other cities
                    add_debug_log(f"Skipping unrecognized city '{base}' - no coordinates and no region context", "uav_course")
                    continue
            if base not in CITY_COORDS:
                CITY_COORDS[base] = coords
            lat, lng = coords
            threat_type, icon = classify(text)
            # Generate individual markers per drone for progressive map loading
            total = count or 1
            for i in range(1, total+1):
                label = base.title()
                if total > 1:
                    label += f" ({i}/{total})"
                if region_hdr and region_hdr not in label.lower():
                    label += f" [{region_hdr.title()}]"
                if approx_flag:
                    label += ' ~'
                course_tracks.append({
                    'id': f"{mid}_c{len(course_tracks)+1}", 'place': label, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': ln[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'course_city_unit', 'count': 1
                })
                add_debug_log(f"Created course track for '{label}' at {lat}, {lng}", "uav_course")
        add_debug_log(f"Total course tracks generated: {len(course_tracks)}", "uav_course")
        log.debug(f"mid={mid} course_tracks_generated: {len(course_tracks)} tracks")
        if course_tracks:
            return course_tracks
        # Salvage fallback: large multi-line message with many 'Ð±Ð¿Ð»Ð° ÐºÑƒÑ€ÑÐ¾Ð¼' but parser produced nothing
        try:
            ll_full = text.lower()
            if course_tracks == [] and ll_full.count('Ð±Ð¿Ð»Ð°') >= 5 and ll_full.count('ÐºÑƒÑ€Ñ') >= 5:
                pat_salv = re.compile(r'(?:\d+\s*[xÑ…]?\s*)?Ð±Ð¿Ð»Ð°[^\n]{0,60}?ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-Ê¼"â€œâ€\'`\s]{3,40})', re.IGNORECASE)
                raw_hits = [m.group(1).strip() for m in pat_salv.finditer(ll_full)]
                uniq = []
                for h in raw_hits:
                    if h and h not in uniq:
                        uniq.append(h)
                salvage_tracks = []
                for idx, token in enumerate(uniq, 1):
                    base_tok = _resolve_city_candidate(token)
                    base_tok = norm_city_token(base_tok)
                    coords = CITY_COORDS.get(base_tok) or (SETTLEMENTS_INDEX.get(base_tok) if SETTLEMENTS_INDEX else None)
                    if not coords:
                        continue
                    lat, lng = coords
                    threat_type, icon = classify(text)
                    salvage_tracks.append({
                        'id': f"{mid}_sf{idx}", 'place': base_tok.title(), 'lat': lat, 'lng': lng,
                        'threat_type': threat_type, 'text': token[:120], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'salvage_course_multi'
                    })
                if salvage_tracks:
                    log.debug(f"salvage_course_multi generated {len(salvage_tracks)} tracks mid={mid}")
                    return salvage_tracks
        except Exception as _e_salv:
            log.debug(f'salvage fallback error mid={mid}: {_e_salv}')

    # --- Generic multi-line UAV near-pass counts (e.g. "5Ñ… Ð±Ð¿Ð»Ð° Ð¿Ð¾Ð²Ð· Ð‘Ð°Ñ€Ð²Ñ–Ð½ÐºÐ¾Ð²Ðµ") ---
    if 'Ð±Ð¿Ð»Ð°' in lower and 'Ð¿Ð¾Ð²Ð·' in lower and re.search(r'\d+[xÑ…]\s*Ð±Ð¿Ð»Ð°', lower):
        lines_near = [ln.strip() for ln in lower.split('\n') if ln.strip()]
        near_tracks = []
        pat_near = re.compile(r'(\d+)[xÑ…]\s*Ð±Ð¿Ð»Ð°[^\n]*?Ð¿Ð¾Ð²Ð·\s+([a-zÐ°-ÑÑ–Ñ—Ñ”Ò‘\-Ê¼\']{3,})')
        for ln in lines_near:
            m = pat_near.search(ln)
            if not m:
                continue
            cnt = int(m.group(1))
            place = (m.group(2) or '').strip("-'Ê¼")
            variants = {place}
            if place.endswith('Ðµ'): variants.add(place[:-1])
            if place.endswith('ÑŽ'):
                variants.add(place[:-1]+'Ñ'); variants.add(place[:-1]+'Ð°')
            if place.endswith('Ñƒ'):
                variants.add(place[:-1]+'Ð°')
            if place.endswith('Ð¾Ð¼Ñƒ'):
                variants.add(place[:-3])
            if place.endswith('Ð¾Ð²Ðµ'):
                variants.add(place[:-2]+'â€™Ñ')  # crude alt
            matched=None; mname=None
            for v in variants:
                if v in CITY_COORDS:
                    matched=CITY_COORDS[v]; mname=v; break
            if not matched and SETTLEMENTS_INDEX:
                for v in variants:
                    if v in SETTLEMENTS_INDEX:
                        matched=SETTLEMENTS_INDEX[v]; mname=v; break
            if not matched:
                # OpenCage fallback
                try:
                    for v in variants:
                        oc = region_enhanced_coords(v)
                        if oc:
                            matched=oc; mname=v; break
                except Exception:
                    matched=None
            if matched:
                if mname not in CITY_COORDS:
                    CITY_COORDS[mname]=matched
                lat,lng = matched
                threat_type, icon = classify(text)
                near_tracks.append({
                    'id': f"{mid}_n{len(near_tracks)+1}", 'place': f"{mname.title()} ({cnt})", 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': ln[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'uav_near_pass', 'count': cnt
                })
        if near_tracks:
            return near_tracks

    # --- Late parenthetical specific settlement fallback (e.g. direction to oblast but (Ð·Ð°Ñ‚Ð¾ÐºÐ°)) ---
    if has_threat(original_text.lower()) and '(' in original_text and ')' in original_text:
        p_tokens = re.findall(r'\(([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-\s]{3,})\)', original_text.lower())
        if p_tokens:
            cand = p_tokens[-1].strip()
            cand = re.sub(r'^(ÑÐ¼Ñ‚|Ñ\.|Ð¼\.|Ð¼Ñ–ÑÑ‚Ð¾|ÑÐµÐ»Ð¸Ñ‰Ðµ)\s+','', cand)
            base_cand = UA_CITY_NORMALIZE.get(cand, cand)
            coords = CITY_COORDS.get(base_cand) or SETTLEMENTS_INDEX.get(base_cand)
            log.debug(f"late_parenthetical mid={mid} cand={cand} base={base_cand} found={bool(coords)}")
            if coords:
                lat,lng = coords
                threat_type, icon = classify(original_text)
                return [{
                    'id': str(mid), 'place': base_cand.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': original_text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'late_parenthetical'
                }]

    # --- Settlement matching using external dataset (if provided) (single first match) ---
    if not region_hits:
        # 1) Multi-list form: "ÐÐ¾Ð²Ð³Ð¾Ñ€Ð¾Ð´-ÑÑ–Ð²ÐµÑ€ÑÑŒÐºÐ¸Ð¹, Ð¨Ð¾ÑÑ‚ÐºÐ°, ÐšÐ¾Ñ€Ð¾Ð¿, ÐšÑ€Ð¾Ð»ÐµÐ²ÐµÑ†ÑŒ - ÑƒÐ²Ð°Ð¶Ð½Ð¾ Ð¿Ð¾ Ð‘ÐŸÐ›Ð"
        # support both hyphen - and en dash â€“ between list and tail
        dash_idx = None
        for dch in [' - ', ' â€“ ', '- ', 'â€“ ']:
            if dch in lower:
                dash_idx = lower.index(dch)
                break
        if ('ÑƒÐ²Ð°Ð¶Ð½Ð¾' in lower or 'Ð¿Ð¾ Ð±Ð¿Ð»Ð°' in lower or 'Ð±Ð¿Ð»Ð°' in lower) and (',' in lower) and dash_idx is not None:
            left = lower[:dash_idx]
            right = lower[dash_idx+1:]
            if any(k in right for k in ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','uav']):
                raw_places = [p.strip() for p in left.split(',') if p.strip()]
                tracks = []
                threat_type, icon = classify(text)
                seen = set()
                for idx, rp in enumerate(raw_places,1):
                    key = rp.replace('Ð¹,','Ð¹').strip()
                    coords = region_enhanced_coords(key)
                    if coords and key not in seen:
                        seen.add(key)
                        lat,lng = coords
                        tracks.append({
                            'id': f"{mid}_m{idx}", 'place': key.title(), 'lat': lat, 'lng': lng,
                            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'multi_settlement'
                        })
                if tracks:
                    return tracks
        # 2) Single settlement search (fallback) with word-boundary and specificity prioritization
        if SETTLEMENTS_INDEX:
            cand_hits = []
            text_len = len(lower)
            for name in SETTLEMENTS_ORDERED:
                start = 0
                while True:
                    idx = lower.find(name, start)
                    if idx == -1:
                        break
                    before_ok = (idx == 0) or not lower[idx-1].isalnum()
                    after_idx = idx + len(name)
                    after_ok = (after_idx == text_len) or not lower[after_idx].isalnum()
                    if before_ok and after_ok:
                        cand_hits.append(name)
                        break  # only need first occurrence
                    start = idx + 1
            if cand_hits:
                # Prefer longer names; deprioritize generic oblast centers when more specific present
                def score(n: str):
                    base_penalty = -5 if n in ['ÑÑƒÐ¼Ð¸'] and len(cand_hits) > 1 else 0
                    return (len(n) + base_penalty)
                cand_hits.sort(key=score, reverse=True)
                chosen = cand_hits[0]
                lat, lng = SETTLEMENTS_INDEX[chosen]
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': chosen.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon,
                    'source_match': 'settlement'
                }]

    # --- Raion (district) detection ---
    # Ð˜Ñ‰ÐµÐ¼ ÐºÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ Ð²Ð¸Ð´Ð° "ÐŸÐ¾ÐºÑ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½", Ð° Ñ‚Ð°ÐºÐ¶Ðµ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ "ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹ Ñ‚Ð° Ð¡ÑƒÐ¼ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½Ñ‹".
    def norm_raion(token: str):
        t = token.lower().strip('- ')
        # ÑƒÐ½Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð´ÐµÑ„Ð¸ÑÐ¾Ð²
        t = t.replace('â€“','-')
        # Ð¼Ð¾Ñ€Ñ„Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ñ -> Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ñ„Ð¾Ñ€Ð¼Ð° -ÑÑŒÐºÐ¸Ð¹
        t = re.sub(r'(ÑÑŒÐºÐ¾Ð¼Ñƒ|ÑÐºÐ¾Ð³Ð¾|ÑÑŒÐºÐ¾Ð³Ð¾|ÑÐºÐ¸Ð¹|ÑÑŒÐºiÐ¹|ÑÑŒÐºÐ¾Ð¹|ÑÑŒÐºÐ¸Ð¼|ÑÐºÐ¾Ð¼)$','ÑÑŒÐºÐ¸Ð¹', t)
        return t
    raion_matches = []
    # Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ðµ 'Ñ€Ð°Ð¹Ð¾Ð½Ð¸'
    plural_pattern = re.compile(r'([Ð-Ð¯A-ZÐ‡Ð†Ð„ÒÐÐ°-Ña-zÑ—Ñ–Ñ”Ò‘Ñ‘,\-\s]{4,}?)Ñ€Ð°Ð¹Ð¾Ð½Ð¸', re.IGNORECASE)
    for pm in plural_pattern.finditer(text):
        segment = pm.group(1)
        # Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾ 'Ñ‚Ð°' Ð¸Ð»Ð¸ Ð·Ð°Ð¿ÑÑ‚Ñ‹Ð¼
        parts = re.split(r'\s+Ñ‚Ð°\s+|,', segment)
        for p in parts:
            cand = p.strip()
            if not cand:
                continue
            # Ð±ÐµÑ€Ñ‘Ð¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ»Ð¾Ð²Ð¾ (ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹)
            last = cand.split()[-1]
            base = norm_raion(last)
            if base in RAION_FALLBACK:
                raion_matches.append((base, RAION_FALLBACK[base]))
    # Ð¾Ð´Ð¸Ð½Ð¾Ñ‡Ð½Ð¾Ðµ 'Ñ€Ð°Ð¹Ð¾Ð½' (Ð»ÑŽÐ±Ð¾Ð¹ Ð¿Ð°Ð´ÐµÐ¶: Ñ€Ð°Ð¹Ð¾Ð½, Ñ€Ð°Ð¹Ð¾Ð½Ñƒ, Ñ€Ð°Ð¹Ð¾Ð½Ð¾Ð¼, Ñ€Ð°Ð¹Ð¾Ð½Ð°)
    raion_pattern = re.compile(r'([Ð-Ð¯A-ZÐ‡Ð†Ð„ÒÐÐ°-Ña-zÑ—Ñ–Ñ”Ò‘Ñ‘\-]{4,})\s+Ñ€Ð°Ð¹Ð¾Ð½(?:Ñƒ|Ð¾Ð¼|Ð°)?', re.IGNORECASE)
    for m_r in raion_pattern.finditer(text):
        base = norm_raion(m_r.group(1))
        if base in RAION_FALLBACK:
            raion_matches.append((base, RAION_FALLBACK[base]))
    # ÐÐ±Ð±Ñ€ÐµÐ²Ð¸Ð°Ñ‚ÑƒÑ€Ð° "Ñ€-Ð½" (Ñƒ Ñ‚.Ñ‡. Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ "Ñ€-Ð½.", "Ñ€Ð½", "Ñ€-Ð½," )
    raion_abbrev_pattern = re.compile(r'([Ð-Ð¯A-ZÐ‡Ð†Ð„ÒÐÐ°-Ña-zÑ—Ñ–Ñ”Ò‘Ñ‘\-]{4,})\s+Ñ€\s*[-â€“]?\s*Ð½\.?', re.IGNORECASE)
    for m_ra in raion_abbrev_pattern.finditer(text):
        base = norm_raion(m_ra.group(1))
        if base in RAION_FALLBACK:
            raion_matches.append((base, RAION_FALLBACK[base]))
    if raion_matches:
        threat_type, icon = classify(text)
        tracks = []
        seen = set()
        for idx,(name,(lat,lng)) in enumerate(raion_matches,1):
            # For some districts, show the main city name instead of district name
            district_to_city_mapping = {
                'Ð¿Ð°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ¸Ð¹': 'ÐŸÐ°Ð²Ð»Ð¾Ð³Ñ€Ð°Ð´',
                'Ð±Ñ–Ð»Ð¾Ñ†ÐµÑ€ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': 'Ð‘Ñ–Ð»Ð° Ð¦ÐµÑ€ÐºÐ²Ð°',
                'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÑ†ÑŒÐºÐ¸Ð¹': 'ÐšÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº',
                'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ¸Ð¹': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²',
                'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾'
            }
            
            if name.lower() in district_to_city_mapping:
                title = district_to_city_mapping[name.lower()]
            else:
                title = f"{name.title()} Ñ€Ð°Ð¹Ð¾Ð½"
            if title in seen: continue
            seen.add(title)
            # Maintain alarm overlay state
            if threat_type == 'alarm':
                RAION_ALARMS[name] = {'place': title, 'lat': lat, 'lng': lng, 'since': time.time()}
            elif threat_type == 'alarm_cancel':
                RAION_ALARMS.pop(name, None)
            tracks.append({
                'id': f"{mid}_d{idx}", 'place': title, 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'source_match': 'raion'
            })
        if tracks:
            log.debug(f"RAION_MATCH mid={mid} -> {[t['place'] for t in tracks]}")
            return tracks

    # --- Hromada detection (e.g., "Ð¥Ð¾Ñ‚Ñ–Ð½ÑÑŒÐºÐ° Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ð°Ð»ÑŒÐ½Ð° Ð³Ñ€Ð¾Ð¼Ð°Ð´Ð°") ---
    hromada_pattern = re.compile(r'([Ð-Ð¯A-ZÐ‡Ð†Ð„ÒÐÐ°-Ña-zÑ—Ñ–Ñ”Ò‘Ñ‘\-]{4,})\s+Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ð°Ð»(?:ÑŒÐ½Ð°|ÑŒÐ½Ð¾Ñ—)?\s+Ð³Ñ€Ð¾Ð¼Ð°Ð´Ð°', re.IGNORECASE)
    hromada_matches = []
    for m_h in hromada_pattern.finditer(text):
        token = m_h.group(1).lower()
        # normalize adjective endings to 'ÑÑŒÐºÐ°'
        base = re.sub(r'(ÑÑŒÐºÐ¾Ñ—|ÑÐºÐ¾Ð¹|ÑÑŒÐºÐ°|ÑÐºÐ¾Ð¹)$', 'ÑÑŒÐºÐ°', token)
        if base in HROMADA_FALLBACK:
            hromada_matches.append((base, HROMADA_FALLBACK[base]))
    if hromada_matches:
        threat_type, icon = classify(text)
        tracks = []
        seen = set()
        for idx,(name,(lat,lng)) in enumerate(hromada_matches,1):
            title = f"{name.title()} Ñ‚ÐµÑ€Ð¸Ñ‚Ð¾Ñ€Ñ–Ð°Ð»ÑŒÐ½Ð° Ð³Ñ€Ð¾Ð¼Ð°Ð´Ð°"
            if title in seen: continue
            seen.add(title)
            tracks.append({
                'id': f"{mid}_h{idx}", 'place': title, 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'source_match': 'hromada'
            })
        if tracks:
            return tracks

    # --- Slash separated settlements PRIORITY (moved earlier so it can't be overridden by region logic) ---
    lower_full_for_slash = text.lower()
    if '/' in lower_full_for_slash and ('Ð±Ð¿Ð»Ð°' in lower_full_for_slash or 'Ð´Ñ€Ð¾Ð½' in lower_full_for_slash) and any(x in lower_full_for_slash for x in ['Ñ… Ð±Ð¿Ð»Ð°','x Ð±Ð¿Ð»Ð°',' Ð±Ð¿Ð»Ð°']):
        # take portion before first dash (â€” or -) which usually separates counts/other text
        left_part = re.split(r'[â€”-]', lower_full_for_slash, 1)[0]
        # Remove trailing count token like "5Ñ… Ð±Ð¿Ð»Ð°" from left part to isolate pure settlements
        left_part = re.sub(r'\b\d+[xÑ…]\s*Ð±Ð¿Ð»Ð°.*$', '', left_part).strip()
        parts = [p.strip() for p in re.split(r'/|\\', left_part) if p.strip()]
        found = []
        # Derive a region stem from any well-known city token to bias geocoding of other parts
        inferred_region = None
        for p in parts:
            base_inf = UA_CITY_NORMALIZE.get(p, p)
            if base_inf in CITY_TO_OBLAST:
                inferred_region = CITY_TO_OBLAST[base_inf]
                break
        for p in parts:
            base = UA_CITY_NORMALIZE.get(p, p)
            coords = CITY_COORDS.get(base)
            if not coords and SETTLEMENTS_INDEX:
                coords = SETTLEMENTS_INDEX.get(base)
            if not coords:
                # If we have inferred region stem, attempt region-qualified geocode first
                if inferred_region and OPENCAGE_API_KEY:
                    oblast_variants = [
                        f"{base} {inferred_region}Ñ‰Ð¸Ð½Ð°",
                        f"{base} {inferred_region}ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ",
                        f"{base} {inferred_region}ÑÐºÐ°Ñ Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ"
                    ]
                    for q in oblast_variants:
                        try:
                            coords = geocode_opencage(q)
                            if coords:
                                break
                        except Exception:
                            pass
                if not coords:
                    try:
                        coords = region_enhanced_coords(base)
                    except Exception:
                        coords = None
            if coords:
                found.append((base.title(), coords))
        if found:
            threat_type, icon = classify(text)
            tracks = []
            for idx,(nm,(lat,lng)) in enumerate(found,1):
                if 'ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´' in lower_full_for_slash:
                    lng -= 0.4
                tracks.append({
                    'id': f"{mid}_s{idx}", 'place': nm, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'slash_combo', 'count': drone_count
                })
            if tracks:
                try:
                    log.debug(f"SLASH_COMBO mid={mid} parts={parts} tracks={[(t['place'], t['lat'], t['lng']) for t in tracks]}")
                except Exception:
                    pass
            if tracks:
                return tracks

    # --- Black Sea aquatory: place marker in sea, not on target city (e.g. "Ð² Ð°ÐºÐ²Ð°Ñ‚Ð¾Ñ€Ñ–Ñ— Ñ‡Ð¾Ñ€Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ñ€Ñ, ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð¾Ð´ÐµÑÑƒ") ---
    lower_sea = text.lower()
    if ('Ð°ÐºÐ²Ð°Ñ‚Ð¾Ñ€' in lower_sea or 'Ð°ÐºÐ²Ð°Ñ‚Ð¾Ñ€Ñ–Ñ—' in lower_sea) and ('Ñ‡Ð¾Ñ€Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ñ€Ñ' in lower_sea or 'Ñ‡Ð¾Ñ€Ð½Ðµ Ð¼Ð¾Ñ€Ðµ' in lower_sea) and ('Ð±Ð¿Ð»Ð°' in lower_sea or 'Ð´Ñ€Ð¾Ð½' in lower_sea):
        # Attempt to capture target city (optional)
        m_target = re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})', lower_sea)
        target_city = None
        if m_target:
            tc = m_target.group(1).lower()
            tc = UA_CITY_NORMALIZE.get(tc, tc)
            target_city = tc.title()
        threat_type, icon = classify(text)
        # Approx northern Black Sea central coords (between Odesa & Crimea offshore)
        sea_lat, sea_lng = 45.3, 30.7
        place_label = 'ÐÐºÐ²Ð°Ñ‚Ð¾Ñ€Ñ–Ñ Ð§Ð¾Ñ€Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ñ€Ñ'
        if target_city:
            place_label += f' (ÐºÑƒÑ€Ñ Ð½Ð° {target_city})'
        return [{
            'id': str(mid), 'place': place_label, 'lat': sea_lat, 'lng': sea_lng,
            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': icon, 'source_match': 'black_sea_course'
        }]

    # --- Bilhorod-Dnistrovskyi coastal UAV patrol ("Ð²Ð·Ð´Ð¾Ð²Ð¶ ÑƒÐ·Ð±ÐµÑ€ÐµÐ¶Ð¶Ñ Ð‘Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð”Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾ Ñ€Ð°Ð¹Ð¾Ð½Ñƒ") ---
    if (('ÑƒÐ·Ð±ÐµÑ€ÐµÐ¶' in lower_sea or 'Ð²Ð·Ð´Ð¾Ð²Ð¶ ÑƒÐ·Ð±ÐµÑ€ÐµÐ¶' in lower_sea) and
        ('Ð±Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾' in lower_sea or 'Ð±ÐµÐ»Ð³Ð¾Ñ€Ð¾Ð´-Ð´Ð½ÐµÑÑ‚Ñ€Ð¾Ð²ÑÐºÐ¾Ð³Ð¾' in lower_sea) and
        ('Ð±Ð¿Ð»Ð°' in lower_sea or 'Ð´Ñ€Ð¾Ð½' in lower_sea)):
        # Base approximate city coordinate; push 0.22Â° south into sea
        city_lat, city_lng = 46.186, 30.345
        lat = city_lat - 0.22
        lng = city_lng
        threat_type, icon = classify(text)
        return [{
            'id': str(mid), 'place': 'Ð£Ð·Ð±ÐµÑ€ÐµÐ¶Ð¶Ñ Ð‘Ñ–Ð»Ð³Ð¾Ñ€Ð¾Ð´-Ð”Ð½Ñ–ÑÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾ Ñ€-Ð½Ñƒ', 'lat': lat, 'lng': lng,
            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
            'marker_icon': icon, 'source_match': 'bilhorod_dnistrovskyi_coast'
        }]

    # --- "Ð¿Ð¾Ð²Ð· <city>" (passing near) with optional direction target "Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ <city>" ---
    lower_pass = text.lower()
    pass_near_detected = False
    if 'Ð¿Ð¾Ð²Ð· ' in lower_pass and ('Ð±Ð¿Ð»Ð°' in lower_pass or 'Ð´Ñ€Ð¾Ð½' in lower_pass):
        pass_match = re.search(r"Ð¿Ð¾Ð²Ð·\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'â€™Ê¼`\-]{3,})", lower_pass)
        dir_match = re.search(r"Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'â€™Ê¼`\-]{3,})(?:\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'â€™Ê¼`\-]{3,}))?", lower_pass)
        places = []
        def norm_c(s: str):
            if not s: return None
            s = s.strip().lower().strip(".,:;()!?")
            s = UA_CITY_NORMALIZE.get(s, s)
            # Morphological heuristics: convert common Ukrainian/Russian case endings to nominative
            candidates = [s]
            if s.endswith('Ñƒ') and len(s) > 4:
                candidates.append(s[:-1] + 'Ð°')
            if s.endswith('ÑŽ') and len(s) > 4:
                candidates.append(s[:-1] + 'Ñ')
            if s.endswith('Ð¸') and len(s) > 4:
                candidates.append(s[:-1] + 'Ð°')
            if s.endswith('Ð¾Ñ—') and len(s) > 5:
                candidates.append(s[:-2] + 'Ð°')
            if s.endswith('Ð¾Ñ—Ñ—') and len(s) > 6:
                candidates.append(s[:-3] + 'Ð°')
            for cand in candidates:
                if region_enhanced_coords(cand):
                    return cand
            return s
        if pass_match:
            c1 = norm_c(pass_match.group(1))
            if c1:
                coords1 = region_enhanced_coords(c1)
                if coords1:
                    places.append((c1.title(), coords1, 'pass_near'))
        if dir_match:
            c2_first = norm_c(dir_match.group(1))
            c2_second_raw = dir_match.group(2)
            full_phrase = None
            if c2_first and c2_second_raw:
                c2_second = norm_c(c2_second_raw)
                cand_phrase = f"{c2_first} {c2_second}".strip()
                if cand_phrase in CITY_COORDS or (SETTLEMENTS_INDEX and cand_phrase in SETTLEMENTS_INDEX):
                    full_phrase = cand_phrase
            c2_key = full_phrase or c2_first
            if c2_key and c2_key != (places[0][0].lower() if places else None):
                coords2 = region_enhanced_coords(c2_key)
                if coords2:
                    places.append((c2_key.title(), coords2, 'direction_target'))
        if places:
            threat_type, icon = classify(text)
            out_tracks = []
            for idx,(nm,(lat,lng),tag) in enumerate(places,1):
                out_tracks.append({
                    'id': f"{mid}_pv{idx}", 'place': nm, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str,
                    'channel': channel, 'marker_icon': icon, 'source_match': tag, 'count': drone_count
                })
            if out_tracks:
                pass_near_detected = True
                return out_tracks

    # --- Pattern: "Ñ€ÑƒÑ…Ð°Ð»Ð¸ÑÑ Ð½Ð° <city1>, Ð·Ð¼Ñ–Ð½Ð¸Ð»Ð¸ ÐºÑƒÑ€Ñ Ð½Ð° <city2>" ---
    lower_course_change = text.lower()
    if 'Ð·Ð¼Ñ–Ð½Ð¸Ð»Ð¸ ÐºÑƒÑ€Ñ Ð½Ð°' in lower_course_change and ('Ñ€ÑƒÑ…Ð°Ð»' in lower_course_change or 'Ñ€ÑƒÑ…Ð°Ð»Ð¸ÑÑŒ' in lower_course_change or 'Ñ€ÑƒÑ…Ð°Ð»Ð¸ÑÑ' in lower_course_change):
        m_to = re.search(r'Ð·Ð¼Ñ–Ð½Ð¸Ð»Ð¸\s+ÐºÑƒÑ€Ñ\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})', lower_course_change)
        m_from = re.search(r'Ñ€ÑƒÑ…Ð°Ð»(?:Ð¸ÑÑ|Ð¸ÑÑŒ|Ð¸|Ð°ÑÑŒ)?\s+Ð½Ð°\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})', lower_course_change)
        places = []
        def norm_simple(s):
            if not s: return None
            s = s.strip().lower().strip(".,:;()!")
            return UA_CITY_NORMALIZE.get(s, s)
        if m_from:
            c_from = norm_simple(m_from.group(1))
            coords_from = region_enhanced_coords(c_from)
            if coords_from:
                places.append((c_from.title(), coords_from, 'course_from'))
        if m_to:
            c_to = norm_simple(m_to.group(1))
            coords_to = region_enhanced_coords(c_to)
            if coords_to:
                # avoid duplicate if same
                if not any(p[0].lower()==c_to for p in places):
                    places.append((c_to.title(), coords_to, 'course_changed_to'))
        if places:
            threat_type, icon = classify(text)
            out = []
            for idx,(name,(lat,lng),tag) in enumerate(places,1):
                out.append({
                    'id': f"{mid}_cc{idx}", 'place': name, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': tag
                })
            if out:
                return out

    # --- Relative direction near a city: "Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ ÐºÐ°Ð¼'ÑÐ½ÑÑŒÐºÐ¾Ð³Ð¾ Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐºÐ°" ---
    rel_dir_lower = text.lower()
    if any(k in rel_dir_lower for k in ['Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ','Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ñ–ÑˆÐµ','ÑÑ…Ñ–Ð´Ð½Ñ–ÑˆÐµ','Ð·Ð°Ñ…Ñ–Ð´Ð½Ñ–ÑˆÐµ']) and ('Ð±Ð¿Ð»Ð°' in rel_dir_lower or 'Ð´Ñ€Ð¾Ð½' in rel_dir_lower):
        # Allow letters plus apostrophes/hyphen
        m_rel = re.search(r"(Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ñ–ÑˆÐµ|Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ñ–ÑˆÐµ|ÑÑ…Ñ–Ð´Ð½Ñ–ÑˆÐµ|Ð·Ð°Ñ…Ñ–Ð´Ð½Ñ–ÑˆÐµ)\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘'`â€™Ê¼\-]{4,})", rel_dir_lower)
        target_dir = re.search(r'Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})', rel_dir_lower)
        if m_rel:
            dir_word = m_rel.group(1)
            raw_city = m_rel.group(2).strip(".,:;()!?")
            def norm_rel_city(s):
                s = s.lower()
                # Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾ÐºÐ¾Ð½Ñ‡Ð°Ð½Ð¸Ð¹ Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð°Ð´ÐµÐ¶Ð° '-ÑÑŒÐºÐ¾Ð³Ð¾' -> 'ÑÑŒÐºÐµ'
                if s.endswith('ÑÑŒÐºÐ¾Ð³Ð¾'):
                    s = s[:-6] + 'ÑÑŒÐºÐµ'
                if s.endswith('Ð¾Ð³Ð¾') and len(s) > 5:
                    s = s[:-3] + 'Ð¾'
                return UA_CITY_NORMALIZE.get(s, s)
            base_city = norm_rel_city(raw_city)
            coords_base = region_enhanced_coords(base_city)
            coords_target = None
            target_name = None
            if target_dir:
                tn = target_dir.group(1).lower().strip('.:,;()!?')
                tn = UA_CITY_NORMALIZE.get(tn, tn)
                coords_target = region_enhanced_coords(tn)
                target_name = tn
            if coords_base:
                lat_b, lng_b = coords_base
                # offset ~0.35 deg lat/long depending on direction
                lat_off, lng_off = 0,0
                if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡' in dir_word: lat_off = 0.35
                elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½' in dir_word: lat_off = -0.35
                elif 'ÑÑ…Ñ–Ð´Ð½' in dir_word: lng_off = 0.55
                elif 'Ð·Ð°Ñ…Ñ–Ð´Ð½' in dir_word: lng_off = -0.55
                rel_lat, rel_lng = lat_b + lat_off, lng_b + lng_off
                threat_type, icon = classify(text)
                tracks = [{
                    'id': f"{mid}_rel1", 'place': base_city.title(), 'lat': rel_lat, 'lng': rel_lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'relative_dir'
                }]
                if coords_target:
                    tracks.append({
                        'id': f"{mid}_rel2", 'place': target_name.title(), 'lat': coords_target[0], 'lng': coords_target[1],
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'direction_target'
                    })
                return tracks

    # --- Parenthetical course city e.g. "ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹ (ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº)" ---
    if 'ÐºÑƒÑ€Ñ' in lower and '(' in lower and ')' in lower and ('Ð±Ð¿Ð»Ð°' in lower or 'Ð´Ñ€Ð¾Ð½' in lower):
        m_par = re.search(r'ÐºÑƒÑ€Ñ[^()]{0,30}\(([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})\)', lower)
        if m_par:
            pc = m_par.group(1).lower()
            pc = UA_CITY_NORMALIZE.get(pc, pc)
            coords = region_enhanced_coords(pc)
            if coords:
                threat_type, icon = classify(text)
                return [{
                    'id': f"{mid}_pc", 'place': pc.title(), 'lat': coords[0], 'lng': coords[1],
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'course_parenthetical'
                }]

    # --- Comma separated settlements followed by threat keyword (e.g. "ÐžÐ±ÑƒÑ…Ñ–Ð²ÐºÐ°, ÐšÑƒÑ€Ð¸Ð»Ñ–Ð²ÐºÐ°, ÐŸÐµÑ‚Ñ€Ð¸ÐºÑ–Ð²ÐºÐ° ÑƒÐ²Ð°Ð³Ð° Ð‘ÐŸÐ›Ð") ---
    lower_commas = text.lower()
    if 'Ð±Ð¿Ð»Ð°' in lower_commas and ',' in lower_commas:
        # Identify first threat keyword position
        threat_kw_idx = None
        for kw in ['ÑƒÐ²Ð°Ð³Ð°','Ð¿Ñ€Ð¾Ð»Ñ–Ñ‚','Ð¿Ñ€Ð¾Ð»Ñ‘Ñ‚','ÑƒÐ²Ð°Ð¶Ð½Ð¾','ÑƒÐ²Ð°Ð¶Ð½Ð¾.','ÑƒÐ²Ð°Ð¶Ð½Ð¾,']:
            pos = lower_commas.find(kw)
            if pos != -1:
                threat_kw_idx = pos
                break
        if threat_kw_idx is not None:
            left_seg = lower_commas[:threat_kw_idx]
            # quick guard to ensure segment not too long
            if 3 <= len(left_seg) <= 180:
                cand_parts = [p.strip() for p in left_seg.split(',') if p.strip()]
                found = []
                for cand in cand_parts:
                    # normalize basic endings (remove trailing punctuation)
                    base = cand.strip(" .!?:;()[]'`â€™Ê¼")
                    if len(base) < 3:
                        continue
                    norm = UA_CITY_NORMALIZE.get(base, base)
                    coords = region_enhanced_coords(norm)
                    if coords:
                        found.append((norm.title(), coords))
                if found:
                    threat_type, icon = classify(text)
                    tracks = []
                    seenp = set()
                    for idx,(nm,(lat,lng)) in enumerate(found,1):
                        if nm in seenp: continue
                        seenp.add(nm)
                        tracks.append({
                            'id': f"{mid}_m{idx}", 'place': nm, 'lat': lat, 'lng': lng,
                            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'multi_settlement_comma', 'count': drone_count
                        })
                    if tracks:
                        return tracks

    # --- PRIORITY: Direction patterns (Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ, Ñ‡ÐµÑ€ÐµÐ·, Ð¿Ð¾Ð²Ð·) - BEFORE region boundary logic ---
    try:
        import re as _re_direction
        
        if has_threat(text) and any(pattern in text.lower() for pattern in ['Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', 'Ñ‡ÐµÑ€ÐµÐ·', 'Ð¿Ð¾Ð²Ð·']):
            direction_targets = []
            
            # Pattern 1: "Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ [city], [oblast]"
            naprym_pattern = r'Ñƒ\s+Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ\s+([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]+?)(?:\s*,\s*([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]*Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–?))?(?:[\.\,\!\?;]|$)'
            naprym_matches = _re_direction.findall(naprym_pattern, text, _re_direction.IGNORECASE)
            for city_raw, oblast_raw in naprym_matches:
                direction_targets.append(('Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ', city_raw.strip(), oblast_raw.strip() if oblast_raw else ''))
            
            # Process direction targets
            for direction_type, city_raw, oblast_raw in direction_targets:
                if direction_type == 'Ñƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÐºÑƒ':
                    city_norm = city_raw.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                    city_norm = re.sub(r'\s+',' ', city_norm).strip()
                    
                    # Try exact lookup
                    coords = CITY_COORDS.get(city_norm)
                    if not coords:
                        # Try normalized lookup
                        city_base = UA_CITY_NORMALIZE.get(city_norm, city_norm)
                        coords = CITY_COORDS.get(city_base)
                    
                    if coords:
                        lat, lng = coords
                        threat_type, icon = classify(text)
                        
                        # Extract drone count
                        import re as _re_count
                        count_match = _re_count.search(r'(\d+)\s*[Ñ…x]?\s*(?:Ð±Ð¿Ð»Ð°|Ð´Ñ€Ð¾Ð½|ÑˆÐ°Ñ…ÐµÐ´)', text.lower())
                        drone_count = int(count_match.group(1)) if count_match else 1
                        
                        add_debug_log(f"PRIORITY: Direction target found - {city_norm} -> {coords}", "direction_priority")
                        return [{
                            'id': str(mid), 'place': city_raw.title(), 'lat': lat, 'lng': lng,
                            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'direction_target_priority', 'count': drone_count
                        }]
                    else:
                        add_debug_log(f"PRIORITY: Direction target not found - {city_norm}", "direction_priority")
    except Exception as e:
        add_debug_log(f"Direction priority processing error: {e}", "direction_priority")

    # PRIORITY: Various Shahed patterns - Process before region boundary logic
    try:
        import re as _re_shahed
        all_shahed_tracks = []
        
        # Pattern 1: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð±Ñ–Ð»Ñ [city]" or "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð±Ñ–Ð»Ñ [city1]/[city2]"
        bilya_pattern = r'(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð±Ñ–Ð»Ñ\s+([Ð-Ð¯Ð°-ÑÐÑ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s\/]+?)(?:\s+Ñ‚Ð°\s+Ñ€Ð°Ð¹Ð¾Ð½)?(?:\s+Ð½Ð°\s+[Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]+)?(?:[\.\,\!\?;]|$)'
        bilya_matches = _re_shahed.findall(bilya_pattern, text, _re_shahed.IGNORECASE)
        
        # Pattern 2: "N ÑˆÐ°Ñ…ÐµÐ´ Ð½Ð° [city]"
        na_pattern = r'(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð½Ð°\s+([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]+?)(?:[\.\,\!\?;]|$)'
        na_matches = _re_shahed.findall(na_pattern, text, _re_shahed.IGNORECASE)
        
        # Pattern 3: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ð· Ð±Ð¾ÐºÑƒ [city]"
        z_boku_pattern = r'(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ð·\s+Ð±Ð¾ÐºÑƒ\s+([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]+?)(?:[\.\,\!\?;]|$)'
        z_boku_matches = _re_shahed.findall(z_boku_pattern, text, _re_shahed.IGNORECASE)
        
        # Pattern 4: "N ÑˆÐ°Ñ…ÐµÐ´Ñ–Ð² Ñ‡ÐµÑ€ÐµÐ· [city1]/[city2]" - multiple cities
        cherez_multi_pattern = r'(\d+)\s+ÑˆÐ°Ñ…ÐµÐ´[Ð°-ÑÑ–Ñ—Ñ”Ñ‘Ñ‹Ð¸Ð¹Ñ—]*\s+Ñ‡ÐµÑ€ÐµÐ·\s+([Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s\/]+?)(?:\s+Ñ€Ð°Ð¹Ð¾Ð½)?(?:\s+Ð½Ð°\s+[Ð-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\'\-\s]+)?(?:[\.\,\!\?;]|$)'
        cherez_matches = _re_shahed.findall(cherez_multi_pattern, text, _re_shahed.IGNORECASE)
        
        all_patterns = [
            (bilya_matches, 'bilya'),
            (na_matches, 'na'), 
            (z_boku_matches, 'z_boku'),
            (cherez_matches, 'cherez')
        ]
        
        for matches, pattern_type in all_patterns:
            for count_str, city_raw in matches:
                # Handle multiple cities separated by /
                cities = [c.strip() for c in city_raw.split('/')]
                
                for city_part in cities:
                    city_norm = city_part.lower().replace('\u02bc',"'").replace('Ê¼',"'").replace("'","'").replace('`',"'")
                    city_norm = re.sub(r'\s+',' ', city_norm).strip()
                    
                    # Special handling for "[city] Ð½Ð° [region]" patterns
                    region_match = re.match(r'^(.+?)\s+Ð½Ð°\s+([Ð°-ÑÑ–Ñ—Ñ”]+Ñ‰Ð¸Ð½Ñ–?|[Ð°-ÑÑ–Ñ—Ñ”]+ÑÑŒÐºÑ–Ð¹?\s+Ð¾Ð±Ð»?\.?|[Ð°-ÑÑ–Ñ—Ñ”]+ÑÑŒÐºÑ–Ð¹?\s+Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–?)$', city_norm)
                    if region_match:
                        city_norm = region_match.group(1).strip()
                        region_hint = region_match.group(2).strip()
                        # Use full message context for resolution
                        coords = ensure_city_coords_with_message_context(city_norm, text)
                        if coords:
                            lat, lng, approx = coords
                            add_debug_log(f"SHAHED: Regional pattern found - {city_norm} Ð½Ð° {region_hint} -> ({lat}, {lng})", "shahed_regional")
                            
                            result_entry = {
                                'id': f"{mid}_sha_{len(threats)+1}",
                                'place': f"{city_part.title()}",
                                'lat': lat, 'lng': lng,
                                'type': 'shahed', 'count': int(count_str),
                                'timestamp': date_str, 'channel': channel
                            }
                            threats.append(result_entry)
                            continue  # Skip regular processing for this city
                    
                    # Apply normalization rules for accusative/genitive cases
                    original_norm = city_norm
                    if city_norm in UA_CITY_NORMALIZE:
                        city_norm = UA_CITY_NORMALIZE[city_norm]
                    
                    # Try accusative endings for cities like "Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ”Ð²Ð°" -> "Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð²", "Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð¸" -> "Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð°"
                    if not (city_norm in CITY_COORDS or region_enhanced_coords(city_norm)):
                        # Try various ending transformations
                        variants = [city_norm]
                        if city_norm.endswith('Ð°'):
                            variants.extend([city_norm[:-1] + 'Ñ–Ð²', city_norm[:-1] + 'Ñ–'])
                        elif city_norm.endswith('Ð¸'):
                            variants.extend([city_norm[:-1] + 'Ð°', city_norm[:-1] + 'Ñ'])
                        elif city_norm.endswith('Ñƒ'):
                            variants.extend([city_norm[:-1] + 'Ð¿', city_norm[:-1] + 'Ðº'])
                        
                        for variant in variants:
                            if variant in CITY_COORDS or region_enhanced_coords(variant):
                                city_norm = variant
                                break
                    
                    # Try to get coordinates
                    coords = region_enhanced_coords(city_norm)
                    if not coords:
                        context_result = ensure_city_coords_with_message_context(city_norm, text)
                        if context_result:
                            coords = context_result[:2]  # Take only lat, lng
                    
                    if coords:
                        lat, lng = coords
                        threat_type, icon = classify(text)
                        count = int(count_str) if count_str.isdigit() else 1
                        
                        # Create multiple tracks for multiple drones
                        tracks_to_create = max(1, count)
                        for i in range(tracks_to_create):
                            track_label = city_part.title()
                            if tracks_to_create > 1:
                                track_label += f" #{i+1}"
                            
                            # Add small coordinate offsets to prevent marker overlap
                            marker_lat = lat
                            marker_lng = lng
                            if tracks_to_create > 1:
                                # Create a circular pattern around the target
                                import math
                                angle = (2 * math.pi * i) / tracks_to_create
                                offset_distance = 0.01  # ~1km offset
                                marker_lat += offset_distance * math.cos(angle)
                                marker_lng += offset_distance * math.sin(angle)
                                
                            all_shahed_tracks.append({
                                'id': f"{mid}_{pattern_type}_{len(all_shahed_tracks)}", 
                                'place': track_label, 
                                'lat': marker_lat, 
                                'lng': marker_lng,
                                'threat_type': threat_type, 
                                'text': text[:500], 
                                'date': date_str, 
                                'channel': channel,
                                'marker_icon': icon, 
                                'source_match': f'{pattern_type}_shahed_priority', 
                                'count': 1
                            })
                        add_debug_log(f"SHAHED {pattern_type.upper()}: {city_norm} ({count}x) -> {coords}", f"shahed_{pattern_type}")
        
        if all_shahed_tracks:
            return all_shahed_tracks
    except Exception as e:
        add_debug_log(f"Shahed patterns processing error: {e}", "shahed_priority")

    # Region boundary logic (fallback single or midpoint for exactly two)
    matched_regions = []
    for name, coords in OBLAST_CENTERS.items():
        if name in lower:
            matched_regions.append((name, coords))
    if matched_regions:
        # ÐÐžÐ’ÐžÐ•: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾Ð³Ð¾ Ð³ÐµÐ¾ÐºÐ¾Ð´Ð¸Ð½Ð³Ð° Ð¿ÐµÑ€ÐµÐ´ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¼Ð°Ñ€ÐºÐµÑ€Ð°Ð¼Ð¸
        if CONTEXT_GEOCODER_AVAILABLE:
            context_result = get_coordinates_context_aware(text)
            if context_result:
                lat, lng, target_city = context_result
                threat_type, icon = classify(text)
                
                print(f"DEBUG Context-aware geocoding: Found primary target '{target_city}' at ({lat}, {lng})")
                
                return [{
                    'id': str(mid), 'place': target_city.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'context_aware_geocoding', 'count': 1
                }]
        
        # Ð•ÑÐ»Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ñ‹ Ð¸ Ð½ÐµÑ‚ ÐºÐ»ÑŽÑ‡ÐµÐ¹ ÑƒÐ³Ñ€Ð¾Ð·, Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼.
        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ð°: Ð¸Ð½Ð¾Ð³Ð´Ð° Ð² messages.json Ð¼Ð¾Ð³Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒÑÑ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð±ÐµÐ· ÑƒÐ³Ñ€Ð¾Ð·.
        if not has_threat(text):
            # Ñ‡Ð¸ÑÑ‚Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¾Ð±Ð»Ð°ÑÑ‚ÐµÐ¹? (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ + Ð´Ð²Ð¾ÐµÑ‚Ð¾Ñ‡Ð¸Ñ/Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹/Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ ÑÑ‚Ñ€Ð¾Ðº)
            stripped = re.sub(r'[\s:]+', ' ', text.lower()).strip()
            only_regions = all(rn in OBLAST_CENTERS for rn in stripped.split() if rn)
            if only_regions or len(text) < 120:
                return None
        # --- ÐÐ°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸ (Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹ / Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹ Ð¸ Ñ‚.Ð¿.) ---
        def detect_direction(lower_txt: str):
            # Support full adjectives with endings (-Ð½Ð¸Ð¹ / -Ð½Ñ / -Ð½ÑŒÐ¾Ð¼Ñƒ) by searching stems
            if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´' in lower_txt or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in lower_txt: return 'nw'
            if 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´' in lower_txt or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in lower_txt: return 'sw'
            if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´' in lower_txt or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in lower_txt: return 'ne'
            if 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´' in lower_txt or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in lower_txt: return 'se'
            # Single directions (allow stems 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½', 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½')
            if re.search(r'\bÐ¿Ñ–Ð²Ð½Ñ–Ñ‡(?!Ð¾-Ñ)(?:Ð½\w*)?\b', lower_txt): return 'n'
            if re.search(r'\bÐ¿Ñ–Ð²Ð´ÐµÐ½Ð½?\w*\b', lower_txt): return 's'
            if re.search(r'\bÑÑ…Ñ–Ð´Ð½?\w*\b', lower_txt): return 'e'
            if re.search(r'\bÐ·Ð°Ñ…Ñ–Ð´Ð½?\w*\b', lower_txt): return 'w'
            return None
        direction_code = None
    if len(matched_regions) == 1 and not raion_matches and not pass_near_detected:
            direction_code = detect_direction(lower)
            # If message also contains course info referencing cities/slash â€“ skip region-level marker to allow city parsing later
            course_words = (' ÐºÑƒÑ€Ñ ' in lower or lower.startswith('ÐºÑƒÑ€Ñ '))
            # Treat city present only if it appears as a standalone word (to avoid 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾' inside 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð¸')
            has_city_token = False
            try:
                import re as _re_ct
                for c_name in CITY_COORDS.keys():
                    if _re_ct.search(r'\b'+_re_ct.escape(c_name)+r'\b', lower):
                        has_city_token = True; break
            except Exception:
                has_city_token = any(c in lower for c in CITY_COORDS.keys())
            has_slash_combo = '/' in lower
            if direction_code and not (course_words and (has_city_token or has_slash_combo)):
                # ---- Special: sector course pattern inside region directional message ----
                # e.g. "ÐºÑƒÑ€Ñ(Ð¾Ð¼) Ð² Ð±Ñ–Ðº ÑÐµÐºÑ‚Ð¾Ñ€Ñƒ Ð¿ÐµÑ€ÐµÑ‰ÐµÐ¿Ð¸Ð½Ðµ - Ð³ÑƒÐ±Ð¸Ð½Ð¸Ñ…Ð°"
                sector_match = re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+Ð²\s+Ð±Ñ–Ðº\s+ÑÐµÐºÑ‚Ð¾Ñ€Ñƒ\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})(?:\s*[-â€“]\s*([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,}))?', lower)
                if sector_match:
                    c1 = sector_match.group(1)
                    c2 = sector_match.group(2)
                    def norm_city(n):
                        if not n: return None
                        n = n.strip().lower()
                        n = re.sub(r'["`Ê¼â€™\'.,:;()]+', '', n)
                        return UA_CITY_NORMALIZE.get(n, n)
                    c1n = norm_city(c1)
                    c2n = norm_city(c2) if c2 else None
                    coords1 = CITY_COORDS.get(c1n) or (SETTLEMENTS_INDEX.get(c1n) if SETTLEMENTS_INDEX else None)
                    coords2 = CITY_COORDS.get(c2n) or (SETTLEMENTS_INDEX.get(c2n) if (c2n and SETTLEMENTS_INDEX) else None)
                    if coords1 or coords2:
                        if coords1 and coords2:
                            lat_o = (coords1[0]+coords2[0])/2
                            lng_o = (coords1[1]+coords2[1])/2
                            place_label = f"{c1n.title()} - {c2n.title()} (ÑÐµÐºÑ‚Ð¾Ñ€)"
                        else:
                            (lat_o,lng_o) = coords1 or coords2
                            place_label = (c1n or c2n).title()
                        threat_type, icon = classify(text)
                        return [{
                            'id': str(mid), 'place': place_label, 'lat': lat_o, 'lng': lng_o,
                            'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                            'marker_icon': icon, 'source_match': 'course_sector', 'count': drone_count
                        }]
                (reg_name, (base_lat, base_lng)) = matched_regions[0]
                
                # Define offset function for coordinate calculations
                def offset(lat, lng, code):
                    # Ð£Ð¼ÐµÐ½ÑŒÑˆÐµÐ½Ð½Ñ‹Ðµ Ð´ÐµÐ»ÑŒÑ‚Ñ‹ Ð´Ð»Ñ Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð² Ð¿Ñ€ÐµÐ´ÐµÐ»Ð°Ñ… Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸
                    # (ÑˆÐ¸Ñ€Ð¾Ñ‚Ð° ~111 ÐºÐ¼, Ð´Ð¾Ð»Ð³Ð¾Ñ‚Ð° * cos(lat))
                    import math
                    lat_step = 0.35  # ÐŸÑ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ 35-40 ÐºÐ¼ Ð²Ð¼ÐµÑÑ‚Ð¾ 60 ÐºÐ¼
                    lng_step = 0.55 / max(0.2, abs(math.cos(math.radians(lat))))  # ÐŸÑ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ 35-40 ÐºÐ¼
                    if code == 'n': return lat+lat_step, lng
                    if code == 's': return lat-lat_step, lng
                    if code == 'e': return lat, lng+lng_step
                    if code == 'w': return lat, lng-lng_step
                    # Ð´Ð¸Ð°Ð³Ð¾Ð½Ð°Ð»Ð¸ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¿Ð¾ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¾ÑÐ¸
                    lat_diag = lat_step * 0.8
                    lng_diag = lng_step * 0.8
                    if code == 'ne': return lat+lat_diag, lng+lng_diag
                    if code == 'nw': return lat+lat_diag, lng-lng_diag
                    if code == 'se': return lat-lat_diag, lng+lng_diag
                    if code == 'sw': return lat-lat_diag, lng-lng_diag
                    return lat, lng
                
                # SPECIAL: Handle messages with start position + course direction
                # e.g. "Ð½Ð° Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ– Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑ‰Ð¸Ð½Ð¸ âž¡ï¸ ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹ Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº"
                start_direction = None
                course_direction = None
                
                # Detect start position (Ð½Ð° Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–/Ð¿Ñ–Ð²Ð´Ð½Ñ–/ÑÑ…Ð¾Ð´Ñ–/Ð·Ð°Ñ…Ð¾Ð´Ñ–)
                if re.search(r'\bÐ½Ð°\s+Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–\b', lower) or re.search(r'\bÐ¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½\w+\s+Ñ‡Ð°ÑÑ‚Ð¸Ð½\w*\b', lower):
                    start_direction = 'n'
                elif re.search(r'\bÐ½Ð°\s+Ð¿Ñ–Ð²Ð´Ð½Ñ–\b', lower) or re.search(r'\bÐ¿Ñ–Ð²Ð´ÐµÐ½Ð½\w+\s+Ñ‡Ð°ÑÑ‚Ð¸Ð½\w*\b', lower):
                    start_direction = 's'
                elif re.search(r'\bÐ½Ð°\s+ÑÑ…Ð¾Ð´Ñ–\b', lower) or re.search(r'\bÑÑ…Ñ–Ð´Ð½\w+\s+Ñ‡Ð°ÑÑ‚Ð¸Ð½\w*\b', lower):
                    start_direction = 'e'
                elif re.search(r'\bÐ½Ð°\s+Ð·Ð°Ñ…Ð¾Ð´Ñ–\b', lower) or re.search(r'\bÐ·Ð°Ñ…Ñ–Ð´Ð½\w+\s+Ñ‡Ð°ÑÑ‚Ð¸Ð½\w*\b', lower):
                    start_direction = 'w'
                
                # Detect course direction (ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ)
                if ('ÐºÑƒÑ€Ñ' in lower and 'Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº' in lower) or ('âž¡' in lower or 'â†’' in lower):
                    if 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in lower or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´' in lower:
                        course_direction = 'nw'
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½' in lower or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´' in lower:
                        course_direction = 'sw'
                    elif 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in lower or 'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´' in lower:
                        course_direction = 'ne'
                    elif 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½' in lower or 'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´' in lower:
                        course_direction = 'se'
                    # Single directions in course
                    elif re.search(r'ÐºÑƒÑ€Ñ\w*\s+Ð½Ð°\s+Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', lower):
                        course_direction = 'n'
                    elif re.search(r'ÐºÑƒÑ€Ñ\w*\s+Ð½Ð°\s+Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ', lower):
                        course_direction = 's'
                    elif re.search(r'ÐºÑƒÑ€Ñ\w*\s+Ð½Ð°\s+ÑÑ…Ñ–Ð´', lower):
                        course_direction = 'e'
                    elif re.search(r'ÐºÑƒÑ€Ñ\w*\s+Ð½Ð°\s+Ð·Ð°Ñ…Ñ–Ð´', lower):
                        course_direction = 'w'
                
                # If we have both start position and course direction, apply them sequentially
                if start_direction and course_direction:
                    # First offset: move to start position within region
                    lat_start, lng_start = offset(base_lat, base_lng, start_direction)
                    # Second offset: apply course direction from start position  
                    lat_final, lng_final = offset(lat_start, lng_start, course_direction)
                    
                    # Create descriptive label
                    start_labels = {'n':'Ð¿Ñ–Ð²Ð½Ð¾Ñ‡Ñ–', 's':'Ð¿Ñ–Ð²Ð´Ð½Ñ–', 'e':'ÑÑ…Ð¾Ð´Ñ–', 'w':'Ð·Ð°Ñ…Ð¾Ð´Ñ–'}
                    course_labels = {
                        'n':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡', 's':'Ð¿Ñ–Ð²Ð´ÐµÐ½ÑŒ', 'e':'ÑÑ…Ñ–Ð´', 'w':'Ð·Ð°Ñ…Ñ–Ð´',
                        'ne':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹ ÑÑ…Ñ–Ð´', 'nw':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹ Ð·Ð°Ñ…Ñ–Ð´', 
                        'se':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹ ÑÑ…Ñ–Ð´', 'sw':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¸Ð¹ Ð·Ð°Ñ…Ñ–Ð´'
                    }
                    start_label = start_labels.get(start_direction, 'Ð¾Ð±Ð»Ð°ÑÑ‚Ñ–')
                    course_label = course_labels.get(course_direction, 'Ð½Ð°Ð¿Ñ€ÑÐ¼Ð¾Ðº')
                    base_disp = reg_name.split()[0].title()
                    
                    threat_type, icon = classify(text)
                    return [{
                        'id': str(mid), 'place': f"{base_disp} (Ð· {start_label} Ð½Ð° {course_label})", 
                        'lat': lat_final, 'lng': lng_final,
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'region_start_course', 'count': drone_count
                    }]
                
                # ÑÐ¼ÐµÑ‰ÐµÐ½Ð¸Ðµ ~50-70 ÐºÐ¼ Ð² ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ (fallback for single direction)
                lat_o, lng_o = offset(base_lat, base_lng, direction_code)
                threat_type, icon = classify(text)
                dir_label_map = {
                    'n':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°', 's':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°', 'e':'ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°', 'w':'Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°',
                    'ne':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°', 'nw':'Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°',
                    'se':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-ÑÑ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°', 'sw':'Ð¿Ñ–Ð²Ð´ÐµÐ½Ð½Ð¾-Ð·Ð°Ñ…Ñ–Ð´Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°'
                }
                dir_phrase = dir_label_map.get(direction_code, 'Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°')
                base_disp = reg_name.split()[0].title()
                return [{
                    'id': str(mid), 'place': f"{base_disp} ({dir_phrase})", 'lat': lat_o, 'lng': lng_o,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'region_direction', 'count': drone_count
                }]
            # ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ â€” Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð°Ð½Ð°Ð»Ð¸Ð· (Ð¸Ñ‰ÐµÐ¼ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ†ÐµÐ»Ð¸ Ñ‚Ð¸Ð¿Ð° "ÐºÑƒÑ€Ñ Ð½Ð° <Ð¼Ñ–ÑÑ‚Ð¾>")
    # Midpoint for explicit course between two regions (e.g. "... Ð½Ð° Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ– ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñƒ")
    if len(matched_regions) == 2 and ('ÐºÑƒÑ€Ñ' in lower or 'âž¡' in lower or 'â†’' in lower) and (' Ð½Ð° ' in lower):
            # ensure we really reference both regions in a course sense: one mentioned before 'ÐºÑƒÑ€Ñ' and the other after 'ÐºÑƒÑ€Ñ' / arrow
            parts_course = re.split(r'ÐºÑƒÑ€Ñ|âž¡|â†’', lower, 1)
            if len(parts_course) == 2:
                before, after_part = parts_course
                r1, r2 = matched_regions[0], matched_regions[1]
                bnames = [r1[0].split()[0].lower(), r2[0].split()[0].lower()]
                # If both region stems appear across the split segments, build midpoint
                cond_split = (any(n[:5] in before for n in bnames) and any(n[:5] in after_part for n in bnames))
                # Fallback heuristic: pattern 'Ð½Ð° <region1>' earlier then arrow/"ÐºÑƒÑ€Ñ" then 'Ð½Ð° <region2>'
                if not cond_split:
                    # Extract simple region stems from OBLAST_CENTERS keys
                    stems = ['Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶','Ð·Ð°Ð¿Ð¾Ñ€', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚']
                    if any(st in lower for st in stems):
                        if re.search(r'Ð½Ð°\s+Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶', lower) and re.search(r'Ð½Ð°\s+Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²', lower):
                            cond_split = True
                if cond_split:
                    (n1,(a1,b1)), (n2,(a2,b2)) = matched_regions
                    lat = (a1+a2)/2; lng = (b1+b2)/2
                    # Slight bias toward destination (second region) if arrow or 'ÐºÑƒÑ€Ñ' present
                    if 'ÐºÑƒÑ€Ñ' in lower or 'âž¡' in lower or 'â†’' in lower:
                        lat = (a1*0.45 + a2*0.55)
                        lng = (b1*0.45 + b2*0.55)
                    threat_type, icon = classify(text)
                    return [{
                        'id': str(mid), 'place': f"ÐœÑ–Ð¶ {n1.split()[0].title()} Ñ‚Ð° {n2.split()[0].title()} (ÐºÑƒÑ€Ñ)", 'lat': lat, 'lng': lng,
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'region_course_midpoint', 'count': drone_count
                    }]

    if len(matched_regions) == 2 and any(w in lower for w in ['Ð¼ÐµÐ¶Ñ–','Ð¼ÐµÐ¶Ñƒ','Ð¼ÐµÐ¶Ð°','Ð¼ÐµÐ¶Ð´Ñƒ','Ð³Ñ€Ð°Ð½Ð¸Ñ†Ñ–','Ð³Ñ€Ð°Ð½Ð¸Ñ†Ð°']):
            (n1,(a1,b1)), (n2,(a2,b2)) = matched_regions
            lat = (a1+a2)/2; lng = (b1+b2)/2
            threat_type, icon = classify(text)
            return [{
                'id': str(mid), 'place': f"ÐœÐµÐ¶Ð° {n1.split()[0].title()}/{n2.split()[0].title()}" , 'lat': lat, 'lng': lng,
                'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                'marker_icon': icon, 'count': drone_count
            }]
    else:
            # If message contains explicit course targets (parsed later), don't emit plain region markers
            course_target_hint = False
            for ln in text.split('\n'):
                ll = ln.lower()
                if 'Ð±Ð¿Ð»Ð°' in ll and 'ÐºÑƒÑ€Ñ' in ll and re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+(?:Ð½Ð°|Ð²|Ñƒ)\s+[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,}', ll):
                    course_target_hint = True
                    break
            if not course_target_hint:
                threat_type, icon = classify(text)
                
                # Extract course information for Shahed threats
                course_info = None
                if threat_type == 'shahed':
                    course_info = extract_shahed_course_info(original_text or text)
                
                tracks = []
                seen = set()
                for idx,(n1,(lat,lng)) in enumerate(matched_regions,1):
                    base = n1.split()[0].title()
                    if base in seen: continue
                    seen.add(base)
                    
                    track = {
                        'id': f"{mid}_r{idx}", 'place': base, 'lat': lat, 'lng': lng,
                        'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                        'marker_icon': icon, 'source_match': 'region_multi_simple', 'count': drone_count
                    }
                    
                    # Add course information if available
                    if course_info:
                        track.update({
                            'course_source': course_info.get('source_city'),
                            'course_target': course_info.get('target_city'),
                            'course_direction': course_info.get('course_direction'),
                            'course_type': course_info.get('course_type')
                        })
                    
                    tracks.append(track)
                if tracks:
                    return tracks
    # City fallback scan (ensure whole-word style match to avoid false hits inside oblast words, e.g. 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾' in 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°')
    for city in UA_CITIES:
        if re.search(r'(?<![a-zÐ°-ÑÑ—Ñ–Ñ”Ò‘])' + re.escape(city) + r'(?![a-zÐ°-ÑÑ—Ñ–Ñ”Ò‘])', lower):
            norm = UA_CITY_NORMALIZE.get(city, city)
            # City fallback: attempt region-qualified first
            coords = None
            if region_hint_global and OPENCAGE_API_KEY:
                coords = geocode_opencage(f"{norm} {region_hint_global}")
            if not coords:
                coords = region_enhanced_coords(norm)
            # If Ð¾Ð±Ð»Ð°ÑÑ‚Ð½Ð¾Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ÑƒÐ¶Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ñ‘Ð½ (matched_regions) Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð¼ Ð³Ð¾Ñ€Ð¾Ð´Ð° Ñ‚Ð¾Ð¹ Ð¶Ðµ Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸
            if matched_regions:
                # Ð±ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ stem Ð¾Ð±Ð»Ð°ÑÑ‚Ð¸
                stem = None
                for (rn, _c) in matched_regions:
                    for s in ['Ñ…Ð°Ñ€ÐºÑ–Ð²','Ð»ÑŒÐ²Ñ–Ð²','ÐºÐ¸Ñ—Ð²','Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²','Ð¿Ð¾Ð»Ñ‚Ð°Ð²','ÑÑƒÐ¼','Ñ‡ÐµÑ€Ð½Ñ–Ð³','Ð²Ð¾Ð»Ð¸Ð½','Ð·Ð°Ð¿Ð¾Ñ€','Ð¾Ð´ÐµÑ','Ð¼Ð¸ÐºÐ¾Ð»Ð°','Ñ‡ÐµÑ€ÐºÐ°','Ð¶Ð¸Ñ‚Ð¾Ð¼','Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒ','Ñ€Ñ–Ð²Ð½','Ñ–Ð²Ð°Ð½Ð¾','Ñ‚ÐµÑ€Ð½Ð¾Ð¿','ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´','ÐºÑ€Ð¾Ð¿Ð¸Ð²','Ð»ÑƒÐ³Ð°Ð½','Ð´Ð¾Ð½ÐµÑ†ÑŒ','Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†']:
                        if s in rn:
                            stem = s; break
                    if stem: break
                if stem and norm in CITY_TO_OBLAST and CITY_TO_OBLAST[norm] != stem:
                    continue
            if coords:
                lat, lng = coords
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': norm.title(), 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'count': drone_count
                }]
            # if city found but no coords even in fallback, continue scanning others (no break)
    # --- Slash separated settlements with drone count (e.g. "Ð´Ð½Ñ–Ð¿Ñ€Ð¾ / ÑÐ°Ð¼Ð°Ñ€ â€” 6Ñ… Ð±Ð¿Ð»Ð° ... ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹") ---
    if '/' in lower and ('Ð±Ð¿Ð»Ð°' in lower or 'Ð´Ñ€Ð¾Ð½' in lower) and any(x in lower for x in ['Ñ… Ð±Ð¿Ð»Ð°','x Ð±Ð¿Ð»Ð°',' Ð±Ð¿Ð»Ð°']):
        left_part = lower.split('â€”')[0].split('-',1)[0]
        parts = [p.strip() for p in re.split(r'/|\\', left_part) if p.strip()]
        found = []
        for p in parts:
            if p in CITY_COORDS:
                found.append((p.title(), CITY_COORDS[p]))
        if found:
            threat_type, icon = classify(text)
            tracks = []
            for idx,(nm,(lat,lng)) in enumerate(found,1):
                # If course west mentioned, offset west a bit
                if 'ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´' in lower or 'ÐºÑƒÑ€Ñ Ð·Ð°Ð¿Ð°Ð´' in lower:
                    lng -= 0.4
                tracks.append({
                    'id': f"{mid}_s{idx}", 'place': nm, 'lat': lat, 'lng': lng,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'slash_combo'
                })
            if tracks:
                return tracks
    # --- Single city with westward course ("ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´Ð½Ð¸Ð¹") adjust marker to west to avoid mistaken northern region offsets ---
    if 'ÐºÑƒÑ€Ñ Ð·Ð°Ñ…Ñ–Ð´' in lower and 'Ð±Ð¿Ð»Ð°' in lower:
        for c in CITY_COORDS.keys():
            if c in lower:
                lat,lng = CITY_COORDS[c]
                threat_type, icon = classify(text)
                return [{
                    'id': str(mid), 'place': c.title(), 'lat': lat, 'lng': lng - 0.4,
                    'threat_type': threat_type, 'text': text[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'course_west'
                }]
    # --- Drone course target parsing (e.g. "Ð‘Ð¿Ð›Ð ÐºÑƒÑ€ÑÐ¾Ð¼ Ð½Ð° ÐÑ–Ð¶Ð¸Ð½") ---
    def _normalize_course_city(w: str):
        # Preserve internal single space for multi-word (e.g. "Ð»Ð¸Ð¿Ð¾Ð²Ð° Ð´Ð¾Ð»Ð¸Ð½Ð°") before stripping punctuation
        w = re.sub(r'\s+', ' ', w.strip().lower())
        # Remove punctuation but keep spaces and hyphen
        w = re.sub(r'["`Ê¼â€™\'.,:;()]+', '', w)
        # Allow letters, spaces, hyphen
        w = re.sub(r'[^a-zÐ°-ÑÑ—Ñ–Ñ”Ò‘Ñ‘\- ]', '', w)
        # Accusative to nominative heuristic for each word (handles phrases like 'Ð²ÐµÐ»Ð¸ÐºÑƒ Ð±Ð°Ð³Ð°Ñ‡ÐºÑƒ', 'Ð»Ð¸Ð¿Ð¾Ð²Ñƒ Ð´Ð¾Ð»Ð¸Ð½Ñƒ')
        parts = [p for p in w.split(' ') if p]
        norm_parts = []
        for p in parts:
            base = p
            # Common feminine accusative endings -> nominative
            if len(base) > 4 and base.endswith(('Ñƒ','ÑŽ')):
                base = base[:-1] + 'Ð°'
            # Handle '-Ñƒ/ÑŽ' endings for multi-word second element 'Ð´Ð¾Ð»Ð¸Ð½Ñƒ' -> 'Ð´Ð¾Ð»Ð¸Ð½Ð°'
            if len(base) > 5 and base.endswith('Ð¸Ð½Ñƒ'):
                base = base[:-2] + 'Ð½Ð°'
            # Special handling for oblast names ending in 'Ñ‰Ð¸Ð½Ñƒ' -> 'Ñ‰Ð¸Ð½Ð°'
            if len(base) > 6 and base.endswith('Ñ‰Ð¸Ð½Ñƒ'):
                base = base[:-1] + 'Ð°'
            norm_parts.append(base)
        w = ' '.join(norm_parts)
        # Apply explicit manual normalization map last (covers irregular)
        if w in UA_CITY_NORMALIZE:
            w = UA_CITY_NORMALIZE[w]
        return w
    course_matches = []
    # Ð˜Ñ‰ÐµÐ¼ ÐºÐ°Ð¶Ð´ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ñ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð¾Ð¼
    for line in text.split('\n'):
        line_low = line.lower()
        if 'Ð±Ð¿Ð»Ð°' in line_low and 'ÐºÑƒÑ€Ñ' in line_low and (' Ð½Ð° ' in line_low or ' Ð² ' in line_low or ' Ñƒ ' in line_low):
            # Capture one or two words as target, allowing hyphens and apostrophes
            m = re.search(r'ÐºÑƒÑ€Ñ(?:Ð¾Ð¼)?\s+(?:Ð½Ð°|Ð²|Ñƒ)\s+([A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,}(?:\s+[A-Za-zÐ-Ð¯Ð°-ÑÐ‡Ñ—Ð†Ñ–Ð„Ñ”ÒÒ‘\-]{3,})?)', line, flags=re.IGNORECASE)
            if m:
                raw_city = m.group(1)
                norm_city = _normalize_course_city(raw_city)
                if norm_city:
                    # If the captured target looks like an oblast (region) name (e.g. 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ð°', 'Ñ‡ÐµÑ€ÐºÐ°ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ'),
                    # we intentionally SKIP adding a precise course target marker to avoid falsely placing it at the oblast's capital city.
                    # User requirement: phrases like 'ÐºÑƒÑ€Ñ(Ð¾Ð¼) Ð½Ð° Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²Ñ‰Ð¸Ð½Ñƒ' must NOT create a marker right in 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾'.
                    # Check both nominative and accusative forms (Ñ‰Ð¸Ð½Ð°/Ñ‰Ð¸Ð½Ñƒ)
                    if re.search(r'(Ñ‰Ð¸Ð½Ð°|Ñ‰Ð¸Ð½Ñƒ|Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ)$', norm_city) or re.search(r'(Ñ‰Ð¸Ð½Ð°|Ñ‰Ð¸Ð½Ñƒ|Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ)$', raw_city.lower()):
                        log.debug(f'skip course_target oblast_only={norm_city} raw={raw_city} mid={mid}')
                        continue
                    coords = region_enhanced_coords(norm_city)
                    if not coords:
                        log.debug(f'course_target_lookup miss city={norm_city} mid={mid} line={line.strip()[:120]!r} region_hint={region_hint_global}')
                        coords = ensure_city_coords(norm_city)
                        # Try context-based lookup if standard lookup fails
                        if not coords:
                            context_result = ensure_city_coords_with_message_context(norm_city, text)
                            if context_result:
                                coords = context_result[:2]  # Take only lat, lng
                    # Oblast stem disambiguation: if global hint exists and known expected stem differs, re-query with region-qualified geocode
                    if coords and region_hint_global and norm_city in CITY_TO_OBLAST:
                        expected_stem = CITY_TO_OBLAST[norm_city]
                        if expected_stem != region_hint_global[:len(expected_stem)]:
                            # attempt region-qualified geocode with expected stem to refine
                            if OPENCAGE_API_KEY:
                                try:
                                    region_phrase = None
                                    # derive full oblast phrase from stem heuristically (simple mapping subset)
                                    stem_map = {
                                        'ÑÑƒÐ¼': 'ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²': 'Ð¿Ð¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ', 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²': 'Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ',
                                        'Ñ…Ð°Ñ€ÐºÑ–Ð²': 'Ñ…Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ'
                                    }
                                    region_phrase = stem_map.get(expected_stem)
                                    if region_phrase:
                                        refined = geocode_opencage(f"{norm_city} {region_phrase}")
                                        if refined:
                                            coords = refined
                                except Exception:
                                    pass
                    if coords:
                        log.debug(f'course_target_match city={norm_city} coords={coords} region_hint={region_hint_global} mid={mid}')
                    # If still no coords AND we have a region hint + OpenCage, try region-qualified query directly for multi-word ambiguous city
                    if not coords and region_hint_global and OPENCAGE_API_KEY:
                        try:
                            refined2 = geocode_opencage(f"{norm_city} {region_hint_global}")
                            if refined2:
                                coords = refined2
                        except Exception:
                            pass
                    if coords:
                        # Extract line-specific drone count if present (e.g. "4Ñ… Ð‘Ð¿Ð›Ð")
                        line_count = None
                        m_lc = re.search(r'(\b\d{1,3})\s*[xÑ…]\s*Ð±Ð¿Ð»Ð°', line_low)
                        if m_lc:
                            try:
                                line_count = int(m_lc.group(1))
                            except Exception:
                                line_count = None
                        # Ensure coords is a tuple of exactly 2 elements (lat, lng)
                        if len(coords) >= 2:
                            coords = coords[:2]
                        course_matches.append((norm_city.title(), coords, line[:200], line_count))
    if course_matches:
        threat_type, icon = classify(text)
        tracks = []
        seen_places = set()
        for idx,(name,(lat,lng),snippet,line_count) in enumerate(course_matches,1):
            if name in seen_places: continue
            seen_places.add(name)
            
            # Extract Shahed course information if this is a Shahed threat
            course_info = None
            if threat_type == 'shahed':
                course_info = extract_shahed_course_info(original_text or text)
            
            # Determine how many tracks to create
            count = line_count if line_count else drone_count
            tracks_to_create = max(1, count if count else 1)
            
            # Create multiple tracks for multiple drones
            for i in range(tracks_to_create):
                track_name = name
                if tracks_to_create > 1:
                    track_name += f" #{i+1}"
                
                # Add small coordinate offsets to prevent marker overlap
                marker_lat = lat
                marker_lng = lng
                if tracks_to_create > 1:
                    # Create a circular pattern around the target
                    import math
                    angle = (2 * math.pi * i) / tracks_to_create
                    offset_distance = 0.01  # ~1km offset
                    marker_lat += offset_distance * math.cos(angle)
                    marker_lng += offset_distance * math.sin(angle)
                
                track = {
                    'id': f"{mid}_c{idx}_{i+1}", 'place': track_name, 'lat': marker_lat, 'lng': marker_lng,
                    'threat_type': threat_type, 'text': snippet[:500], 'date': date_str, 'channel': channel,
                    'marker_icon': icon, 'source_match': 'course_target', 'count': 1
                }
                
                # Add course information if available
                if course_info:
                    track.update({
                        'course_source': course_info.get('source_city'),
                        'course_target': course_info.get('target_city'),
                        'course_direction': course_info.get('course_direction'),
                        'course_type': course_info.get('course_type')
                    })
                
                tracks.append(track)
        if tracks:
            return tracks
    
    # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð¸ Ñ‚Ñ€ÐµÐºÐ¾Ð²
    try:
        if text and len(text) > 1000:
            print(f"DEBUG: LONG MESSAGE NO TRACKS - mid={mid}, length={len(text)}, preview: {text[:200]}...")
            # ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ð¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²
            lower_check = text.lower()
            keywords = {'Ð±Ð¿Ð»Ð°': lower_check.count('Ð±Ð¿Ð»Ð°'), 'ÑˆÐ°Ñ…ÐµÐ´': lower_check.count('ÑˆÐ°Ñ…ÐµÐ´'), 
                       'ÐºÑƒÑ€Ñ': lower_check.count('ÐºÑƒÑ€Ñ'), 'Ñ€Ð°Ð¹Ð¾Ð½': lower_check.count('Ñ€Ð°Ð¹Ð¾Ð½')}
            print(f"DEBUG: Long message keywords: {keywords}")
    except Exception:
        pass
    
    # Final check: if we found single UAV threats earlier but no other tracks, return the UAV threats
    if 'single_uav_threats' in locals() and single_uav_threats:
        add_debug_log(f"FINAL: Returning single UAV threats only: {len(single_uav_threats)}", "final_single_uav")
        return single_uav_threats
    
    return None

async def fetch_loop():
    log.info('fetch_loop() started')
    if not client:
        log.warning('Telegram client not configured; skipping fetch loop.')
        return
    log.info('fetch_loop: client exists, proceeding')
    async def ensure_connected():
        log.info('ensure_connected() called')
        if client.is_connected():
            log.info('Client already connected')
            auth_status = await client.is_user_authorized()
            log.info(f'Authorization status: {auth_status}')
            return auth_status
        try:
            log.info('Connecting client...')
            await client.connect()
            log.info('Client connected successfully')
            # If bot token provided and not authorized yet, try bot login
            if BOT_TOKEN and not await client.is_user_authorized():
                try:
                    log.info('Trying bot token login...')
                    await client.start(bot_token=BOT_TOKEN)
                except Exception as be:
                    log.error(f'Bot start failed: {be}')
            auth_status = await client.is_user_authorized()
            log.info(f'Final authorization status: {auth_status}')
            if not auth_status:
                log.error('Not authorized. Use /auth/start & /auth/complete to login or set TELEGRAM_SESSION.')
                return False
            return True
        except AuthKeyDuplicatedError:
            log.error('AuthKeyDuplicatedError: duplicate session. Provide new TELEGRAM_SESSION or re-auth.')
            return False
        except AuthKeyUnregisteredError:
            log.error('AuthKeyUnregisteredError: Session invalid/expired. Re-auth needed.')
            return False
        except FloodWaitError as fe:
            wait = int(getattr(fe, 'seconds', 60))
            log.warning(f'FloodWait: sleeping {wait}s before reconnect.')
            await asyncio.sleep(wait)
            return False
        except Exception as e:
            log.warning(f'ensure_connected error: {e}')
            return False

    if not await ensure_connected():
        AUTH_STATUS.update({'authorized': False, 'reason': 'not_authorized_initial'})
        await asyncio.sleep(180)
        return
    else:
        AUTH_STATUS.update({'authorized': True, 'reason': 'ok'})
    tz = pytz.timezone('Europe/Kyiv')
    processed = {m.get('id') for m in load_messages()}
    all_data = load_messages()
    # -------- Initial backfill (last BACKFILL_MINUTES, default 50) --------
    try:
        backfill_minutes = int(os.getenv('BACKFILL_MINUTES', '50'))
    except ValueError:
        backfill_minutes = 50
    backfill_cutoff = datetime.now(tz) - timedelta(minutes=backfill_minutes)
    if backfill_minutes > 0:
        log.info(f'Starting backfill for last {backfill_minutes} minutes...')
        total_backfilled = 0
        total_raw = 0
        for ch in CHANNELS:
            ch_strip = ch.strip()
            if not ch_strip:
                continue
            print(f"DEBUG: Processing backfill for channel: {ch_strip}")
            fetched = 0
            try:
                if not await ensure_connected():
                    log.warning('Disconnected during backfill; aborting backfill early.')
                    break
                async for msg in client.iter_messages(ch_strip, limit=400):  # cap to avoid huge history
                    if not msg.text:
                        continue
                    dt = msg.date.astimezone(tz)
                    if dt < backfill_cutoff:
                        break  # older than needed
                    if msg.id in processed:
                        continue
                    tracks = process_message(msg.text, msg.id, dt.strftime('%Y-%m-%d %H:%M:%S'), ch_strip)
                    if tracks:
                        print(f"DEBUG: Message {msg.id} generated {len(tracks)} tracks")
                        merged_any = False
                        merged_refs = []
                        for t in tracks:
                            if t.get('place'):
                                t['place'] = ensure_ua_place(t['place'])
                            merged, ref = maybe_merge_track(all_data, t)
                            if merged:
                                merged_any = True
                                merged_refs.append(ref)
                            else:
                                all_data.append(t)
                        processed.add(msg.id)
                        if merged_any:
                            log.info(f'Merged track(s) for {ch_strip} #{msg.id} into existing point(s).')
                        fetched += 1
                    else:
                        print(f"DEBUG: Message {msg.id} generated NO tracks (filtered or no matches)")
                        if ALWAYS_STORE_RAW:
                            all_data.append({
                                'id': str(msg.id),
                                'place': None,
                                'lat': None,
                                'lng': None,
                                'threat_type': None,
                                'text': msg.text[:500],
                                'date': dt.strftime('%Y-%m-%d %H:%M:%S'),
                                'channel': ch_strip,
                                'pending_geo': True
                            })
                            processed.add(msg.id)
                            total_raw += 1
                        print(f"DEBUG: Message {msg.id} - ALWAYS_STORE_RAW={ALWAYS_STORE_RAW}, stored as raw")
                        log.debug(f'Backfill skip (no geo): {ch_strip} #{msg.id} {msg.text[:80]!r}')
                if fetched:
                    total_backfilled += fetched
                    log.info(f'Backfilled {fetched} messages from {ch_strip}')
            except Exception as e:
                log.warning(f'Backfill error {ch_strip}: {e}')
    if backfill_minutes > 0:
        if total_backfilled or (ALWAYS_STORE_RAW and 'total_raw' in locals() and total_raw):
            save_messages(all_data)
            log.info(f'Backfill saved: {total_backfilled} geo, {locals().get("total_raw",0)} raw')
        log.info('Backfill completed.')
    while True:
        new_tracks = []
        for ch in CHANNELS:
            ch = ch.strip()
            if not ch:
                continue
            if ch in INVALID_CHANNELS:
                log.debug(f'Skip invalid channel {ch}')
                continue
            msgs_seen = 0
            msgs_recent_window = 0
            geo_added = 0
            try:
                if not await ensure_connected():
                    # If session invalid we stop loop gracefully
                    if not client.is_connected():
                        log.error('Stopping live loop due to lost/invalid session.')
                        AUTH_STATUS.update({'authorized': False, 'reason': 'lost_session'})
                        return
                log.debug(f'Polling channel {ch} (last processed count={len(processed)})')
                async for msg in client.iter_messages(ch, limit=20):
                    msgs_seen += 1
                    if not msg.text:
                        continue
                    if msg.id in processed:
                        continue
                    dt = msg.date.astimezone(tz)
                    if dt < datetime.now(tz) - timedelta(minutes=30):
                        # Older than live window
                        continue
                    msgs_recent_window += 1
                    tracks = process_message(msg.text, msg.id, dt.strftime('%Y-%m-%d %H:%M:%S'), ch)
                    if tracks:
                        merged_any = False
                        appended = []
                        for t in tracks:
                            if t.get('place'):
                                t['place'] = ensure_ua_place(t['place'])
                            merged, ref = maybe_merge_track(all_data, t)
                            if merged:
                                merged_any = True
                            else:
                                new_tracks.append(t)
                                appended.append(t)
                        geo_added += 1
                        processed.add(msg.id)
                        if merged_any and not appended:
                            log.info(f'Merged live track(s) {ch} #{msg.id} (no new marker).')
                        else:
                            log.info(f'Added track from {ch} #{msg.id} (+{len(appended)} new, merged={merged_any})')
                    else:
                        # Store raw if enabled to allow later reprocessing / debugging (e.g., napramok multi-line posts)
                        if ALWAYS_STORE_RAW:
                            all_data.append({
                                'id': str(msg.id), 'place': None, 'lat': None, 'lng': None,
                                'threat_type': None, 'text': msg.text[:800], 'date': dt.strftime('%Y-%m-%d %H:%M:%S'),
                                'channel': ch, 'pending_geo': True
                            })
                            processed.add(msg.id)
                        log.debug(f'Live skip (no geo): {ch} #{msg.id} {msg.text[:80]!r}')
            except AuthKeyDuplicatedError:
                log.error('AuthKeyDuplicatedError during live fetch. Ending loop until session replaced.')
                AUTH_STATUS.update({'authorized': False, 'reason': 'authkey_duplicated'})
                return
            except FloodWaitError as fe:
                wait = int(getattr(fe, 'seconds', 60))
                log.warning(f'FloodWait while reading {ch}: sleep {wait}s')
                await asyncio.sleep(wait)
            # Generic RPC errors will be caught by broad Exception if specific class not available
            except Exception as e:
                msg = str(e)
                log.warning(f'Error reading {ch}: {msg}')
                # Auto-mark invalid entity errors to skip future attempts this runtime
                markers = ['Cannot find any entity', 'CHANNEL_PRIVATE', 'USERNAME_NOT_OCCUPIED', 'TOPIC_DELETED']
                if any(mk in msg for mk in markers):
                    INVALID_CHANNELS.add(ch)
                    log.warning(f'Marking channel {ch} as invalid; will skip further reads this session.')
            finally:
                # Post-channel diagnostics to help debug silent channels like 'napramok'
                log.debug(
                    f'Channel diag {ch}: iter_messages_seen={msgs_seen}, recent_window={msgs_recent_window}, geo_added={geo_added}, invalid={ch in INVALID_CHANNELS}'
                )
                if msgs_seen == 0:
                    log.warning(f'Channel {ch} returned no messages this cycle (possible resolution/access issue).')
                elif msgs_recent_window == 0:
                    log.debug(f'Channel {ch} had messages but none within last 30m window.')
                elif geo_added == 0:
                    log.debug(f'Channel {ch} had {msgs_recent_window} recent messages but none produced geo tracks.')
        if new_tracks:
            # Append truly new tracks (merges already applied in-place)
            all_data.extend(new_tracks)
            save_messages(all_data)
            try:
                broadcast_new(new_tracks)
            except Exception as e:
                log.debug(f'SSE broadcast failed: {e}')
        else:
            # If only merges happened (no brand-new tracks), still persist periodically
            save_messages(all_data)
        await asyncio.sleep(60)

def start_fetch_thread():
    global FETCH_THREAD_STARTED
    log.info('start_fetch_thread() called')
    if not client:
        log.warning('start_fetch_thread: client is None')
        return
    if FETCH_THREAD_STARTED:
        log.info('start_fetch_thread: already started')
        return
    log.info('start_fetch_thread: starting new thread')
    FETCH_THREAD_STARTED = True
    loop = asyncio.new_event_loop()
    def runner():
        log.info('fetch_thread runner started')
        if FETCH_START_DELAY > 0:
            log.info(f'Delaying Telegram fetch start for {FETCH_START_DELAY}s (FETCH_START_DELAY).')
            time.sleep(FETCH_START_DELAY)
        asyncio.set_event_loop(loop)
        try:
            log.info('About to call fetch_loop()')
            loop.run_until_complete(fetch_loop())
        except AuthKeyDuplicatedError:
            AUTH_STATUS.update({'authorized': False, 'reason': 'authkey_duplicated_runner'})
            log.error('Fetch loop stopped: duplicated auth key.')
        except Exception as e:
            AUTH_STATUS.update({'authorized': False, 'reason': f'crash:{e.__class__.__name__}'})
            log.error(f'Fetch loop crashed: {e}')
        finally:
            FETCH_THREAD_STARTED = False
            log.info('fetch_thread runner finished')
    threading.Thread(target=runner, daemon=True).start()
    log.info('start_fetch_thread: thread started successfully')

def replace_client(new_session: str):
    global client, session_str
    session_str = new_session
    try:
        if client:
            try:
                # Telethon has disconnect
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(client.disconnect())
            except Exception:
                pass
    finally:
        client = TelegramClient(StringSession(new_session), API_ID, API_HASH)
        AUTH_STATUS.update({'authorized': True, 'reason': 'replaced_session'})
        start_fetch_thread()

# ----------------- Session watcher (auto reload new_session.txt) -----------------
SESSION_WATCH_FILE = os.getenv('SESSION_WATCH_FILE', 'new_session.txt')
SESSION_WATCH_INTERVAL = int(os.getenv('SESSION_WATCH_INTERVAL', '20'))
_watch_thread_started = False
_last_session_file_mtime = 0

def start_session_watcher():
    global _watch_thread_started, _last_session_file_mtime
    if _watch_thread_started:
        return
    _watch_thread_started = True
    def _watch():
        global _last_session_file_mtime, session_str
        while True:
            try:
                if os.path.exists(SESSION_WATCH_FILE):
                    mt = os.path.getmtime(SESSION_WATCH_FILE)
                    if mt != _last_session_file_mtime:
                        _last_session_file_mtime = mt
                        with open(SESSION_WATCH_FILE,'r',encoding='utf-8') as f:
                            new_s = f.read().strip()
                        if new_s and new_s != session_str:
                            log.info('Session watcher: detected updated session file, reloading...')
                            replace_client(new_s)
                # If we are unauthorized due to duplicate key, keep looking for replacement
                if AUTH_STATUS.get('reason','').startswith('authkey_duplicated') and not client.is_connected():
                    # just a hint in logs every few cycles
                    if int(time.time()) % (SESSION_WATCH_INTERVAL*3) == 0:
                        log.info('Waiting for new session (AuthKeyDuplicatedError). Generate via /auth endpoints.')
            except Exception as e:
                log.debug(f'Session watcher error: {e}')
            time.sleep(SESSION_WATCH_INTERVAL)
    threading.Thread(target=_watch, daemon=True).start()

@app.route('/')
def index():
    # BANDWIDTH OPTIMIZATION: Add caching headers for main page
    response = render_template('index.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=300'  # 5 minutes cache
    resp.headers['ETag'] = f'index-{int(time.time() // 300)}'
    return resp

@app.route('/blackouts')
def blackouts():
    """Power blackout schedules and information page"""
    response = render_template('blackouts.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=300'  # 5 minutes cache
    return resp

@app.route('/about')
def about():
    """About NEPTUN project page"""
    response = render_template('about.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=3600'  # 1 hour cache
    return resp

@app.route('/faq')
def faq():
    """Frequently Asked Questions page"""
    response = render_template('faq.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=3600'  # 1 hour cache
    return resp

@app.route('/privacy')
def privacy():
    """Privacy Policy page"""
    response = render_template('privacy.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=86400'  # 24 hours cache
    return resp

@app.route('/terms')
def terms():
    """Terms of Service page"""
    response = render_template('terms.html')
    resp = app.response_class(response)
    resp.headers['Cache-Control'] = 'public, max-age=86400'  # 24 hours cache
    return resp

# Address database for blackout schedules with subgroups
# Format: group can be "1.1", "1.2", "2.1", "2.2", "3.1", "3.2" etc.
# TODO: Replace with real database and API integration
BLACKOUT_ADDRESSES = {
    # Kyiv - all subgroups across different districts
    'ÐºÐ¸Ñ—Ð² Ñ…Ñ€ÐµÑ‰Ð°Ñ‚Ð¸Ðº': {'group': '1.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð²ÑƒÐ»Ð¸Ñ†Ñ Ñ…Ñ€ÐµÑ‰Ð°Ñ‚Ð¸Ðº': {'group': '1.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð¼Ð°Ð¹Ð´Ð°Ð½': {'group': '1.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð¿ÐµÑ‡ÐµÑ€ÑÑŒÐº': {'group': '2.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð¿Ð¾Ð´Ñ–Ð»': {'group': '2.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '3.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð¾Ð±Ð¾Ð»Ð¾Ð½ÑŒ': {'group': '3.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ñ‚Ñ€Ð¾Ñ”Ñ‰Ð¸Ð½Ð°': {'group': '1.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð¿Ð¾Ð·Ð½ÑÐºÐ¸': {'group': '2.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð´Ð°Ñ€Ð½Ð¸Ñ†Ñ': {'group': '3.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð»Ñ–Ð²Ð¾Ð±ÐµÑ€ÐµÐ¶Ð½Ð°': {'group': '2.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð°ÐºÐ°Ð´ÐµÐ¼Ð¼Ñ–ÑÑ‚ÐµÑ‡ÐºÐ¾': {'group': '3.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ñ‚ÐµÑ€ÐµÐ¼ÐºÐ¸': {'group': '1.2', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð²Ð¸ÑˆÐ½ÐµÐ²Ðµ': {'group': '2.1', 'city': 'ÐšÐ¸Ñ—Ð²', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'ÐºÐ¸Ñ—Ð² Ð±Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ': {'group': '1.1', 'city': 'Ð‘Ð¾Ñ€Ð¸ÑÐ¿Ñ–Ð»ÑŒ', 'oblast': 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐšÐ¸Ñ—Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    
    # Odesa - all subgroups
    'Ð¾Ð´ÐµÑÐ° Ð´ÐµÑ€Ð¸Ð±Ð°ÑÑ–Ð²ÑÑŒÐºÐ°': {'group': '1.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ð¿Ñ€Ð¸Ð¼Ð¾Ñ€ÑÑŒÐºÐ¸Ð¹': {'group': '1.2', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '2.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ð°Ñ€ÐºÐ°Ð´Ñ–Ñ': {'group': '2.2', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ñ‚Ð°Ñ—Ñ€Ð¾Ð²Ð°': {'group': '3.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° ÐºÐ¾Ñ‚Ð¾Ð²ÑÑŒÐºÐ¾Ð³Ð¾': {'group': '3.2', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ð¼Ð¾Ð»Ð´Ð°Ð²Ð°Ð½ÐºÐ°': {'group': '1.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ð¿ÐµÑ€ÐµÑÐ¸Ð¿ÑŒ': {'group': '2.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° ÑÑƒÐ²Ð¾Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '3.1', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¾Ð´ÐµÑÐ° Ñ‡Ð¾Ñ€Ð½Ð¾Ð¼Ð¾Ñ€ÐºÐ°': {'group': '1.2', 'city': 'ÐžÐ´ÐµÑÐ°', 'oblast': 'ÐžÐ´ÐµÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš ÐžÐ´ÐµÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    
    # Kharkiv - all subgroups
    'Ñ…Ð°Ñ€ÐºÑ–Ð² ÑÑƒÐ¼ÑÑŒÐºÐ°': {'group': '1.1', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.2', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² ÑÐ°Ð»Ñ‚Ñ–Ð²ÐºÐ°': {'group': '2.1', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ð½Ð°Ð³Ñ–Ñ€Ð½Ð¸Ð¹': {'group': '2.2', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ñ…Ð¾Ð»Ð¾Ð´Ð½Ð° Ð³Ð¾Ñ€Ð°': {'group': '3.1', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ð¿Ð°Ð²Ð»Ð¾Ð²Ðµ Ð¿Ð¾Ð»Ðµ': {'group': '3.2', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ð¼Ð¾ÑÐºÐ¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '1.1', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² Ñ–Ð½Ð´ÑƒÑÑ‚Ñ€Ñ–Ð°Ð»ÑŒÐ½Ð¸Ð¹': {'group': '2.1', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð°Ñ€ÐºÑ–Ð² ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '1.2', 'city': 'Ð¥Ð°Ñ€ÐºÑ–Ð²', 'oblast': 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð¡Ñ…Ñ–Ð´ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Dnipro - all subgroups
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð³Ð°Ð³Ð°Ñ€Ñ–Ð½Ð°': {'group': '1.2', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð½Ð°Ð³Ñ–Ñ€Ð½Ð¸Ð¹': {'group': '2.1', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð¿Ñ€Ð¾ÑÐ¿ÐµÐºÑ‚': {'group': '2.2', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð¿Ñ€Ð¸Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐº': {'group': '3.1', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð½Ð¾Ð²Ð¾ÐºÐ¾Ð´Ð°Ñ†ÑŒÐºÐ¸Ð¹': {'group': '3.2', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ ÑÐ¾Ð±Ð¾Ñ€Ð½Ð¸Ð¹': {'group': '1.1', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð´Ð½Ñ–Ð¿Ñ€Ð¾ Ð°Ð¼ÑƒÑ€': {'group': '2.1', 'city': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾', 'oblast': 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    
    # Lviv - all subgroups
    'Ð»ÑŒÐ²Ñ–Ð² Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² Ð¿Ð»Ð¾Ñ‰Ð° Ñ€Ð¸Ð½Ð¾Ðº': {'group': '1.2', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² Ñ„Ñ€Ð°Ð½ÐºÐ°': {'group': '2.1', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² ÑÐ¸Ñ…Ñ–Ð²': {'group': '2.2', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² Ð»Ð¸Ñ‡Ð°ÐºÑ–Ð²ÑÑŒÐºÐ°': {'group': '3.1', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² ÑÐ¸Ñ…Ñ–Ð²ÑÑŒÐºÐ°': {'group': '3.2', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² Ð·Ð°Ð»Ñ–Ð·Ð½Ð¸Ñ‡Ð½Ð¸Ð¹': {'group': '1.1', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑŒÐ²Ñ–Ð² ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð›ÑŒÐ²Ñ–Ð²', 'oblast': 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð›ÑŒÐ²Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Zaporizhzhia - all subgroups
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ Ð¿Ñ€Ð¾ÑÐ¿ÐµÐºÑ‚': {'group': '1.2', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ Ñ…Ð¾Ñ€Ñ‚Ð¸Ñ†ÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ ÑˆÐµÐ²Ñ‡ÐµÐ½ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.2', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ Ð·Ð°Ð²Ð¾Ð´ÑÑŒÐºÐ¸Ð¹': {'group': '3.1', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð·Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '3.2', 'city': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶Ñ', 'oblast': 'Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð·ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    
    # Vinnytsia - all subgroups
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†Ñ', 'oblast': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ ÑÐ¾Ð±Ð¾Ñ€Ð½Ð°': {'group': '1.2', 'city': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†Ñ', 'oblast': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐµ': {'group': '2.1', 'city': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†Ñ', 'oblast': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ Ð²Ð¸ÑˆÐµÐ½ÑŒÐºÐ°': {'group': '2.2', 'city': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†Ñ', 'oblast': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð²Ñ–Ð½Ð½Ð¸Ñ†Ñ Ð·Ð°Ð¼Ð¾ÑÑ‚Ñ': {'group': '3.1', 'city': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†Ñ', 'oblast': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð’Ñ–Ð½Ð½Ð¸Ñ†ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Poltava - all subgroups
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð° Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð° ÑÐ¾Ð±Ð¾Ñ€Ð½Ð¾ÑÑ‚Ñ–': {'group': '1.2', 'city': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð° ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¿Ð¾Ð»Ñ‚Ð°Ð²Ð° Ð¿Ð¾Ð´Ñ–Ð»ÑŒÑÑŒÐºÐ¸Ð¹': {'group': '2.2', 'city': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Chernihiv - all subgroups
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð² Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð² Ð¼Ð¸Ñ€Ð°': {'group': '1.2', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð² Ð´ÐµÑÐ½ÑÐ½ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Zhytomyr - all subgroups
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€', 'oblast': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°', 'provider': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°': {'group': '1.2', 'city': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€', 'oblast': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°', 'provider': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ Ð±Ð¾Ð³ÑƒÐ½Ñ–Ñ': {'group': '2.1', 'city': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€', 'oblast': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°', 'provider': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¶Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ ÐºÐ¾Ñ€Ð¾Ð»ÑŒÐ¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.2', 'city': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€', 'oblast': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€ÑÑŒÐºÐ°', 'provider': 'Ð–Ð¸Ñ‚Ð¾Ð¼Ð¸Ñ€Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Cherkasy - all subgroups
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸', 'oblast': 'Ð§ÐµÑ€ÐºÐ°ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸ ÑÐ¾Ð±Ð¾Ñ€Ð½Ð°': {'group': '1.2', 'city': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸', 'oblast': 'Ð§ÐµÑ€ÐºÐ°ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€go'},
    'Ñ‡ÐµÑ€ÐºÐ°ÑÐ¸ Ð¿Ñ€Ð¸Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸', 'oblast': 'Ð§ÐµÑ€ÐºÐ°ÑÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€ÐºÐ°ÑÐ¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Sumy - all subgroups
    'ÑÑƒÐ¼Ð¸ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð¡ÑƒÐ¼Ð¸', 'oblast': 'Ð¡ÑƒÐ¼ÑÑŒÐºÐ°', 'provider': 'Ð¡ÑƒÐ¼Ð¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÑÑƒÐ¼Ð¸ ÑÐ¾Ð±Ð¾Ñ€Ð½Ð°': {'group': '1.2', 'city': 'Ð¡ÑƒÐ¼Ð¸', 'oblast': 'Ð¡ÑƒÐ¼ÑÑŒÐºÐ°', 'provider': 'Ð¡ÑƒÐ¼Ð¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÑÑƒÐ¼Ð¸ ÐºÐ¾Ð²Ð¿Ð°ÐºÑ–Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð¡ÑƒÐ¼Ð¸', 'oblast': 'Ð¡ÑƒÐ¼ÑÑŒÐºÐ°', 'provider': 'Ð¡ÑƒÐ¼Ð¸Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Khmelnytskyi - all subgroups
    'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ð¿Ñ€Ð¾ÑÐ¿ÐµÐºÑ‚': {'group': '1.2', 'city': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ð·Ð°Ð³Ð¾Ñ†ÑŒÐºÐ°': {'group': '2.1', 'city': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ°', 'provider': 'Ð¥Ð¼ÐµÐ»ÑŒÐ½Ð¸Ñ†ÑŒÐºÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Rivne - all subgroups
    'Ñ€Ñ–Ð²Ð½Ðµ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð Ñ–Ð²Ð½Ðµ', 'oblast': 'Ð Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ°', 'provider': 'Ð Ñ–Ð²Ð½ÐµÐ½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ€Ñ–Ð²Ð½Ðµ ÑÐ¾Ð±Ð¾Ñ€Ð½Ð°': {'group': '1.2', 'city': 'Ð Ñ–Ð²Ð½Ðµ', 'oblast': 'Ð Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ°', 'provider': 'Ð Ñ–Ð²Ð½ÐµÐ½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ€Ñ–Ð²Ð½Ðµ Ð¿Ñ–Ð²Ð½Ñ–Ñ‡Ð½Ð¸Ð¹': {'group': '2.1', 'city': 'Ð Ñ–Ð²Ð½Ðµ', 'oblast': 'Ð Ñ–Ð²Ð½ÐµÐ½ÑÑŒÐºÐ°', 'provider': 'Ð Ñ–Ð²Ð½ÐµÐ½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Ivano-Frankivsk - all subgroups
    'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº', 'oblast': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÑ€Ð¸ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº Ð½ÐµÐ·Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ–': {'group': '1.2', 'city': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº', 'oblast': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÑ€Ð¸ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ–Ð²Ð°Ð½Ð¾-Ñ„Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº Ð¿Ð°ÑÑ–Ñ‡Ð½Ð°': {'group': '2.1', 'city': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐº', 'oblast': 'Ð†Ð²Ð°Ð½Ð¾-Ð¤Ñ€Ð°Ð½ÐºÑ–Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÑ€Ð¸ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Ternopil - all subgroups
    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ', 'oblast': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ°', 'provider': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ Ñ€ÑƒÑÑŒÐºÐ°': {'group': '1.2', 'city': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ', 'oblast': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ°', 'provider': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‚ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ ÑÑ…Ñ–Ð´Ð½Ð¸Ð¹': {'group': '2.1', 'city': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒ', 'oblast': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÑÑŒÐºÐ°', 'provider': 'Ð¢ÐµÑ€Ð½Ð¾Ð¿Ñ–Ð»ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Lutsk - all subgroups
    'Ð»ÑƒÑ†ÑŒÐº Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð›ÑƒÑ†ÑŒÐº', 'oblast': 'Ð’Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ°', 'provider': 'Ð’Ð¾Ð»Ð¸Ð½ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑƒÑ†ÑŒÐº Ð²Ð¾Ð»Ñ–': {'group': '1.2', 'city': 'Ð›ÑƒÑ†ÑŒÐº', 'oblast': 'Ð’Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ°', 'provider': 'Ð’Ð¾Ð»Ð¸Ð½ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð»ÑƒÑ†ÑŒÐº Ð²Ð¾ÐºÐ·Ð°Ð»ÑŒÐ½Ð°': {'group': '2.1', 'city': 'Ð›ÑƒÑ†ÑŒÐº', 'oblast': 'Ð’Ð¾Ð»Ð¸Ð½ÑÑŒÐºÐ°', 'provider': 'Ð’Ð¾Ð»Ð¸Ð½ÑŒÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Chernivtsi - all subgroups
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ– Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ– Ð³Ð¾Ð»Ð¾Ð²Ð½Ð°': {'group': '1.2', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ‡ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ– ÑÐ°Ð´Ð³Ð¾Ñ€Ð°': {'group': '2.1', 'city': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–', 'oblast': 'Ð§ÐµÑ€Ð½Ñ–Ð²ÐµÑ†ÑŒÐºÐ°', 'provider': 'Ð§ÐµÑ€Ð½Ñ–Ð²Ñ†Ñ–Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Uzhhorod - all subgroups
    'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð£Ð¶Ð³Ð¾Ñ€Ð¾Ð´', 'oblast': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ°', 'provider': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´ ÐºÐ¾Ñ€Ð·Ð¾': {'group': '1.2', 'city': 'Ð£Ð¶Ð³Ð¾Ñ€Ð¾Ð´', 'oblast': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ°', 'provider': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÑƒÐ¶Ð³Ð¾Ñ€Ð¾Ð´ Ð±Ð¾Ð·Ð´Ð¾Ñˆ': {'group': '2.1', 'city': 'Ð£Ð¶Ð³Ð¾Ñ€Ð¾Ð´', 'oblast': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚ÑÑŒÐºÐ°', 'provider': 'Ð—Ð°ÐºÐ°Ñ€Ð¿Ð°Ñ‚Ñ‚ÑÐ¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Kropyvnytskyi (Kirovohrad) - all subgroups
    'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'ÐšÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ°', 'provider': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ð²ÐµÐ»Ð¸ÐºÐ° Ð¿ÐµÑ€ÑÐ¿ÐµÐºÑ‚Ð¸Ð²Ð½Ð°': {'group': '1.2', 'city': 'ÐšÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ°', 'provider': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÐºÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹ Ñ„Ð¾Ñ€Ñ‚ÐµÑ‡Ð½Ð¸Ð¹': {'group': '2.1', 'city': 'ÐšÑ€Ð¾Ð¿Ð¸Ð²Ð½Ð¸Ñ†ÑŒÐºÐ¸Ð¹', 'oblast': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´ÑÑŒÐºÐ°', 'provider': 'ÐšÑ–Ñ€Ð¾Ð²Ð¾Ð³Ñ€Ð°Ð´Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Mykolaiv - all subgroups
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð² Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²', 'oblast': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð² ÑÐ¾Ð±Ð¾Ñ€Ð½Ð°': {'group': '1.2', 'city': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²', 'oblast': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð² Ñ–Ð½Ð³ÑƒÐ»ÑŒÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²', 'oblast': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ð¼Ð¸ÐºÐ¾Ð»Ð°Ñ—Ð² ÐºÐ¾Ñ€Ð°Ð±ÐµÐ»ÑŒÐ½Ð¸Ð¹': {'group': '2.2', 'city': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²', 'oblast': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²ÑÑŒÐºÐ°', 'provider': 'ÐœÐ¸ÐºÐ¾Ð»Ð°Ñ—Ð²Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Kherson - all subgroups
    'Ñ…ÐµÑ€ÑÐ¾Ð½ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'Ð¥ÐµÑ€ÑÐ¾Ð½', 'oblast': 'Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°', 'provider': 'Ð¥ÐµÑ€ÑÐ¾Ð½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…ÐµÑ€ÑÐ¾Ð½ ÑƒÑˆÐ°ÐºÐ¾Ð²Ð°': {'group': '1.2', 'city': 'Ð¥ÐµÑ€ÑÐ¾Ð½', 'oblast': 'Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°', 'provider': 'Ð¥ÐµÑ€ÑÐ¾Ð½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'Ñ…ÐµÑ€ÑÐ¾Ð½ Ð´Ð½Ñ–Ð¿Ñ€Ð¾Ð²ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'Ð¥ÐµÑ€ÑÐ¾Ð½', 'oblast': 'Ð¥ÐµÑ€ÑÐ¾Ð½ÑÑŒÐºÐ°', 'provider': 'Ð¥ÐµÑ€ÑÐ¾Ð½Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    
    # Mariupol (DTEK Donetsk region)
    'Ð¼Ð°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'ÐœÐ°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ', 'oblast': 'Ð”Ð¾Ð½ÐµÑ†ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð¾Ð½ÐµÑ†ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    'Ð¼Ð°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ Ð»Ñ–Ð²Ð¾Ð±ÐµÑ€ÐµÐ¶Ð½Ð¸Ð¹': {'group': '2.1', 'city': 'ÐœÐ°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ', 'oblast': 'Ð”Ð¾Ð½ÐµÑ†ÑŒÐºÐ°', 'provider': 'Ð”Ð¢Ð•Ðš Ð”Ð¾Ð½ÐµÑ†ÑŒÐºÑ– ÐµÐ»ÐµÐºÑ‚Ñ€Ð¾Ð¼ÐµÑ€ÐµÐ¶Ñ–'},
    
    # Kremenchuk - all subgroups
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº Ñ†ÐµÐ½Ñ‚Ñ€': {'group': '1.1', 'city': 'ÐšÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ°': {'group': '1.2', 'city': 'ÐšÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
    'ÐºÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº Ð°Ð²Ñ‚Ð¾Ð·Ð°Ð²Ð¾Ð´ÑÑŒÐºÐ¸Ð¹': {'group': '2.1', 'city': 'ÐšÑ€ÐµÐ¼ÐµÐ½Ñ‡ÑƒÐº', 'oblast': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²ÑÑŒÐºÐ°', 'provider': 'ÐŸÐ¾Ð»Ñ‚Ð°Ð²Ð°Ð¾Ð±Ð»ÐµÐ½ÐµÑ€Ð³Ð¾'},
}

# Merge with extended database if available
if UKRAINE_ADDRESSES_DB:
    BLACKOUT_ADDRESSES.update(UKRAINE_ADDRESSES_DB)
    print(f"INFO: Merged addresses database, total: {len(BLACKOUT_ADDRESSES)} addresses")

# Blackout schedules by group and subgroup
# Format: group "1.1", "1.2", "2.1", "2.2", "3.1", "3.2"
# Each subgroup has different timing within main group
# TODO: Fetch from real APIs (DTEK, Ukrenergo)
BLACKOUT_SCHEDULES = {
    # Group 1 subgroups
    '1.1': [
        {'time': '00:00 - 04:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '04:00 - 08:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '16:00 - 20:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '20:00 - 24:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
    ],
    '1.2': [
        {'time': '00:00 - 04:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '04:00 - 08:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '16:00 - 20:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '20:00 - 24:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
    ],
    
    # Group 2 subgroups
    '2.1': [
        {'time': '00:00 - 04:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '04:00 - 08:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '16:00 - 20:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '20:00 - 24:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
    ],
    '2.2': [
        {'time': '00:00 - 04:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '04:00 - 08:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '12:00 - 16:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '16:00 - 20:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '20:00 - 24:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
    ],
    
    # Group 3 subgroups
    '3.1': [
        {'time': '00:00 - 04:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '04:00 - 08:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '16:00 - 20:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '20:00 - 24:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
    ],
    '3.2': [
        {'time': '00:00 - 04:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '04:00 - 08:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '16:00 - 20:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '20:00 - 24:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
    ],
    
    # Fallback for old integer groups (backward compatibility)
    1: [
        {'time': '06:00 - 10:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '10:00 - 14:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '14:00 - 18:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '18:00 - 22:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '22:00 - 02:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
    ],
    2: [
        {'time': '08:00 - 12:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '12:00 - 16:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '16:00 - 20:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '20:00 - 00:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '00:00 - 04:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
    ],
    3: [
        {'time': '04:00 - 08:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'normal'},
        {'time': '08:00 - 12:00', 'label': 'ÐÐºÑ‚Ð¸Ð²Ð½Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'active'},
        {'time': '12:00 - 16:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
        {'time': '16:00 - 20:00', 'label': 'ÐœÐ¾Ð¶Ð»Ð¸Ð²Ðµ Ð²Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ', 'status': 'upcoming'},
        {'time': '20:00 - 00:00', 'label': 'Ð•Ð»ÐµÐºÑ‚Ñ€Ð¾Ð¿Ð¾ÑÑ‚Ð°Ñ‡Ð°Ð½Ð½Ñ', 'status': 'normal'},
    ],
}

@app.route('/api/search_cities')
def search_cities():
    """Get all cities and addresses for autocomplete"""
    try:
        # Collect unique cities
        cities_set = set()
        addresses_list = []
        
        for address_key, data in BLACKOUT_ADDRESSES.items():
            city = data.get('city', '')
            if city:
                cities_set.add(city)
            
            # Parse address_key to extract street and building
            # Format: "city street" or "city street building"
            parts = address_key.split()
            if len(parts) >= 2:
                street = ' '.join(parts[1:])
                addresses_list.append({
                    'city': city,
                    'street': street,
                    'building': '',  # Can be extracted if needed
                    'group': data.get('group', ''),
                    'oblast': data.get('oblast', ''),
                    'provider': data.get('provider', '')
                })
        
        # Convert cities set to sorted list
        cities_list = sorted(list(cities_set))
        
        return jsonify({
            'cities': cities_list,
            'addresses': addresses_list[:200]  # Limit for performance
        })
        
    except Exception as e:
        print(f"ERROR in search_cities: {str(e)}")
        return jsonify({
            'cities': UKRAINE_CITIES if UKRAINE_CITIES else [],
            'addresses': []
        })


@app.route('/api/all_cities_with_queues')
def get_all_cities_with_queues():
    """Get all cities with their queues for the schedule grid"""
    try:
        # Group addresses by city and queue
        cities_data = {}
        
        for address_key, data in BLACKOUT_ADDRESSES.items():
            city = data.get('city', '')
            oblast = data.get('oblast', '')
            queue = data.get('group', '')
            provider = data.get('provider', '')
            
            if not city:
                continue
            
            # Create city key
            city_key = f"{city}, {oblast}"
            
            if city_key not in cities_data:
                cities_data[city_key] = {
                    'city': city,
                    'oblast': oblast,
                    'provider': provider,
                    'queues': set()
                }
            
            if queue:
                cities_data[city_key]['queues'].add(queue)
        
        # Convert to list and format
        result = []
        for city_key, data in cities_data.items():
            queues_list = sorted(list(data['queues']))
            
            # Determine current hour for status
            current_hour = datetime.now(pytz.timezone('Europe/Kiev')).hour
            
            # Check if any queue has active blackout now
            has_active_blackout = False
            active_queues = []
            
            for queue in queues_list:
                schedule = BLACKOUT_SCHEDULES.get(queue, [])
                for slot in schedule:
                    if slot.get('status') == 'active':
                        # Parse time range
                        time_range = slot.get('time', '')
                        if ' - ' in time_range:
                            start_time = time_range.split(' - ')[0]
                            start_hour = int(start_time.split(':')[0])
                            end_hour = (start_hour + 4) % 24
                            
                            if start_hour <= current_hour < end_hour or (end_hour < start_hour and (current_hour >= start_hour or current_hour < end_hour)):
                                has_active_blackout = True
                                active_queues.append(queue)
            
            # Determine status
            if has_active_blackout:
                status = 'active'
                status_text = f"Ð’Ñ–Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ Ñ‡ÐµÑ€Ð³: {', '.join(active_queues)}"
            elif len(queues_list) > 0:
                status = 'warning'
                status_text = f"Ð§ÐµÑ€Ð³Ð¸: {', '.join(queues_list)}"
            else:
                status = 'stable'
                status_text = "Ð¡Ñ‚Ð°Ð±Ñ–Ð»ÑŒÐ½Ð¾"
            
            result.append({
                'city': data['city'],
                'oblast': data['oblast'],
                'provider': data['provider'],
                'queues': queues_list,
                'status': status,
                'statusText': status_text,
                'queuesCount': len(queues_list)
            })
        
        # Sort by city name
        result.sort(key=lambda x: x['city'])
        
        return jsonify({
            'success': True,
            'cities': result,
            'total': len(result)
        })
        
    except Exception as e:
        log.error(f"Error in get_all_cities_with_queues: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/get_schedule')
def get_schedule():
    """Get blackout schedule for a specific address using real geocoding and live DTEK updates"""
    city = request.args.get('city', '').strip()
    street = request.args.get('street', '').strip()
    building = request.args.get('building', '').strip()
    
    if not city:
        return jsonify({'error': 'ÐœÑ–ÑÑ‚Ð¾ Ð¾Ð±Ð¾Ð²\'ÑÐ·ÐºÐ¾Ð²Ðµ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ'}), 400
    
    # Try to use live schedule updater first
    if SCHEDULE_UPDATER_AVAILABLE:
        try:
            # Ensure cache is fresh
            if not schedule_updater.is_cache_valid(max_age_hours=1):
                schedule_updater.update_all_schedules()
            
            # Get schedule from live updater
            result = schedule_updater.get_schedule_for_address(city, street, building)
            
            if result and result.get('schedule'):
                return jsonify({
                    'found': True,
                    'address': f"{city}, {street} {building}".strip(),
                    'city': city,
                    'group': result.get('queue'),
                    'provider': result.get('provider'),
                    'schedule': result.get('schedule'),
                    'last_update': result.get('last_update'),
                    'source': 'live_dtek'
                })
        except Exception as e:
            log.warning(f"Live schedule failed, falling back: {e}")
    
    # Fallback to API client
    if not BLACKOUT_API_AVAILABLE:
        return get_schedule_fallback(city, street, building)
    
    try:
        # Use real API client to get schedule
        result = blackout_client.get_schedule_for_address(city, street, building)
        
        if not result['found']:
            return jsonify({
                'error': f'ÐÐ´Ñ€ÐµÑÑƒ "{city}" Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ñ‚Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ–ÑÑ‚ÑŒ Ð½Ð°Ð¿Ð¸ÑÐ°Ð½Ð½Ñ Ð¼Ñ–ÑÑ‚Ð°.'
            }), 404
        
        # Add oblast info if available
        if 'ÐºÐ¸Ñ—Ð²' in city.lower():
            result['oblast'] = 'ÐšÐ¸Ñ—Ð²ÑÑŒÐºÐ°'
        elif 'Ð¾Ð´ÐµÑ' in city.lower():
            result['oblast'] = 'ÐžÐ´ÐµÑÑŒÐºÐ°'
        elif 'Ñ…Ð°Ñ€ÐºÑ–Ð²' in city.lower():
            result['oblast'] = 'Ð¥Ð°Ñ€ÐºÑ–Ð²ÑÑŒÐºÐ°'
        elif 'Ð´Ð½Ñ–Ð¿Ñ€' in city.lower():
            result['oblast'] = 'Ð”Ð½Ñ–Ð¿Ñ€Ð¾Ð¿ÐµÑ‚Ñ€Ð¾Ð²ÑÑŒÐºÐ°'
        elif 'Ð»ÑŒÐ²Ñ–Ð²' in city.lower():
            result['oblast'] = 'Ð›ÑŒÐ²Ñ–Ð²ÑÑŒÐºÐ°'
        else:
            result['oblast'] = 'Ð£ÐºÑ€Ð°Ñ—Ð½Ð°'
        
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"Error in get_schedule: {e}")
        return jsonify({
            'error': 'ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ– Ð³Ñ€Ð°Ñ„Ñ–ÐºÑƒ. Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ñ‰Ðµ Ñ€Ð°Ð·.'
        }), 500


def get_schedule_fallback(city, street, building):
    """Fallback method using static data if API client is unavailable"""
    city_lower = city.lower()
    street_lower = street.lower() if street else ''
    
    # Try to find matching address in static database
    best_match = None
    best_match_data = None
    best_score = 0
    
    for address_key, data in BLACKOUT_ADDRESSES.items():
        key_parts = address_key.split()
        key_city = key_parts[0] if len(key_parts) > 0 else ''
        key_street = ' '.join(key_parts[1:]) if len(key_parts) > 1 else ''
        
        score = 0
        if city_lower in key_city or key_city in city_lower:
            score += 2
        if street_lower and (street_lower in key_street or key_street in street_lower):
            score += 2
        
        if score > best_score:
            best_score = score
            best_match = address_key
            best_match_data = data
    
    if not best_match_data or best_score < 2:
        return jsonify({'error': f'ÐÐ´Ñ€ÐµÑÑƒ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð´Ð»Ñ {city}. Ð¡Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ñ–Ð½ÑˆÐµ Ð¼Ñ–ÑÑ‚Ð¾ Ð°Ð±Ð¾ Ð²ÑƒÐ»Ð¸Ñ†ÑŽ.'}), 404
    
    # Reconstruct readable address
    parts = best_match.split()
    city_name = parts[0].capitalize()
    street_name = ' '.join(parts[1:]).capitalize() if len(parts) > 1 else ''
    readable_address = f'{city_name}'
    if street_name:
        readable_address += f', {street_name}'
    if building:
        readable_address += f', {building}'
    
    # Get schedule for the group
    group = best_match_data['group']
    schedule = BLACKOUT_SCHEDULES.get(group, [])
    
    return jsonify({
        'address': readable_address,
        'city': best_match_data['city'],
        'oblast': best_match_data['oblast'],
        'group': group,
        'provider': best_match_data['provider'],
        'schedule': schedule
    })


@app.route('/api/live_schedules')
def get_live_schedules():
    """Get live schedules from DTEK and Ukrenergo with automatic hourly updates"""
    try:
        if not SCHEDULE_UPDATER_AVAILABLE:
            return jsonify({
                'error': 'ÐÐ²Ñ‚Ð¾Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ð³Ñ€Ð°Ñ„Ñ–ÐºÑ–Ð² Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ðµ',
                'fallback': True
            }), 503
        
        # Check if cache is still valid
        if not schedule_updater.is_cache_valid(max_age_hours=1):
            log.info("Cache expired, triggering update...")
            schedule_updater.update_all_schedules()
        
        # Get cached schedules
        schedules = schedule_updater.get_cached_schedules()
        
        if not schedules:
            return jsonify({
                'error': 'Ð“Ñ€Ð°Ñ„Ñ–ÐºÐ¸ Ñ‚Ð¸Ð¼Ñ‡Ð°ÑÐ¾Ð²Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ–',
                'retry_after': 60
            }), 503
        
        return jsonify({
            'success': True,
            'schedules': schedules,
            'last_update': schedules.get('last_update'),
            'next_update': 'Ñ‡ÐµÑ€ÐµÐ· Ð³Ð¾Ð´Ð¸Ð½Ñƒ'
        })
        
    except Exception as e:
        log.error(f"Error in get_live_schedules: {e}")
        return jsonify({
            'error': 'ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð³Ñ€Ð°Ñ„Ñ–ÐºÑ–Ð²'
        }), 500


@app.route('/api/schedule_status')
def get_schedule_status():
    """Get status of automatic schedule updates"""
    try:
        if not SCHEDULE_UPDATER_AVAILABLE:
            return jsonify({
                'available': False,
                'message': 'ÐÐ²Ñ‚Ð¾Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ðµ'
            })
        
        last_update = schedule_updater.last_update
        cache_valid = schedule_updater.is_cache_valid()
        
        return jsonify({
            'available': True,
            'last_update': last_update.isoformat() if last_update else None,
            'cache_valid': cache_valid,
            'next_update': 'Ñ‡ÐµÑ€ÐµÐ· Ð³Ð¾Ð´Ð¸Ð½Ñƒ' if cache_valid else 'Ð·Ð°Ñ€Ð°Ð·',
            'scheduler_running': scheduler.running if scheduler_initialized else False,
            'scheduler_initialized': scheduler_initialized
        })
        
    except Exception as e:
        log.error(f"Error in get_schedule_status: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/force_update', methods=['POST'])
def force_schedule_update():
    """Force immediate schedule update (admin only)"""
    try:
        if not SCHEDULE_UPDATER_AVAILABLE:
            return jsonify({
                'success': False,
                'error': 'Schedule updater not available'
            }), 503
        
        log.info("ðŸ”„ Manual schedule update triggered")
        result = schedule_updater.update_all_schedules()
        
        return jsonify({
            'success': True,
            'message': 'Ð“Ñ€Ð°Ñ„Ñ–ÐºÐ¸ ÑƒÑÐ¿Ñ–ÑˆÐ½Ð¾ Ð¾Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾',
            'last_update': schedule_updater.last_update.isoformat() if schedule_updater.last_update else None,
            'data': result is not None
        })
        
    except Exception as e:
        log.error(f"Error in force_schedule_update: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def _prune_comments():
    # keep only last COMMENTS_MAX comments
    global COMMENTS
    if len(COMMENTS) > COMMENTS_MAX:
        COMMENTS = COMMENTS[-COMMENTS_MAX:]

@app.route('/comments', methods=['GET','POST'])
def comments_endpoint():
    """GET returns recent anonymous comments. POST inserts a new one persistently.

    Persistence strategy:
      - Store each comment into SQLite (comments table) with epoch for ordering.
      - Maintain small in-memory tail cache to avoid DB hit storms on rapid polling.
      - On GET always fetch from DB (limit) for durability across redeploys.
    """
    if request.method == 'POST':
        try:
            data = request.get_json(force=True, silent=True) or {}
        except Exception:
            data = {}
        text = (data.get('text') or '').strip()
        if not text:
            return jsonify({'ok': False, 'error': 'empty'}), 400
        reply_to = (data.get('reply_to') or '').strip() or None
        if reply_to and not re.fullmatch(r'[0-9a-fA-F]{6,20}', reply_to):
            reply_to = None  # sanitize unexpected format
        # rudimentary spam / flooding throttles (per-IP simple memory window)
        ip = request.headers.get('X-Forwarded-For', request.remote_addr) or 'unknown'
        now_ts = time.time()
        # simple rate tracker: store recent post times per IP in a module-level dict
        rt = getattr(app, '_comment_rate', None)
        if rt is None:
            rt = {}
            setattr(app, '_comment_rate', rt)
        arr = rt.get(ip, [])
        # drop entries older than 60s
        arr = [t for t in arr if now_ts - t < 60]
        if len(arr) >= 8:  # max 8 comments per minute per IP
            return jsonify({'ok': False, 'error': 'rate_limited'}), 429
        arr.append(now_ts)
        rt[ip] = arr
        # basic length clamp
        if len(text) > 800:
            text = text[:800]
        item = {
            'id': uuid.uuid4().hex[:10],
            'text': text,
            'ts': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),
            'epoch': now_ts,
            'reply_to': reply_to
        }
        cache_item = {k: item[k] for k in ('id','text','ts')}
        if reply_to:
            cache_item['reply_to'] = reply_to
        COMMENTS.append(cache_item)  # store subset in memory cache
        _prune_comments()
        # persist
        save_comment_record(item)
        resp_item = {k: item[k] for k in ('id','text','ts')}
        if reply_to:
            resp_item['reply_to'] = reply_to
        return jsonify({'ok': True, 'item': resp_item})
    # GET
    limit = 80
    rows = load_recent_comments(limit=limit)
    if not rows and COMMENTS:  # fallback to cache if DB query unexpectedly empty
        rows = COMMENTS[-limit:]
    return jsonify({'ok': True, 'items': rows})

@app.route('/comments/react', methods=['POST'])
def comment_react_endpoint():
    """Toggle emoji reactions on comments."""
    try:
        data = request.get_json(force=True, silent=True) or {}
    except Exception:
        return jsonify({'ok': False, 'error': 'invalid_json'}), 400
    
    comment_id = (data.get('comment_id') or '').strip()
    emoji = (data.get('emoji') or '').strip()
    
    # Validation
    if not comment_id or not emoji:
        return jsonify({'ok': False, 'error': 'missing_params'}), 400
        
    # Validate emoji is in allowed list
    allowed_emojis = ['ðŸ‘', 'â¤ï¸', 'ðŸ”¥', 'ðŸ˜¢', 'ðŸ˜¡', 'ðŸ˜‚', 'ðŸ‘Ž']
    if emoji not in allowed_emojis:
        return jsonify({'ok': False, 'error': 'invalid_emoji'}), 400
    
    # Get user IP for uniqueness
    ip = request.headers.get('X-Forwarded-For', request.remote_addr) or 'unknown'
    
    # Rate limiting: max 20 reactions per minute per IP
    now_ts = time.time()
    rt = getattr(app, '_reaction_rate', None)
    if rt is None:
        rt = {}
        setattr(app, '_reaction_rate', rt)
    
    arr = rt.get(ip, [])
    arr = [t for t in arr if now_ts - t < 60]  # Keep last 60 seconds
    if len(arr) >= 20:
        return jsonify({'ok': False, 'error': 'rate_limited'}), 429
    arr.append(now_ts)
    rt[ip] = arr
    
    # Toggle reaction
    result = toggle_comment_reaction(comment_id, emoji, ip)
    
    if result['action'] == 'error':
        return jsonify({'ok': False, 'error': 'server_error'}), 500
    
    return jsonify({
        'ok': True, 
        'action': result['action'],
        'reactions': result['reactions']
    })

@app.route('/active_alarms')
def active_alarms_endpoint():
    """Return current active oblast & raion air alarms (for polygon styling)."""
    
    # Rate limit Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½: Ð²ÑÐµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð¼ÐµÑŽÑ‚ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ñ‹Ð¹ Ð´Ð¾ÑÑ‚ÑƒÐ¿
    
    try:
        now_ep = time.time()
        cutoff = now_ep - APP_ALARM_TTL_MINUTES*60
        for dct in (ACTIVE_OBLAST_ALARMS, ACTIVE_RAION_ALARMS):
            for k in list(dct.keys()):
                if dct[k]['last'] < cutoff:
                    dct.pop(k, None)
        obl_list = []
        for k,v in ACTIVE_OBLAST_ALARMS.items():
            base = k.lower()
            pcode = OBLAST_PCODE.get(base)
            obl_list.append({'name': k, 'since': v['since'], **({'pcode':pcode} if pcode else {})})
        return jsonify({
            'oblasts': obl_list,
            'raions': [{'name': k, 'since': v['since']} for k,v in ACTIVE_RAION_ALARMS.items()],
            'ttl_minutes': APP_ALARM_TTL_MINUTES
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/alarms_stats')
def alarms_stats():
    """Return recent alarm events history (start/cancel/expire) with optional query params:
    ?level=oblast|raion  ?name=<substring>  ?minutes=<window>  ?limit=N
    """
    level_f = request.args.get('level')
    name_sub = (request.args.get('name') or '').lower().strip()
    minutes = int(request.args.get('minutes', '720'))  # default 12h
    limit = int(request.args.get('limit','500'))
    if limit > 2000: limit = 2000
    cutoff = time.time() - minutes*60
    rows = []
    try:
        with _visits_db_conn() as conn:
            q = "SELECT level,name,event,ts FROM alarm_events WHERE ts >= ?"
            params = [cutoff]
            if level_f in ('oblast','raion'):
                q += " AND level = ?"; params.append(level_f)
            if name_sub:
                q += " AND LOWER(name) LIKE ?"; params.append(f"%{name_sub}%")
            q += " ORDER BY ts DESC LIMIT ?"; params.append(limit)
            cur = conn.execute(q, tuple(params))
            for level,name,event,tsv in cur.fetchall():
                rows.append({'level': level, 'name': name, 'event': event, 'ts': tsv})
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    return jsonify({'items': rows, 'count': len(rows), 'window_minutes': minutes})

@app.route('/data')
def data():
    global FALLBACK_REPARSE_CACHE, MAX_REPARSE_CACHE_SIZE
    
    # Rate limit Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½: Ð²ÑÐµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸ Ð¸Ð¼ÐµÑŽÑ‚ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ñ‹Ð¹ Ð´Ð¾ÑÑ‚ÑƒÐ¿
    
    # BANDWIDTH OPTIMIZATION: Add aggressive caching headers  
    response_headers = {
        'Cache-Control': 'public, max-age=60, s-maxage=60',  # Increased cache to 60 seconds
        'ETag': f'data-{int(time.time() // 60)}',  # Cache for 60 seconds
        'Vary': 'Accept-Encoding'
    }
    
    # Check if client has cached version
    client_etag = request.headers.get('If-None-Match')
    if client_etag == response_headers['ETag']:
        return Response(status=304, headers=response_headers)
    
    # Use user-provided timeRange or fall back to global configured MONITOR_PERIOD_MINUTES
    try:
        time_range = int(request.args.get('timeRange', MONITOR_PERIOD_MINUTES))
        # Limit to reasonable values to prevent abuse and reduce bandwidth
        time_range = max(10, min(time_range, 100))  # Reduced from 200 to 100
    except (ValueError, TypeError):
        time_range = MONITOR_PERIOD_MINUTES
    
    print(f"[DEBUG] /data endpoint called with timeRange={request.args.get('timeRange')}, MONITOR_PERIOD_MINUTES={MONITOR_PERIOD_MINUTES}, using time_range={time_range}")
    messages = load_messages()
    print(f"[DEBUG] Loaded {len(messages)} total messages")
    tz = pytz.timezone('Europe/Kyiv')
    now = datetime.now(tz).replace(tzinfo=None)
    min_time = now - timedelta(minutes=time_range)
    print(f"[DEBUG] Filtering messages since {min_time} (last {time_range} minutes)")
    hidden = set(load_hidden())
    out = []  # geo tracks
    events = []  # list-only (alarms, cancellations, other non-geo informational)
    for m in messages:
        try:
            dt = datetime.strptime(m.get('date',''), '%Y-%m-%d %H:%M:%S')
        except Exception:
            continue
        if dt >= min_time:
            # Fallback reparse: if message lacks geo but contains course pattern, try to derive markers now
            txt_low = (m.get('text') or '').lower()
            msg_id = m.get('id')
            
            # Skip multi-regional UAV messages - they're already handled by immediate processing
            text_full = m.get('text') or ''
            text_lines = text_full.split('\n')
            region_count = sum(1 for line in text_lines if any(region in line.lower() for region in ['Ñ‰Ð¸Ð½Ð°:', 'Ñ‰Ð¸Ð½Ð°]', 'Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ:', 'ÐºÑ€Ð°Ð¹:']) or (
                'Ñ‰Ð¸Ð½Ð°' in line.lower() and line.lower().strip().endswith(':')
            ))
            uav_count = sum(1 for line in text_lines if 'Ð±Ð¿Ð»Ð°' in line.lower() and ('ÐºÑƒÑ€Ñ' in line.lower() or 'Ð½Ð° ' in line.lower()))
            
            if (not m.get('lat')) and (not m.get('lng')) and ('Ð±Ð¿Ð»Ð°' in txt_low and 'ÐºÑƒÑ€Ñ' in txt_low and ' Ð½Ð° ' in txt_low):
                # Skip if this is a multi-regional UAV message (already processed immediately)
                if region_count >= 2 and uav_count >= 3:
                    add_debug_log(f"Skipping fallback reparse for multi-regional UAV message ID {msg_id}", "reparse")
                    continue
                    
                # Check if we've already reparsed this message to avoid duplicate processing
                if msg_id in FALLBACK_REPARSE_CACHE:
                    add_debug_log(f"Skipping fallback reparse for message ID {msg_id} - already processed", "reparse")
                    continue
                
                try:
                    # Add to cache to prevent future reprocessing
                    FALLBACK_REPARSE_CACHE.add(msg_id)
                    # Limit cache size to prevent memory growth
                    if len(FALLBACK_REPARSE_CACHE) > MAX_REPARSE_CACHE_SIZE:
                        # Remove oldest half of the cache (approximate LRU)
                        cache_list = list(FALLBACK_REPARSE_CACHE)
                        FALLBACK_REPARSE_CACHE = set(cache_list[len(cache_list)//2:])
                    
                    add_debug_log(f"Fallback reparse for message ID {msg_id} - first time processing", "reparse")
                    reparsed = process_message(m.get('text') or '', m.get('id'), m.get('date'), m.get('channel') or m.get('source') or '')
                    if isinstance(reparsed, list) and reparsed:
                        for t in reparsed:
                            try:
                                lat_r = round(float(t.get('lat')), 3)
                                lng_r = round(float(t.get('lng')), 3)
                            except Exception:
                                continue
                            text_r = (t.get('text') or '')
                            source_r = t.get('channel') or t.get('source') or ''
                            marker_key_r = f"{lat_r},{lng_r}|{text_r}|{source_r}"
                            if marker_key_r in hidden:
                                continue
                            out.append(t)
                        # Skip adding original as event if we produced tracks
                        if reparsed:
                            continue
                except Exception:
                    pass
            # list-only (no coordinates) -> push into events list if not suppressed
            if m.get('list_only'):
                if not m.get('suppress'):
                    events.append(m)
                continue  # skip trying to interpret as marker
            # build marker key similar to frontend hide logic (rounded lat/lng + text + source/channel)
            try:
                lat = round(float(m.get('lat')), 3)
                lng = round(float(m.get('lng')), 3)
            except Exception:
                continue  # not a proper geo marker
            text = (m.get('text') or '')
            source = m.get('source') or m.get('channel') or ''
            marker_key = f"{lat},{lng}|{text}|{source}"
            if marker_key in hidden:
                continue
            # Backward compatibility: allow prefix match (text truncated when stored) for same lat,lng,source
            base_prefix = f"{lat},{lng}|"
            if not any(h.startswith(base_prefix) for h in hidden if '|' in h):
                pass
            else:
                # iterate candidates with same coords and source, compare text prefix
                skip = False
                for h in hidden:
                    if not h.startswith(base_prefix):
                        continue
                    try:
                        _, htext, hsource = h.split('|',2)
                    except ValueError:
                        continue
                    if hsource == source and text.startswith(htext):
                        skip = True
                        break
                if skip:
                    continue
            # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€: ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸ Ð±ÐµÐ· ÑÐ²Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð² ÑƒÐ³Ñ€Ð¾Ð· (Ð¼Ð¾Ð³Ð»Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒÑÑ ÑÑ‚Ð°Ñ€Ñ‹Ð¼Ð¸ Ð²ÐµÑ€ÑÐ¸ÑÐ¼Ð¸ Ð»Ð¾Ð³Ð¸ÐºÐ¸)
            low_txt = text.lower()
            if m.get('source_match','').startswith('region') and not any(k in low_txt for k in ['Ð±Ð¿Ð»Ð°','Ð´Ñ€Ð¾Ð½','ÑˆÐ°Ñ…ÐµÐ´','shahed','geran','Ñ€Ð°ÐºÐµÑ‚Ð°','Ñ€Ð°ÐºÐµÑ‚Ð¸','missile','iskander','s-300','s300','ÐºÐ°Ð±','Ð°Ñ€Ñ‚Ð¸Ð»','Ð³Ñ€Ð°Ð´Ð°','ÑÐ¼ÐµÑ€Ñ‡','ÑƒÑ€Ð°Ð³Ð°Ð½','mlrs','avia','Ð°Ð²Ñ–Ð°','Ð°Ð²Ð¸Ð°','Ð±Ð¾Ð¼Ð±Ð°']):
                continue
            out.append(m)
    # Sort events by time desc (latest first) like markers implicitly (messages stored chronological)
    try:
        events.sort(key=lambda x: x.get('date',''), reverse=True)
    except Exception:
        pass
    
    print(f"[DEBUG] Returning {len(out)} tracks and {len(events)} events")
    
    # BANDWIDTH OPTIMIZATION: Minimize response size and add caching
    response_data = {
        'tracks': out[:100],  # Limit to 100 tracks max to reduce bandwidth
        'events': events[:20],  # Limit to 20 events max
        'all_sources': CHANNELS[:10],  # Limit sources
        'trajectories': []
    }
    
    resp = jsonify(response_data)
    # Add aggressive caching headers to reduce bandwidth
    resp.headers.update(response_headers)
    return resp

@app.route('/channels')
def list_channels():
    return jsonify({'channels': CHANNELS, 'invalid': list(INVALID_CHANNELS)})

@app.route('/debug_parse', methods=['POST'])
def debug_parse():
    """Ad-hoc debugging endpoint to inspect parser output for a stored message or raw text.

    POST JSON:
      {"id": <message_id>}  -> reparse stored message text
      or
      {"text": "raw message text", "channel": "optional", "date": "YYYY-MM-DD HH:MM:SS"}

    Response: { ok: bool, source: 'stored'|'raw', message: {...original message fields subset...}, tracks: [...], count: N }
    """
    try:
        payload = request.get_json(force=True, silent=True) or {}
    except Exception:
        try:
            payload = request.get_json(silent=True) or {}
        except Exception:
            payload = {}
    mid = payload.get('id')
    raw_text = payload.get('text')
    # Allow base64-encoded text to avoid client console encoding corruption
    if not raw_text:
        b64_txt = payload.get('b64') or payload.get('b64_text') or None
        if b64_txt:
            try:
                import base64
                raw_text = base64.b64decode(b64_txt).decode('utf-8', errors='replace')
            except Exception:
                raw_text = ''
    channel = payload.get('channel') or ''
    date_str = payload.get('date') or datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
    src = 'raw'
    if mid and not raw_text:
        # look up stored messages
        try:
            for m in load_messages():
                if str(m.get('id')) == str(mid):
                    raw_text = m.get('text') or ''
                    channel = m.get('channel') or m.get('source') or channel
                    date_str = m.get('date') or date_str
                    src = 'stored'
                    break
        except Exception:
            pass
    if not raw_text:
        return jsonify({'ok': False, 'error': 'no_text_provided'}), 400
    try:
        tracks = process_message(raw_text, str(mid) if mid else 'debug', date_str, channel)
    except Exception as e:
        return jsonify({'ok': False, 'error': f'parse_error: {e}'}), 500
    return jsonify({
        'ok': True,
        'source': src,
        'message': {
            'id': mid,
            'channel': channel,
            'date': date_str,
            'text': raw_text[:2000]
        },
        'count': len(tracks) if isinstance(tracks, list) else 0,
        'tracks': tracks if isinstance(tracks, list) else []
    })

@app.route('/test_parse')
def test_parse():
    """Test endpoint to manually test message parsing without auth."""
    test_message = "Ð§ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²Ñ‰Ð¸Ð½Ð°: 1 Ð‘Ð¿Ð›Ð Ð½Ð° ÐšÐ¾Ð·ÐµÐ»ÐµÑ†ÑŒ 1 Ð‘Ð¿Ð›Ð Ð½Ð° ÐÐ¾ÑÑ–Ð²ÐºÑƒ 1 Ð‘Ð¿Ð›Ð Ð½ÐµÐ¿Ð¾Ð´Ð°Ð»Ñ–Ðº Ð†Ñ‡Ð½Ñ– 2 Ð‘Ð¿Ð›Ð Ð½Ð° ÐšÑƒÐ»Ð¸ÐºÑ–Ð²ÐºÑƒ 2 Ð‘Ð¿Ð›Ð Ð¼Ñ–Ð¶ ÐšÐ¾Ñ€ÑŽÐºÑ–Ð²ÐºÐ¾ÑŽ Ñ‚Ð° ÐœÐµÐ½Ð¾ÑŽ Ð¡ÑƒÐ¼Ñ‰Ð¸Ð½Ð°: 3 Ð‘Ð¿Ð›Ð Ð² Ñ€Ð°Ð¹Ð¾Ð½Ñ– ÐšÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿Ñƒ ã…¤ âž¡ÐŸÑ–Ð´Ð¿Ð¸ÑÐ°Ñ‚Ð¸ÑÑ"
    
    try:
        print("="*50)
        print("MANUAL TEST STARTED")
        print("="*50)
        tracks = process_message(test_message, 'TEST_1', '2025-09-05 17:20:00', 'test')
        print("="*50)
        print("MANUAL TEST COMPLETED")
        print("="*50)
        
        return jsonify({
            'success': True,
            'message': test_message,
            'tracks_count': len(tracks) if tracks else 0,
            'tracks': tracks,
            'test_time': datetime.now().isoformat()
        })
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"ERROR in test_parse: {error_details}")
        return jsonify({
            'success': False,
            'error': str(e),
            'traceback': error_details
        }), 500
    """Locate a settlement or raion by name. Query param: q=<name>
    Returns: {status:'ok', name, lat, lng, source:'dict'|'geocode'|'fallback'} or {status:'not_found'}
    Lightweight normalization reusing UA_CITY_NORMALIZE and CITY_COORDS. Falls back to ensure_city_coords (may geocode if key allowed).
    """
    q = (request.args.get('q') or '').strip()
    if not q:
        return jsonify({'status':'empty'}), 400
    raw = q.lower()
    # Basic cleanup similar to parser's normalization
    raw = re.sub(r'["`Ê¼â€™\'".,:;()]+','', raw)
    raw = re.sub(r'\s+',' ', raw)
    # Try direct dict match
    key = raw
    if key in UA_CITY_NORMALIZE:
        key = UA_CITY_NORMALIZE[key]
    # Heuristic accusative -> nominative (simple feminine endings) if still not found
    if key not in CITY_COORDS and len(key) > 4 and key.endswith(('Ñƒ','ÑŽ')):
        alt = key[:-1] + 'Ð°'
        if alt in CITY_COORDS:
            key = alt
    # Direct dictionary coordinate fetch
    if key in CITY_COORDS:
        lat,lng = CITY_COORDS[key]
        return jsonify({'status':'ok','name':key.title(),'lat':lat,'lng':lng,'source':'dict'})
    # Check full settlements index (all cities/villages loaded from external file)
    if 'SETTLEMENTS_INDEX' in globals() and key in SETTLEMENTS_INDEX:
        lat,lng = SETTLEMENTS_INDEX[key]
        return jsonify({'status':'ok','name':key.title(),'lat':lat,'lng':lng,'source':'settlement'})
    # If not exact, attempt prefix suggestions for UI autocomplete
    if 'SETTLEMENTS_INDEX' in globals() and len(key) >= 3:
        pref = key
        matches = [n for n in SETTLEMENTS_INDEX.keys() if n.startswith(pref)][:15]
        if not matches and pref.endswith(('Ñƒ','ÑŽ')):
            pref2 = pref[:-1] + 'Ð°'
            matches = [n for n in SETTLEMENTS_INDEX.keys() if n.startswith(pref2)][:15]
        if matches:
            return jsonify({'status':'suggest','query':q,'matches':matches})
    # Attempt dynamic ensure (geocode) unless negative cache prohibits
    coords = None
    try:
        coords = ensure_city_coords(key)
    except Exception:
        coords = None
    if coords:
        lat,lng = coords
        return jsonify({'status':'ok','name':key.title(),'lat':lat,'lng':lng,'source':'geocode'})
    return jsonify({'status':'not_found','query':q}), 404

@app.route('/add_channel', methods=['POST'])
def add_channel():
    """Add a channel username or numeric ID at runtime.
    Body JSON: {"id": "-1001234567890", "secret": "..."}
    Requires AUTH_SECRET match if set.
    Persists into channels_dynamic.json and updates global list.
    """
    if AUTH_SECRET and request.json.get('secret') != AUTH_SECRET:
        return jsonify({'status':'error','error':'unauthorized'}), 403
    cid = str(request.json.get('id','')).strip()
    if not cid:
        return jsonify({'status':'error','error':'empty_id'}), 400
    global CHANNELS
    # Normalize removing leading @ or https link wrappers
    cid = cid.replace('https://t.me/','').replace('t.me/','')
    # Remove joinchat pattern if present (cannot directly fetch by invite hash)
    if cid.startswith('+'):
        # Cannot use invite hash directly; require numeric ID user already joined from session
        return jsonify({'status':'error','error':'invite_link_not_supported_use_numeric_id'}), 400
    if cid not in CHANNELS:
        CHANNELS.append(cid)
        # Persist dynamic list excluding originals from env for clarity
        orig_env = os.getenv('TELEGRAM_CHANNELS', '').split(',') if os.getenv('TELEGRAM_CHANNELS') else []
        dynamic_part = [c for c in CHANNELS if c.strip() and c.strip() not in orig_env]
        save_dynamic_channels(dynamic_part)
        log.info(f'Added channel {cid}. Total now {len(CHANNELS)}')
        return jsonify({'status':'ok','added':cid,'total':len(CHANNELS)})
    return jsonify({'status':'ok','added':False,'message':'exists','total':len(CHANNELS)})

# ---------------- Manual marker management -----------------
@app.route('/admin/add_manual_marker', methods=['POST'])
def admin_add_manual_marker():
    """Add a manual marker via admin panel.
    JSON body: {"lat":..., "lng":..., "text":"...", "place":"...", "threat_type":"shahed", "icon":"optional.png"}
    Requires secret if configured.
    """
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    payload = request.get_json(silent=True) or {}
    try:
        lat = float(payload.get('lat'))
        lng = float(payload.get('lng'))
        if not (43 <= lat <= 53.8 and 21 <= lng <= 41.5):
            raise ValueError('out_of_bounds')
        text = (payload.get('text') or '').strip()
        if not text:
            raise ValueError('empty_text')
        place = (payload.get('place') or '').strip()
        threat_type = (payload.get('threat_type') or '').strip().lower() or 'manual'
        allowed_types = {'shahed','raketa','avia','pvo','vibuh','alarm','alarm_cancel','mlrs','artillery','obstril','fpv','pusk','manual'}
        if threat_type not in allowed_types:
            threat_type = 'manual'
        icon = (payload.get('icon') or '').strip()
        now_dt = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')
        mid = 'manual-' + uuid.uuid4().hex[:12]
        messages = load_messages()
        # Build message dict similar to parsed messages
        msg = {
            'id': mid,
            'date': now_dt,
            'text': text,
            'place': place,
            'lat': round(lat, 6),
            'lng': round(lng, 6),
            'threat_type': threat_type,
            'marker_icon': icon or None,
            'manual': True,
            'channel': 'manual',
            'source': 'manual'
        }
        messages.append(msg)
        save_messages(messages)
        return jsonify({'status':'ok','id':mid})
    except Exception as e:
        return jsonify({'status':'error','error':str(e)}), 400

@app.route('/admin/markers')
def admin_markers():
    """API endpoint to get recent markers for admin map"""
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    
    all_msgs = load_messages()
    # Get recent markers (exclude pending geo)
    recent_markers = [m for m in reversed(all_msgs) if m.get('lat') and m.get('lng') and not m.get('pending_geo')][:120]
    
    return jsonify({
        'status': 'ok',
        'markers': recent_markers,
        'count': len(recent_markers)
    })

@app.route('/admin/raw_msgs')
def admin_raw_msgs():
    """API endpoint to get raw messages (pending geo) for admin panel"""
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    
    all_msgs = load_messages()
    raw_msgs = [m for m in reversed(all_msgs) if m.get('pending_geo')][:100]  # latest 100
    raw_count = len([m for m in all_msgs if m.get('pending_geo')])
    
    return jsonify({
        'status': 'ok',
        'raw_msgs': raw_msgs,
        'raw_count': raw_count
    })

@app.route('/admin/delete_manual_marker', methods=['POST'])
def admin_delete_manual_marker():
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    payload = request.get_json(silent=True) or {}
    mid = (payload.get('id') or '').strip()
    if not mid:
        return jsonify({'status':'error','error':'missing id'}), 400
    try:
        messages = load_messages()
        new_list = [m for m in messages if not (m.get('manual') and m.get('id') == mid)]
        if len(new_list) == len(messages):
            return jsonify({'status':'ok','deleted':False})
        save_messages(new_list)
        return jsonify({'status':'ok','deleted':True})
    except Exception as e:
        return jsonify({'status':'error','error':str(e)}), 500

@app.route('/hide_marker', methods=['POST'])
def hide_marker():
    """Store a marker key so it's excluded from subsequent /data responses."""
    try:
        payload = request.get_json(force=True) or {}
        lat = round(float(payload.get('lat')), 3)
        lng = round(float(payload.get('lng')), 3)
        text = (payload.get('text') or '').strip()
        source = (payload.get('source') or '').strip()
        marker_key = f"{lat},{lng}|{text}|{source}"
        hidden = load_hidden()
        if marker_key not in hidden:
            hidden.append(marker_key)
            save_hidden(hidden)
        return jsonify({'status':'ok','hidden_count':len(hidden)})
    except Exception as e:
        log.warning(f"hide_marker error: {e}")
        return jsonify({'status':'error','error':str(e)}), 400

@app.route('/unhide_marker', methods=['POST'])
def unhide_marker():
    """Remove previously hidden marker by key or by lat,lng plus text/source prefix match."""
    try:
        payload = request.get_json(force=True) or {}
        key = (payload.get('key') or '').strip()
        hidden = load_hidden()
        changed = False
        if key:
            if key.isdigit():
                idx = int(key)
                if 0 <= idx < len(hidden):
                    del hidden[idx]
                    changed = True
            elif key in hidden:
                hidden.remove(key)
                changed = True
        else:
            lat = payload.get('lat')
            lng = payload.get('lng')
            text = (payload.get('text') or '').strip()
            source = (payload.get('source') or '').strip()
            if lat is not None and lng is not None:
                try:
                    lat_r = round(float(lat), 3)
                    lng_r = round(float(lng), 3)
                except Exception:
                    lat_r = lng_r = None
                base_prefix = f"{lat_r},{lng_r}|" if lat_r is not None else None
                if base_prefix:
                    for h in list(hidden):
                        if not h.startswith(base_prefix):
                            continue
                        try:
                            _, htext, hsource = h.split('|', 2)
                        except ValueError:
                            continue
                        if source and hsource != source:
                            continue
                        if not text or htext.startswith(text) or text.startswith(htext):
                            hidden.remove(h)
                            changed = True
        if changed:
            save_hidden(hidden)
        else:
            log.info(f"unhide_marker: no change for key='{key}' payload={payload}")
        return jsonify({'status': 'ok', 'removed': changed, 'remaining': len(hidden)})
    except Exception as e:
        log.warning(f"unhide_marker error: {e}")
        return jsonify({'status': 'error', 'error': str(e)}), 400

# Guard against duplicate registration if this file is imported twice or a previous
# health endpoint already exists (avoids Flask AssertionError: overwriting endpoint)
if 'health' not in app.view_functions:
    @app.route('/health')
    def health():  # type: ignore
        now = time.time()
        
        # Basic stats + prune visitors
        with ACTIVE_LOCK:
            for vid, meta in list(ACTIVE_VISITORS.items()):
                ts = meta if isinstance(meta,(int,float)) else meta.get('ts',0)
                if now - ts > ACTIVE_TTL:
                    del ACTIVE_VISITORS[vid]
            visitors = len(ACTIVE_VISITORS)
        return jsonify({'status':'ok','messages':len(load_messages()), 'auth': AUTH_STATUS, 'visitors': visitors})

@app.route('/ads.txt')
def ads_txt():
    """Redirect to Ezoic ads.txt manager for automatic updates"""
    from flask import redirect
    return redirect('https://srv.adstxtmanager.com/19390/neptun.in.ua', code=301)

@app.route('/robots.txt')
def robots_txt():
    """Serve robots.txt for search engines"""
    return send_from_directory('static', 'robots.txt', mimetype='text/plain')

@app.route('/sitemap.xml')
def sitemap_xml():
    """Serve sitemap.xml for search engines"""
    return send_from_directory('static', 'sitemap.xml', mimetype='application/xml')

@app.route('/presence', methods=['POST'])
def presence():
    # Rate limiting removed
    data = request.get_json(silent=True) or {}
    vid = data.get('id')
    client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', request.remote_addr)
    print(f"[DEBUG] /presence called with id={vid} from IP {client_ip}")
    if not vid:
        return jsonify({'status':'error','error':'id required'}), 400
    now = time.time()
    blocked = set(load_blocked())
    if vid in blocked:
        return jsonify({'status':'blocked'})
    remote_ip = request.headers.get('X-Forwarded-For', request.remote_addr or '')
    ua = request.headers.get('User-Agent', '')[:300]
    # Load stats and register first-seen if new
    stats = _load_visit_stats()
    if vid not in stats:
        stats[vid] = now
        # prune occasionally (1/200 probability)
        if int(now) % 200 == 0:
            _prune_visit_stats()
        _save_visit_stats()
    # Update rolling today/week stats (independent persistence)
    try:
        _update_recent_visits(vid)
    except Exception as e:
        log.warning(f"recent visits update failed: {e}")
    # Record in SQLite persistent store
    try:
        record_visit_sql(vid, now)
    except Exception:
        pass
    # Query DB for persistent first_seen to avoid resetting session on process restart
    db_first = None
    try:
        with _visits_db_conn() as conn:
            cur = conn.execute("SELECT first_seen FROM visits WHERE id=?", (vid,))
            row = cur.fetchone()
            if row and row[0]:
                try:
                    db_first = float(row[0])
                except Exception:
                    db_first = None
    except Exception:
        pass
    with ACTIVE_LOCK:
        prev = ACTIVE_VISITORS.get(vid) if isinstance(ACTIVE_VISITORS.get(vid), dict) else {}
        first_seen = prev.get('first') or db_first or stats.get(vid) or now
        ACTIVE_VISITORS[vid] = {'ts': now, 'first': first_seen, 'ip': remote_ip, 'ua': prev.get('ua') or ua}
        # prune
        for k, meta in list(ACTIVE_VISITORS.items()):
            ts = meta if isinstance(meta,(int,float)) else meta.get('ts',0)
            if now - ts > ACTIVE_TTL:
                del ACTIVE_VISITORS[k]
        count = len(ACTIVE_VISITORS)
    print(f"[DEBUG] Returning visitors count: {count}")
    return jsonify({'status':'ok','visitors': count})
def presence():
    # CRITICAL BANDWIDTH PROTECTION: Rate limit presence endpoint
    client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', request.remote_addr)
    presence_requests = request_counts.get(f"{client_ip}_presence", [])
    now_time = time.time()
    
    # Clean old requests (last 120 seconds - allow every 2 minutes)
    presence_requests = [req_time for req_time in presence_requests if now_time - req_time < 120]
    
    # Allow only 1 presence request per 2 minutes per IP
    if len(presence_requests) >= 1:
        return jsonify({'error': 'Presence endpoint rate limited - wait 2 minutes'}), 429
    
    presence_requests.append(now_time)
    request_counts[f"{client_ip}_presence"] = presence_requests
    
    data = request.get_json(silent=True) or {}
    vid = data.get('id')
    print(f"[DEBUG] /presence called with id={vid} from IP {client_ip}")
    if not vid:
        return jsonify({'status':'error','error':'id required'}), 400
    now = time.time()
    blocked = set(load_blocked())
    if vid in blocked:
        return jsonify({'status':'blocked'})
    remote_ip = request.headers.get('X-Forwarded-For', request.remote_addr or '')
    ua = request.headers.get('User-Agent', '')[:300]
    # Load stats and register first-seen if new
    stats = _load_visit_stats()
    if vid not in stats:
        stats[vid] = now
        # prune occasionally (1/200 probability)
        if int(now) % 200 == 0:
            _prune_visit_stats()
        _save_visit_stats()
    # Update rolling today/week stats (independent persistence)
    try:
        _update_recent_visits(vid)
    except Exception as e:
        log.warning(f"recent visits update failed: {e}")
    # Record in SQLite persistent store
    try:
        record_visit_sql(vid, now)
    except Exception:
        pass
    # Query DB for persistent first_seen to avoid resetting session on process restart
    db_first = None
    try:
        with _visits_db_conn() as conn:
            cur = conn.execute("SELECT first_seen FROM visits WHERE id=?", (vid,))
            row = cur.fetchone()
            if row and row[0]:
                try:
                    db_first = float(row[0])
                except Exception:
                    db_first = None
    except Exception:
        pass
    with ACTIVE_LOCK:
        prev = ACTIVE_VISITORS.get(vid) if isinstance(ACTIVE_VISITORS.get(vid), dict) else {}
        first_seen = prev.get('first') or db_first or stats.get(vid) or now
        ACTIVE_VISITORS[vid] = {'ts': now, 'first': first_seen, 'ip': remote_ip, 'ua': prev.get('ua') or ua}
        # prune
        for k, meta in list(ACTIVE_VISITORS.items()):
            ts = meta if isinstance(meta,(int,float)) else meta.get('ts',0)
            if now - ts > ACTIVE_TTL:
                del ACTIVE_VISITORS[k]
        count = len(ACTIVE_VISITORS)
    print(f"[DEBUG] Returning visitors count: {count}")
    return jsonify({'status':'ok','visitors': count})

@app.route('/raion_alarms')
def raion_alarms():
    # ...existing code...
    
    # Expose current active district air alarms
    out = []
    now = time.time()
    for key, info in list(RAION_ALARMS.items()):
        # Optional expiry cleanup (e.g., 3h stale auto-clear)
        if now - info.get('since', now) > 3*3600:
            RAION_ALARMS.pop(key, None)
            continue
        out.append({
            'raion': key,
            'place': info['place'],
            'lat': info['lat'],
            'lng': info['lng'],
            'since': info['since']
        })
    return jsonify({'alarms': out, 'count': len(out)})

# SSE stream endpoint
@app.route('/stream')
def stream():
    def gen():
        q = queue.Queue()
        SUBSCRIBERS.add(q)
        last_ping = time.time()
        try:
            while True:
                try:
                    item = q.get(timeout=5)
                    yield f'data: {item}\n\n'
                except Exception:
                    pass
                now_t = time.time()
                if now_t - last_ping > 25:
                    last_ping = now_t
                    yield ': ping\n\n'
        except GeneratorExit:
            pass
        finally:
            SUBSCRIBERS.discard(q)
    headers = {
        'Cache-Control': 'no-store',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no'
    }
    return Response(gen(), mimetype='text/event-stream', headers=headers)

def broadcast_new(tracks):
    """Send new geo tracks to all connected SSE subscribers."""
    if not tracks:
        return
    payload = json.dumps({'tracks': tracks}, ensure_ascii=False)
    dead = []
    for q in list(SUBSCRIBERS):
        try:
            q.put_nowait(payload)
        except Exception:
            dead.append(q)
    for d in dead:
        SUBSCRIBERS.discard(d)
def broadcast_control(event:dict):
    try:
        payload = json.dumps({'control': event}, ensure_ascii=False)
    except Exception:
        return
    dead = []
    for q in list(SUBSCRIBERS):
        try:
            q.put_nowait(payload)
        except Exception:
            dead.append(q)

# ---------------- Admin & blocking endpoints -----------------
def _require_secret(req):
    if not AUTH_SECRET:
        return True
    supplied = req.args.get('secret') or req.headers.get('X-Auth-Secret') or req.form.get('secret')
    return supplied and supplied == AUTH_SECRET

@app.route('/version')
def version_check():
    return {'version': '2024-12-06-oblast-raion-fix', 'timestamp': time.time()}

@app.route('/test_oblast_raion')
def test_oblast_raion():
    if not _require_secret(request):
        return Response('Forbidden', status=403)
    
    test_text = "Ð—Ð°Ð³Ñ€Ð¾Ð·Ð° Ð·Ð°ÑÑ‚Ð¾ÑÑƒÐ²Ð°Ð½Ð½Ñ Ð‘ÐŸÐ›Ð. ÐŸÐµÑ€ÐµÐ¹Ð´Ñ–Ñ‚ÑŒ Ð² ÑƒÐºÑ€Ð¸Ñ‚Ñ‚Ñ! | Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ñ‡ÐµÑ€Ð½Ñ–Ð³Ñ–Ð²ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½), ÐºÐ¸Ñ—Ð²ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (Ð²Ð¸ÑˆÐ³Ð¾Ñ€Ð¾Ð´ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½), ÑÑƒÐ¼ÑÑŒÐºÐ° Ð¾Ð±Ð»Ð°ÑÑ‚ÑŒ (ÑÑƒÐ¼ÑÑŒÐºÐ¸Ð¹, ÐºÐ¾Ð½Ð¾Ñ‚Ð¾Ð¿ÑÑŒÐºÐ¸Ð¹ Ñ€Ð°Ð¹Ð¾Ð½Ð¸) - Ð·Ð°Ð³Ñ€Ð¾Ð·Ð° ÑƒÐ´Ð°Ñ€Ð½Ð¸Ñ… Ð±Ð¿Ð»Ð°!"
    result = process_message(test_text, 'test_99999', '2024-12-06', 'test_channel')
    
    return {
        'test_text': test_text,
        'result': result,
        'debug_logs': [log for log in DEBUG_LOGS if log.get('category') == 'oblast_raion'][-10:]
    }

@app.route('/test-pusk')
def test_pusk_icon():
    """Test route to debug pusk.png display issues"""
    with open('/Users/vladimirmalik/Desktop/render2/test_pusk_icon.html', 'r', encoding='utf-8') as f:
        return f.read()

@app.route('/admin')
def admin_panel():
    if not _require_secret(request):
        return Response('Forbidden', status=403)
    # Ensure rolling recent visits file is seeded from durable SQLite (survives redeploy)
    try:
        _seed_recent_from_sql()
    except Exception:
        pass
    now = time.time()
    # Merge ACTIVE_VISITORS volatile data with persistent DB to avoid session age resets on restart
    # Strategy: build dict from DB for active window; overlay runtime (for ip/ua freshness)
    db_active = {s['id']: s for s in _active_sessions_from_db(ACTIVE_TTL)}
    with ACTIVE_LOCK:
        visitors = []
        for vid, meta in ACTIVE_VISITORS.items():
            if isinstance(meta,(int,float)):
                mem_first = meta
                mem_last = meta
                db_sess = db_active.get(vid)
                if db_sess:
                    first_ts = db_sess.get('first') or mem_first
                    last_ts = db_sess.get('last') or mem_last
                else:
                    first_ts = mem_first
                    last_ts = mem_last
            else:
                mem_first = meta.get('first') or meta.get('ts', now)
                mem_last = meta.get('ts', mem_first)
                db_sess = db_active.get(vid)
                if db_sess:
                    # Use earlier first (older session start) and later last (most recent activity)
                    first_ts = min(mem_first, db_sess.get('first') or mem_first)
                    last_ts = max(mem_last, db_sess.get('last') or mem_last)
                else:
                    first_ts, last_ts = mem_first, mem_last
            if first_ts > last_ts:
                first_ts, last_ts = last_ts, first_ts
            sess_age = int(now - first_ts)
            idle_age = int(now - last_ts)
            ua = (meta.get('ua') if isinstance(meta, dict) else '') or ''
            ip = (meta.get('ip') if isinstance(meta, dict) else '') or ''
            visitors.append({
                'id': vid,
                'ip': ip,
                'age': sess_age,
                'age_fmt': _fmt_age(sess_age),
                'ua': ua,
                'ua_short': _ua_label(ua) if ua else '',
                'last_seen': _fmt_age(idle_age)
            })
    blocked = load_blocked()
    # Load raw (pending geo) messages
    all_msgs = load_messages()
    raw_msgs = [m for m in reversed(all_msgs) if m.get('pending_geo')][:100]  # latest 100
    # Collect last N geo markers (exclude pending geo) for hide management
    recent_markers = [m for m in reversed(all_msgs) if m.get('lat') and m.get('lng') and not m.get('pending_geo')][:120]
    # --- Visit stats aggregation (prefer durable SQLite to survive redeploy) ---
    daily_unique, week_unique = sql_unique_counts()
    if daily_unique is None:
        # fallback to rolling sets file if DB unavailable
        daily_unique, week_unique = _recent_counts()
    if daily_unique is None:  # final fallback to json stats
        stats = _load_visit_stats()
        tz = pytz.timezone('Europe/Kyiv')
        now_dt = datetime.now(tz)
        today_str = now_dt.strftime('%Y-%m-%d')
        week_cut = now_dt - timedelta(days=7)
        daily_unique = 0
        week_unique = 0
        for vid, ts in stats.items():
            try:
                tsf = float(ts)
            except Exception:
                continue
            dt = datetime.fromtimestamp(tsf, tz)
            if dt.strftime('%Y-%m-%d') == today_str:
                daily_unique += 1
            if dt >= week_cut:
                week_unique += 1
    hidden_keys = load_hidden()
    parsed_hidden = []
    for hk in hidden_keys:
        try:
            coord_part, text_part, source_part = hk.split('|',2)
            lat_str, lng_str = coord_part.split(',',1)
            parsed_hidden.append({'lat':lat_str,'lng':lng_str,'text':text_part,'source':source_part,'key':hk})
        except Exception:
            continue
    return render_template(
        'admin.html',
        visitors=visitors,
        blocked=blocked,
        raw_msgs=raw_msgs,
        raw_count=len([m for m in all_msgs if m.get('pending_geo')]),
        secret=(request.args.get('secret') or ''),
        monitor_period=MONITOR_PERIOD_MINUTES,
        markers=recent_markers,
        daily_unique=daily_unique,
        week_unique=week_unique,
        hidden_markers=parsed_hidden,
        neg_geocode=list(_load_neg_geocode_cache().items())[:150],
        debug_logs=DEBUG_LOGS
    )

@app.route('/admin/clear_debug_logs', methods=['POST'])
def clear_debug_logs():
    if not _require_secret(request):
        return jsonify({'status': 'forbidden'}), 403
    global DEBUG_LOGS
    DEBUG_LOGS.clear()
    return jsonify({'status': 'ok', 'cleared': True})

@app.route('/admin/set_monitor_period', methods=['POST'])
def set_monitor_period():
    if not _require_secret(request):
        return jsonify({'status': 'forbidden'}), 403
    global MONITOR_PERIOD_MINUTES
    payload = request.get_json(silent=True) or request.form
    try:
        val = int(payload.get('value'))
        if not (1 <= val <= 360):
            raise ValueError('out of range')
        MONITOR_PERIOD_MINUTES = val
        save_config()
        print(f"[DEBUG] MONITOR_PERIOD_MINUTES updated to {MONITOR_PERIOD_MINUTES} minutes")
        return jsonify({'status':'ok','monitor_period':MONITOR_PERIOD_MINUTES})
    except Exception as e:
        return jsonify({'status':'error','error':str(e)}), 400

@app.route('/admin/neg_geocode_clear', methods=['POST'])
def admin_neg_geocode_clear():
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    global _neg_geocode_cache
    _neg_geocode_cache = {}
    _save_neg_geocode_cache()
    return jsonify({'status':'ok','cleared':True})

@app.route('/admin/neg_geocode_delete', methods=['POST'])
def admin_neg_geocode_delete():
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    payload = request.get_json(silent=True) or {}
    name = (payload.get('name') or '').strip().lower()
    if not name:
        return jsonify({'status':'error','error':'name required'}),400
    cache = _load_neg_geocode_cache()
    if name in cache:
        del cache[name]
        _save_neg_geocode_cache()
        return jsonify({'status':'ok','deleted':True})
    return jsonify({'status':'error','error':'not found'}),404

@app.route('/admin/stats', methods=['GET'])
def admin_stats():
    """Get comprehensive system statistics for admin dashboard"""
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    
    try:
        all_msgs = load_messages()
        now = time.time()
        tz = pytz.timezone('Europe/Kyiv')
        
        # Message statistics
        total_messages = len(all_msgs)
        pending_geo = len([m for m in all_msgs if m.get('pending_geo')])
        with_coordinates = len([m for m in all_msgs if m.get('lat') and m.get('lng')])
        
        # Recent activity (last 24h)
        cutoff_24h = now - 86400
        recent_msgs = [m for m in all_msgs if _msg_timestamp(m) > cutoff_24h]
        
        # Threat type breakdown
        threat_counts = {}
        for msg in all_msgs:
            if not msg.get('pending_geo') and msg.get('lat') and msg.get('lng'):
                threat_type = msg.get('threat_type', 'unknown')
                threat_counts[threat_type] = threat_counts.get(threat_type, 0) + 1
        
        # System health
        with ACTIVE_LOCK:
            active_users = len(ACTIVE_VISITORS)
        blocked_users = len(load_blocked())
        hidden_markers = len(load_hidden())
        neg_cache_size = len(_load_neg_geocode_cache())
        debug_logs_count = len(DEBUG_LOGS)
        
        return jsonify({
            'status': 'ok',
            'stats': {
                'messages': {
                    'total': total_messages,
                    'pending_geo': pending_geo,
                    'with_coordinates': with_coordinates,
                    'recent_24h': len(recent_msgs)
                },
                'threats': threat_counts,
                'system': {
                    'active_users': active_users,
                    'blocked_users': blocked_users,
                    'hidden_markers': hidden_markers,
                    'neg_cache_size': neg_cache_size,
                    'debug_logs': debug_logs_count,
                    'monitor_period': MONITOR_PERIOD_MINUTES
                },
                'timestamp': now
            }
        })
    except Exception as e:
        return jsonify({'status': 'error', 'error': str(e)}), 500

@app.route('/admin/cleanup', methods=['POST'])
def admin_cleanup():
    """Clean up old data to maintain performance"""
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    
    payload = request.get_json(silent=True) or {}
    days_to_keep = int(payload.get('days', 7))  # Keep last 7 days by default
    
    try:    
        cutoff_time = time.time() - (days_to_keep * 86400)
        
        # Clean old messages
        all_msgs = load_messages()
        old_count = len(all_msgs)
        new_msgs = [m for m in all_msgs if _msg_timestamp(m) > cutoff_time]
        
        # Always keep at least 100 most recent messages
        if len(new_msgs) < 100 and len(all_msgs) >= 100:
            new_msgs = sorted(all_msgs, key=_msg_timestamp, reverse=True)[:100]
        
        save_messages(new_msgs)
        
        # Clean old debug logs (keep last 500)
        global DEBUG_LOGS
        if len(DEBUG_LOGS) > 500:
            DEBUG_LOGS = DEBUG_LOGS[-500:]
        
        # Clean old visitor data from SQLite
        try:
            conn = sqlite3.connect(VISIT_DB_PATH)
            c = conn.cursor()
            c.execute("DELETE FROM visits WHERE first_seen < ?", (cutoff_time,))
            deleted_visits = c.rowcount
            conn.commit()
            conn.close()
        except Exception:
            deleted_visits = 0
        
        return jsonify({
            'status': 'ok',
            'cleaned': {
                'messages': old_count - len(new_msgs),
                'debug_logs': max(0, len(DEBUG_LOGS) - 500),
                'visitor_records': deleted_visits
            },
            'remaining': {
                'messages': len(new_msgs),
                'debug_logs': len(DEBUG_LOGS)
            }
        })
    except Exception as e:
        return jsonify({'status': 'error', 'error': str(e)}), 500

@app.route('/admin/export', methods=['GET'])
def admin_export():
    """Export data for backup/analysis"""
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    
    export_type = request.args.get('type', 'messages')
    
    try:
        if export_type == 'messages':
            all_msgs = load_messages()
            # Remove sensitive data
            clean_msgs = []
            for msg in all_msgs:
                clean_msg = {k: v for k, v in msg.items() if k not in ['id']}
                clean_msgs.append(clean_msg)
            return jsonify({'status': 'ok', 'data': clean_msgs, 'count': len(clean_msgs)})
        
        elif export_type == 'stats':
            with ACTIVE_LOCK:
                active_count = len(ACTIVE_VISITORS)
            
            return jsonify({
                'status': 'ok',
                'data': {
                    'active_users': active_count,
                    'blocked_users': len(load_blocked()),
                    'hidden_markers': len(load_hidden()),
                    'neg_cache_entries': len(_load_neg_geocode_cache()),
                    'debug_logs': len(DEBUG_LOGS),
                    'monitor_period': MONITOR_PERIOD_MINUTES,
                    'export_time': time.time()
                }
            })
        
        else:
            return jsonify({'status': 'error', 'error': 'Invalid export type'}), 400
    
    except Exception as e:
        return jsonify({'status': 'error', 'error': str(e)}), 500
        _save_neg_geocode_cache()
        return jsonify({'status':'ok','removed':name})
    return jsonify({'status':'ok','removed':None})

@app.route('/block', methods=['POST'])
def block_id():
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    payload = request.get_json(silent=True) or request.form
    vid = (payload or {}).get('id')
    if not vid:
        return jsonify({'status':'error','error':'id required'}), 400
    blocked = load_blocked()
    if vid not in blocked:
        blocked.append(vid)
        save_blocked(blocked)
    # push control event so client can self-block immediately
    broadcast_control({'type':'block','id':vid})
    return jsonify({'status':'ok','blocked':blocked})

@app.route('/unblock', methods=['POST'])
def unblock_id():
    if not _require_secret(request):
        return jsonify({'status':'forbidden'}), 403
    payload = request.get_json(silent=True) or request.form
    vid = (payload or {}).get('id')
    if not vid:
        return jsonify({'status':'error','error':'id required'}), 400
    blocked = load_blocked()
    if vid in blocked:
        blocked.remove(vid)
        save_blocked(blocked)
    return jsonify({'status':'ok','blocked':blocked})

def _fmt_age(age_seconds:int)->str:
    # Format seconds to H:MM:SS (or M:SS if <1h)
    if age_seconds < 3600:
        m, s = divmod(age_seconds, 60)
        return f"{m}:{s:02d}"
    h, rem = divmod(age_seconds, 3600)
    m, s = divmod(rem, 60)
    return f"{h}:{m:02d}:{s:02d}"

def _ua_label(ua:str)->str:
    u = ua.lower()
    # Simple detection heuristics
    if 'android' in u:
        if 'wv' in u or 'version/' in u:
            base = 'Android WebView'
        else:
            base = 'Android'
    elif 'iphone' in u or 'ipad' in u or 'ipod' in u:
        base = 'iOS'
    elif 'mac os x' in u and 'mobile' not in u:
        base = 'macOS'
    elif 'windows nt' in u:
        base = 'Windows'
    elif 'linux' in u:
        base = 'Linux'
    else:
        base = 'Other'
    # Browser
    browser = 'Browser'
    if 'chrome/' in u and 'edg/' not in u and 'opr/' not in u:
        browser = 'Chrome'
    elif 'edg/' in u:
        browser = 'Edge'
    elif 'firefox/' in u:
        browser = 'Firefox'
    elif 'safari/' in u and 'chrome/' not in u:
        browser = 'Safari'
    elif 'opr/' in u or 'opera' in u:
        browser = 'Opera'
    return f"{base} {browser}"

def _load_opencage_cache():
    global _opencage_cache
    if _opencage_cache is not None:
        return _opencage_cache
    if os.path.exists(OPENCAGE_CACHE_FILE):
        try:
            with open(OPENCAGE_CACHE_FILE, 'r', encoding='utf-8') as f:
                _opencage_cache = json.load(f)
        except Exception:
            _opencage_cache = {}
    else:
        _opencage_cache = {}
    return _opencage_cache

def _save_opencage_cache():
    if _opencage_cache is None:
        return
    try:
        with open(OPENCAGE_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(_opencage_cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning(f"Failed saving OpenCage cache: {e}")


SETTLEMENTS_FILE = os.getenv('SETTLEMENTS_FILE', 'settlements_ua.json')
SETTLEMENTS_INDEX = {}
SETTLEMENTS_ORDERED = []  # (RAION_FALLBACK consolidated earlier; duplicate definition removed)

# --------------- Optional Git auto-commit settings ---------------
GIT_AUTO_COMMIT = os.getenv('GIT_AUTO_COMMIT', '0') not in ('0','false','False','')
GIT_REPO_SLUG = os.getenv('GIT_REPO_SLUG')  # e.g. 'vavaika22423232/neptun'
GIT_SYNC_TOKEN = os.getenv('GIT_SYNC_TOKEN')  # GitHub PAT (classic or fine-grained) with repo write
GIT_COMMIT_INTERVAL = int(os.getenv('GIT_COMMIT_INTERVAL', '180'))  # seconds between commits
_last_git_commit = 0

# Delay before first Telegram connect (helps Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÑÐµÑ‡ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ€Ð¾Ð³Ð¾ Ð¸ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¸Ð½ÑÑ‚Ð°Ð½ÑÐ° Ð¿Ñ€Ð¸ Ð´ÐµÐ¿Ð»Ð¾Ðµ)
FETCH_START_DELAY = int(os.getenv('FETCH_START_DELAY', '0'))  # seconds

def maybe_git_autocommit():
    """If enabled, commit & push updated messages.json back to GitHub.
    Requirements:
      - Set GIT_AUTO_COMMIT=1
      - Provide GIT_REPO_SLUG (owner/repo)
      - Provide GIT_SYNC_TOKEN (PAT with repo write)
    The container build must include git (Render base images do).
    Commits throttled by GIT_COMMIT_INTERVAL seconds.
    """
    global _last_git_commit
    if not GIT_AUTO_COMMIT or not GIT_REPO_SLUG or not GIT_SYNC_TOKEN:
        return
    now = time.time()
    if now - _last_git_commit < GIT_COMMIT_INTERVAL:
        return
    if not os.path.isdir('.git'):
        raise RuntimeError('Not a git repo')
    # Configure user (once)
    def run(cmd):
        return subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    run('git config user.email "bot@local"')
    run('git config user.name "Auto Sync Bot"')
    # Set remote URL embedding token (avoid logging token!)
    safe_remote = f'https://x-access-token:{GIT_SYNC_TOKEN}@github.com/{GIT_REPO_SLUG}.git'
    # Do not print safe_remote (contains secret)
    # Update origin only if needed
    remotes = run('git remote -v').stdout
    if 'origin' not in remotes or GIT_REPO_SLUG not in remotes:
        run('git remote remove origin')
        run(f'git remote add origin "{safe_remote}"')
    # Stage & commit if there is a change
    run(f'git add {MESSAGES_FILE}')
    status = run('git status --porcelain').stdout
    if MESSAGES_FILE not in status:
        return  # no actual diff
    commit_msg = f'Update {MESSAGES_FILE} (auto)'  # no secrets
    run(f'git commit -m "{commit_msg}"')
    push_res = run('git push origin HEAD:main')
    if push_res.returncode == 0:
        _last_git_commit = now
    else:
        # If push fails (e.g., diverged), attempt pull+rebase then push
        run('git fetch origin')
        run('git rebase origin/main || git rebase --abort')
        push_res2 = run('git push origin HEAD:main')
        if push_res2.returncode == 0:
            _last_git_commit = now
        # else: give up silently to avoid spamming logs

def _load_settlements():
    global SETTLEMENTS_INDEX, SETTLEMENTS_ORDERED
    if not os.path.exists(SETTLEMENTS_FILE):
        return
    try:
        with open(SETTLEMENTS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # Expect list of dicts with keys: name, lat, lng (or lon)
        for item in data:
            try:
                name = item.get('name') or item.get('n')
                if not name:
                    continue
                lat = float(item.get('lat'))
                lng = float(item.get('lng') or item.get('lon'))
                key = name.strip().lower()
                if key and key not in SETTLEMENTS_INDEX:
                    SETTLEMENTS_INDEX[key] = (lat, lng)
            except Exception:
                continue
        # Order names by length descending to match longer first (avoids partial overshadowing)
        SETTLEMENTS_ORDERED = sorted(SETTLEMENTS_INDEX.keys(), key=len, reverse=True)[:50000]  # hard cap
        log.info(f'Loaded settlements: {len(SETTLEMENTS_INDEX)} (using top {len(SETTLEMENTS_ORDERED)})')
    except Exception as e:
        log.warning(f'Failed to load settlements file {SETTLEMENTS_FILE}: {e}')

_load_settlements()

def geocode_opencage(place: str):
    if not OPENCAGE_API_KEY:
        return None
    cache = _load_opencage_cache()
    key = place.strip().lower()
    now = int(datetime.utcnow().timestamp())
    if key in cache:
        entry = cache[key]
        if now - entry.get('ts', 0) < OPENCAGE_TTL:
            return tuple(entry['coords']) if entry['coords'] else None
    import requests
    try:
        resp = requests.get('https://api.opencagedata.com/geocode/v1/json', params={
            'q': place,
            'key': OPENCAGE_API_KEY,
            'language': 'uk',
            'limit': 1,
            'countrycode': 'ua'
        }, timeout=6)
        if resp.status_code == 200:
            data = resp.json()
            if data.get('results'):
                g = data['results'][0]['geometry']
                coords = (g['lat'], g['lng'])
                cache[key] = {'ts': now, 'coords': coords}
                _save_opencage_cache()
                return coords
        cache[key] = {'ts': now, 'coords': None}
        _save_opencage_cache()
        return None
    except Exception as e:
        log.warning(f"OpenCage error for '{place}': {e}")
        cache[key] = {'ts': now, 'coords': None}
        _save_opencage_cache()
        return None

"""(Removed duplicate legacy process_message; canonical version defined earlier.)"""

# ----------------------- Deferred initialization hooks -----------------------
@app.before_request
def _init_background():
    global INIT_ONCE
    if INIT_ONCE:
        return
    INIT_ONCE = True
    _startup_diagnostics()
    # Start background workers
    try:
        start_fetch_thread()
    except Exception as e:
        log.error(f'Failed to start fetch thread: {e}\n{traceback.format_exc()}')
    try:
        start_session_watcher()
    except Exception as e:
        log.error(f'Failed to start session watcher: {e}\n{traceback.format_exc()}')

@app.route('/startup_diag')
def startup_diag():
    """Expose current diagnostic snapshot (no secrets)."""
    try:
        info = {
            'pid': os.getpid(),
            'python': sys.version.split()[0],
            'platform': platform.platform(),
            'channels': CHANNELS,
            'authorized': AUTH_STATUS,
            'messages_file_exists': os.path.exists(MESSAGES_FILE),
            'messages_count': len(load_messages()),
            'fetch_thread_started': FETCH_THREAD_STARTED,
            'session_present': bool(session_str),
            'retention_minutes': MESSAGES_RETENTION_MINUTES,
            'retention_max_count': MESSAGES_MAX_COUNT,
            'subscribers': len(SUBSCRIBERS)
        }
        return jsonify(info)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# Manual trigger (idempotent) if needed before first page hit
@app.route('/startup_init', methods=['POST'])
def startup_init():
    _init_background()
    return jsonify({'status': 'ok'})

# BANDWIDTH PROTECTION: Custom static route will compete with Flask's built-in route
# Flask will prioritize our custom route due to specificity

# Graceful shutdown handler
import atexit
import signal

def shutdown_scheduler():
    """Shutdown scheduler gracefully"""
    if SCHEDULE_UPDATER_AVAILABLE and 'scheduler' in globals():
        try:
            if scheduler.running:
                log.info("Shutting down scheduler...")
                scheduler.shutdown(wait=False)
                log.info("âœ… Scheduler shutdown complete")
        except Exception as e:
            log.error(f"Error shutting down scheduler: {e}")

# Register shutdown handlers
atexit.register(shutdown_scheduler)
signal.signal(signal.SIGTERM, lambda sig, frame: shutdown_scheduler())
signal.signal(signal.SIGINT, lambda sig, frame: shutdown_scheduler())

if __name__ == '__main__':
    # Local / container direct run (not needed if a WSGI server like gunicorn is used)
    port = int(os.getenv('PORT', '5000'))
    host = os.getenv('HOST', '0.0.0.0')
    log.info(f'Launching Flask app on {host}:{port}')
    # Eager start (still guarded) so that fetch begins even without first HTTP request locally
    try:
        _init_background()
    except Exception:
        pass
    app.run(host=host, port=port, debug=False)
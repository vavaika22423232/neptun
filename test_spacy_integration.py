#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import spacy
from spacy import displacy
import re

# –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∫—Ä–∞–∏–Ω—Å–∫—É—é –º–æ–¥–µ–ª—å
nlp = spacy.load('uk_core_news_sm')

def test_spacy_ner():
    """–¢–µ—Å—Ç Named Entity Recognition –¥–ª—è —É–∫—Ä–∞–∏–Ω—Å–∫–∏—Ö –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π"""
    
    test_messages = [
        "1 —à–∞—Ö–µ–¥ –Ω–∞ –ú–∏–∫–æ–ª–∞—ó–≤–∫—É –Ω–∞ –°—É–º—â–∏–Ω—ñ",
        "–ë–ø–õ–ê –∫—É—Ä—Å–æ–º –Ω–∞ –•–∞—Ä–∫—ñ–≤ —á–µ—Ä–µ–∑ –ü–æ–ª—Ç–∞–≤—É", 
        "2—Ö –ë–ø–õ–ê –ø–æ–≤–∑ –ö–æ–Ω–æ—Ç–æ–ø —É –Ω–∞–ø—Ä—è–º–∫—É –ì–ª—É—Ö–æ–≤–∞",
        "–ß–µ—Ä–Ω—ñ–≥—ñ–≤—â–∏–Ω–∞: 3 —à–∞—Ö–µ–¥–∏ –Ω–∞ –ù–æ–≤–≥–æ—Ä–æ–¥-–°—ñ–≤–µ—Ä—Å—å–∫–∏–π",
        "–û–±—Å—Ç—Ä—ñ–ª –•–µ—Ä—Å–æ–Ω–∞ —Ç–∞ –ú–∏–∫–æ–ª–∞—î–≤–∞",
        "–†–∞–∫–µ—Ç–Ω–∏–π —É–¥–∞—Ä –ø–æ –ö—Ä–µ–º–µ–Ω—á—É–∫—É –ü–æ–ª—Ç–∞–≤—Å—å–∫–æ—ó –æ–±–ª–∞—Å—Ç—ñ",
        "üí• –°–∏–Ω–µ–ª—å–Ω–∏–∫–æ–≤–µ ‚Äî 1—Ö –ë–ø–õ–ê –¥–æ–≤–∫–æ–ª–∞ –º—ñ—Å—Ç–∞"
    ]
    
    print("=== SpaCy Named Entity Recognition Test ===\n")
    
    for i, message in enumerate(test_messages, 1):
        print(f"–¢–µ—Å—Ç {i}: {message}")
        doc = nlp(message)
        
        print("–¢–æ–∫–µ–Ω–∏ —Ç–∞ —ó—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
        for token in doc:
            print(f"  {token.text:15} | POS: {token.pos_:10} | Tag: {token.tag_:10} | Dep: {token.dep_:10}")
        
        print("–ù–∞–∑–≤–∞–Ω—ñ —Å—É—Ç–Ω–æ—Å—Ç—ñ (Named Entities):")
        for ent in doc.ents:
            print(f"  {ent.text:20} | Label: {ent.label_:10} | Start: {ent.start} | End: {ent.end}")
        
        print("–Ü–º–µ–Ω–Ω–∏–∫–æ–≤—ñ –≥—Ä—É–ø–∏ (Noun Chunks):")
        try:
            for chunk in doc.noun_chunks:
                print(f"  {chunk.text:20} | Root: {chunk.root.text:10} | Dep: {chunk.root.dep_}")
        except NotImplementedError:
            print("  (–ù–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó –º–æ–≤–∏)")
        
        print("-" * 60)

def extract_cities_with_spacy(text):
    """–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –º—ñ—Å—Ç –∑ —Ç–µ–∫—Å—Ç—É –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é SpaCy"""
    doc = nlp(text)
    
    cities = []
    regions = []
    
    # –í–∏—Ç—è–≥—É—î–º–æ –≥–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –æ–±'—î–∫—Ç–∏ —á–µ—Ä–µ–∑ NER
    for ent in doc.ents:
        if ent.label_ in ['LOC', 'GPE']:  # Location, Geopolitical entity
            if any(region_marker in ent.text.lower() for region_marker in ['—â–∏–Ω–∞', '–æ–±–ª–∞—Å—Ç—å', '–æ–±–ª']):
                regions.append(ent.text)
            else:
                cities.append(ent.text)
    
    # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –ø–æ—à—É–∫ —á–µ—Ä–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∏
    for token in doc:
        # –®—É–∫–∞—î–º–æ –º—ñ—Å—Ç–∞ –ø—ñ—Å–ª—è –ø—Ä–∏–π–º–µ–Ω–Ω–∏–∫—ñ–≤ "–Ω–∞", "–ø–æ–≤–∑", "—á–µ—Ä–µ–∑"
        if token.text.lower() in ['–Ω–∞', '–ø–æ–≤–∑', '—á–µ—Ä–µ–∑'] and token.i + 1 < len(doc):
            next_token = doc[token.i + 1]
            if next_token.pos_ == 'NOUN' or next_token.pos_ == 'PROPN':
                # –ó–±–∏—Ä–∞—î–º–æ –º–æ–∂–ª–∏–≤—É –Ω–∞–∑–≤—É –º—ñ—Å—Ç–∞ (–º–æ–∂–µ –±—É—Ç–∏ –±–∞–≥–∞—Ç–æ—Å–ª–æ–≤–Ω–æ—é)
                city_tokens = []
                for j in range(token.i + 1, min(token.i + 4, len(doc))):  # –î–æ 3 —Å–ª—ñ–≤
                    if doc[j].pos_ in ['NOUN', 'PROPN', 'ADJ'] or doc[j].text == '-':
                        city_tokens.append(doc[j].text)
                    else:
                        break
                
                if city_tokens:
                    potential_city = ' '.join(city_tokens)
                    if potential_city not in cities:
                        cities.append(potential_city)
    
    return cities, regions

def test_city_extraction():
    """–¢–µ—Å—Ç –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –º—ñ—Å—Ç"""
    
    test_messages = [
        "1 —à–∞—Ö–µ–¥ –Ω–∞ –ú–∏–∫–æ–ª–∞—ó–≤–∫—É –Ω–∞ –°—É–º—â–∏–Ω—ñ",
        "–ë–ø–õ–ê –∫—É—Ä—Å–æ–º –Ω–∞ –•–∞—Ä–∫—ñ–≤ —á–µ—Ä–µ–∑ –ü–æ–ª—Ç–∞–≤—É", 
        "2—Ö –ë–ø–õ–ê –ø–æ–≤–∑ –ö–æ–Ω–æ—Ç–æ–ø —É –Ω–∞–ø—Ä—è–º–∫—É –ì–ª—É—Ö–æ–≤–∞",
        "–ß–µ—Ä–Ω—ñ–≥—ñ–≤—â–∏–Ω–∞: 3 —à–∞—Ö–µ–¥–∏ –Ω–∞ –ù–æ–≤–≥–æ—Ä–æ–¥-–°—ñ–≤–µ—Ä—Å—å–∫–∏–π",
        "–û–±—Å—Ç—Ä—ñ–ª –•–µ—Ä—Å–æ–Ω–∞ —Ç–∞ –ú–∏–∫–æ–ª–∞—î–≤–∞"
    ]
    
    print("\n=== SpaCy City Extraction Test ===\n")
    
    for i, message in enumerate(test_messages, 1):
        print(f"–¢–µ—Å—Ç {i}: {message}")
        cities, regions = extract_cities_with_spacy(message)
        
        print(f"  –ú—ñ—Å—Ç–∞: {cities}")
        print(f"  –†–µ–≥—ñ–æ–Ω–∏: {regions}")
        print("-" * 60)

def test_morphology():
    """–¢–µ—Å—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥—ñ—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–∫—ñ–≤"""
    
    test_words = [
        "–ú–∏–∫–æ–ª–∞—ó–≤–∫—É",  # –ó–Ω–∞—Ö—ñ–¥–Ω–∏–π –≤—ñ–¥–º—ñ–Ω–æ–∫
        "–ú–∏–∫–æ–ª–∞—ó–≤–∫–∞",  # –ù–∞–∑–∏–≤–Ω–∏–π –≤—ñ–¥–º—ñ–Ω–æ–∫  
        "–•–∞—Ä–∫—ñ–≤",
        "–•–∞—Ä–∫–æ–≤–∞",
        "–ü–æ–ª—Ç–∞–≤—É",
        "–ü–æ–ª—Ç–∞–≤–∞"
    ]
    
    print("\n=== SpaCy Morphology Test ===\n")
    
    for word in test_words:
        doc = nlp(word)
        token = doc[0]
        
        print(f"–°–ª–æ–≤–æ: {word}")
        print(f"  –õ–µ–º–∞: {token.lemma_}")
        print(f"  POS: {token.pos_}")
        print(f"  –ú–æ—Ä—Ñ–æ–ª–æ–≥—ñ—è: {token.morph}")
        print("-" * 30)

if __name__ == "__main__":
    test_spacy_ner()
    test_city_extraction() 
    test_morphology()

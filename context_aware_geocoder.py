#!/usr/bin/env python3
"""
Context-Aware Geocoding Plugin
Intelligent text analysis to determine primary targets vs regional context
"""

import re
import spacy
from typing import List, Dict, Tuple, Optional

class ContextAwareGeocoder:
    """
    Intelligent geocoder that understands context and determines primary targets
    """
    
    def __init__(self):
        # Load SpaCy model if available
        try:
            self.nlp = spacy.load('uk_core_news_sm')
            self.spacy_available = True
        except (ImportError, OSError):
            self.spacy_available = False
            self.nlp = None
        
        # Target prepositions that indicate primary location
        self.target_prepositions = [
            '–Ω–∞–¥', '–Ω–∞', '—É', '–≤', '–¥–æ', '–ø—ñ–¥', '–±—ñ–ª—è', '–ø–æ–±–ª–∏–∑—É', '–∫—É—Ä—Å–æ–º –Ω–∞'
        ]
        
        # Regional context indicators (less priority)
        self.region_indicators = [
            '—â–∏–Ω–∞', '–æ–±–ª–∞—Å—Ç—å', '–æ–±–ª', '—Ä–µ–≥—ñ–æ–Ω', '—Ä–∞–π–æ–Ω'
        ]
        
        # Action words that indicate threats/movement towards targets
        self.action_words = [
            '—à–∞—Ö–µ–¥–∏', '–±–ø–ª–∞', '—Ä–∞–∫–µ—Ç–∏', '–∫—É—Ä—Å–æ–º', '–ª–µ—Ç—è—Ç—å', '—Ä—É—Ö–∞—é—Ç—å—Å—è', '–Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç—å—Å—è'
        ]
    
    def analyze_message_context(self, text: str) -> Dict:
        """
        Analyze message to determine primary targets vs regional context
        
        Returns:
            Dict with 'primary_targets', 'regional_context', and 'confidence'
        """
        result = {
            'primary_targets': [],
            'regional_context': [],
            'confidence': 0.0,
            'analysis': []
        }
        
        # Normalize text
        text_lower = text.lower()
        
        # Pattern 1: "–Ω–∞–¥/–Ω–∞ [target]" - high priority
        target_pattern = r'(?:–Ω–∞–¥|–Ω–∞|—É|–≤|–¥–æ|–ø—ñ–¥|–±—ñ–ª—è|–ø–æ–±–ª–∏–∑—É|–∫—É—Ä—Å–æ–º\s+–Ω–∞)\s+([a-z–∞-—è—ñ—ó—î“ë\' º\'`\-]+(?:–æ—é|–∫–æ—é|–æ—é|–µ—é|—é)?)'
        target_matches = re.findall(target_pattern, text_lower)
        
        for match in target_matches:
            city = self._normalize_city_form(match)
            result['primary_targets'].append({
                'name': city,
                'confidence': 0.9,
                'reason': 'target_preposition',
                'original': match
            })
            result['analysis'].append(f"Found target preposition: '{match}' ‚Üí '{city}'")
        
        # Pattern 2: Regional context (lower priority)
        region_pattern = r'([a-z–∞-—è—ñ—ó—î“ë\' º\'`\-]+(?:—â–∏–Ω[–∞–∏]|–æ–±–ª–∞—Å—Ç[—å—ñ]|–æ–±–ª\.?))'
        region_matches = re.findall(region_pattern, text_lower)
        
        for match in region_matches:
            region_clean = match.replace('—â–∏–Ω–∞', '').replace('–æ–±–ª–∞—Å—Ç—å', '').replace('–æ–±–ª.', '').replace('–æ–±–ª', '').strip()
            result['regional_context'].append({
                'name': region_clean,
                'confidence': 0.3,
                'reason': 'regional_indicator',
                'original': match
            })
            result['analysis'].append(f"Found regional context: '{match}' ‚Üí '{region_clean}'")
        
        # Pattern 3: Action words + city context
        if any(action in text_lower for action in self.action_words):
            # Look for cities mentioned after action words
            action_city_pattern = r'(?:—à–∞—Ö–µ–¥–∏|–±–ø–ª–∞|—Ä–∞–∫–µ—Ç–∏|–∞—Ç–∞–∫—É–≤–∞–ª–∏|–æ–±—Å—Ç—Ä—ñ–ª—è–ª–∏).*?([a-z–∞-—è—ñ—ó—î“ë\' º\'`\-]+(?:–æ—é|–∫–æ—é|–æ—é|–µ—é|—é|–∫–æ|—Ü—ñ|—É|—Å—å–∫|—á—É–∫|–∫–∞|–Ω–∞|–≤–æ|—ñ–≤|—Å—å–∫))\b'
            action_matches = re.findall(action_city_pattern, text_lower)
            
            for match in action_matches:
                # Skip action words themselves and region indicators
                if (match not in self.action_words and 
                    not any(indicator in match for indicator in self.region_indicators) and
                    len(match) > 3):  # Skip short words
                    city = self._normalize_city_form(match)
                    result['primary_targets'].append({
                        'name': city,
                        'confidence': 0.7,
                        'reason': 'action_context',
                        'original': match
                    })
                    result['analysis'].append(f"Found action context: '{match}' ‚Üí '{city}'")
        
        # Calculate overall confidence
        if result['primary_targets']:
            result['confidence'] = max(target['confidence'] for target in result['primary_targets'])
        
        # Remove duplicates and sort by confidence
        result['primary_targets'] = self._deduplicate_targets(result['primary_targets'])
        result['regional_context'] = self._deduplicate_targets(result['regional_context'])
        
        return result
    
    def _normalize_city_form(self, city_form: str) -> str:
        """
        Normalize declined city forms to nominative case
        """
        city = city_form.strip().lower()
        
        # Special cases first
        special_cases = {
            '–∫—Ä–µ–º–µ–Ω—á—É—Ü—ñ': '–∫—Ä–µ–º–µ–Ω—á—É–∫',
            '–∫–∏—î–≤—ñ': '–∫–∏—ó–≤',
            '–ª—å–≤–æ–≤—ñ': '–ª—å–≤—ñ–≤',
            '—Ö–∞—Ä–∫–æ–≤—ñ': '—Ö–∞—Ä–∫—ñ–≤',
            '–¥–Ω—ñ–ø—Ä—ñ': '–¥–Ω—ñ–ø—Ä–æ',
            '–æ–¥–µ—Å—ñ': '–æ–¥–µ—Å–∞',
            '–ø–æ–ª—Ç–∞–≤—ñ': '–ø–æ–ª—Ç–∞–≤–∞'
        }
        
        if city in special_cases:
            return special_cases[city]
        
        # General morphological patterns
        endings_map = {
            '–æ—é': '–∞',
            '–∫–æ—é': '–∫–∞', 
            '–µ—é': '—è',
            '—é': '–∞',
            '–∫–æ': '–∫–∏',
            '—Ü—ñ': '',    # Just remove -—Ü—ñ 
            '—É': '–∞',
            '—ñ': '',     # Remove -—ñ for locative
        }
        
        for ending, replacement in endings_map.items():
            if city.endswith(ending):
                return city[:-len(ending)] + replacement
        
        return city
    
    def _deduplicate_targets(self, targets: List[Dict]) -> List[Dict]:
        """
        Remove duplicate targets and keep highest confidence
        """
        unique_targets = {}
        
        for target in targets:
            name = target['name']
            if name not in unique_targets or target['confidence'] > unique_targets[name]['confidence']:
                unique_targets[name] = target
        
        return sorted(unique_targets.values(), key=lambda x: x['confidence'], reverse=True)
    
    def get_priority_geocoding_candidates(self, text: str) -> List[Tuple[str, str, float]]:
        """
        Get prioritized list of geocoding candidates
        
        Returns:
            List of (city_name, region_context, confidence) tuples
        """
        analysis = self.analyze_message_context(text)
        candidates = []
        
        # Primary targets get highest priority
        for target in analysis['primary_targets']:
            region = None
            if analysis['regional_context']:
                region = analysis['regional_context'][0]['name']  # Use most confident region
            
            candidates.append((target['name'], region, target['confidence']))
        
        # If no primary targets found, fall back to regional context
        if not candidates and analysis['regional_context']:
            for region in analysis['regional_context']:
                candidates.append((region['name'], None, region['confidence'] * 0.5))  # Lower confidence
        
        return candidates

# Global instance
context_geocoder = ContextAwareGeocoder()

def get_context_aware_geocoding(text: str) -> List[Tuple[str, str, float]]:
    """
    Public function to get context-aware geocoding candidates
    """
    return context_geocoder.get_priority_geocoding_candidates(text)

if __name__ == "__main__":
    # Test the context analyzer
    test_messages = [
        "—á–µ—Ä–Ω—ñ–≥—ñ–≤—â–∏–Ω–∞ - –Ω–æ–≤—ñ —à–∞—Ö–µ–¥–∏ –Ω–∞–¥ —Å–µ–º–µ–Ω—ñ–≤–∫–æ—é",
        "—à–∞—Ö–µ–¥–∏ –Ω–∞–¥ —Å–µ–º–µ–Ω—ñ–≤–∫–æ—é —á–µ—Ä–Ω—ñ–≥—ñ–≤—Å—å–∫–æ—ó –æ–±–ª–∞—Å—Ç—ñ",
        "–ë–ø–õ–ê –∫—É—Ä—Å–æ–º –Ω–∞ —Å–µ–º–µ–Ω—ñ–≤–∫—É",
        "–Ω–æ–≤–∏–Ω–∏ –∑ —Ö–∞—Ä–∫—ñ–≤—â–∏–Ω–∏ - –±–ø–ª–∞ —É –±–∞–ª–∞–∫–ª—ñ—ó",
        "–¥–æ–Ω–µ—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å - —Ä–∞–∫–µ—Ç–∏ –Ω–∞ –ø–æ–∫—Ä–æ–≤—Å—å–∫"
    ]
    
    geocoder = ContextAwareGeocoder()
    
    for msg in test_messages:
        print(f"\nüîç Testing: '{msg}'")
        analysis = geocoder.analyze_message_context(msg)
        print(f"Primary targets: {analysis['primary_targets']}")
        print(f"Regional context: {analysis['regional_context']}")
        print(f"Confidence: {analysis['confidence']}")
        
        candidates = geocoder.get_priority_geocoding_candidates(msg)
        print(f"Geocoding candidates: {candidates}")
        print("---")
